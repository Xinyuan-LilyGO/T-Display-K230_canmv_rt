From 30106dcec747574f1f2d1028b26a472dc3a3bc71 Mon Sep 17 00:00:00 2001
From: yechang <yechang@rt-thread.com>
Date: Mon, 6 Mar 2023 15:29:30 +0800
Subject: [PATCH 4/5] add openblas_rvv_v2

---
 Makefile                                   |  237 +-
 Makefile.prebuild                          |   12 +-
 Makefile.riscv64                           |    6 +
 TargetList.txt                             |    6 +-
 common_riscv64.h                           |    7 +-
 cpuid_riscv64.c                            |    5 +
 getarch.c                                  |   73 +-
 kernel/riscv64/KERNEL.C908V                |  190 +
 kernel/riscv64/dgemm_kernel_8x4_c908v.c    |  978 ++++
 kernel/riscv64/sgemm_kernel_16x4_c908v.c   | 1633 +++++++
 param.h                                    |   46 +-
 riscv_vector_itr.h                         | 5135 ++++++++++++++++++++
 tests/fortran_example/SConscript           |   20 +
 tests/fortran_example/SConstruct           |    8 +
 tests/fortran_example/openblas_fortran.cpp |   97 +
 tests/openblas_level1/SConscript           |    2 +-
 tests/openblas_level1/openblas_level1.cpp  |   55 +-
 tests/openblas_level2/SConscript           |    2 +-
 tests/openblas_level2/openblas_level2.cpp  |   66 +-
 tests/openblas_level3/SConscript           |    2 +-
 tests/openblas_level3/openblas_level3.cpp  |   66 +-
 21 files changed, 8402 insertions(+), 244 deletions(-)
 create mode 100644 kernel/riscv64/KERNEL.C908V
 create mode 100644 kernel/riscv64/dgemm_kernel_8x4_c908v.c
 create mode 100644 kernel/riscv64/sgemm_kernel_16x4_c908v.c
 create mode 100644 riscv_vector_itr.h
 create mode 100644 tests/fortran_example/SConscript
 create mode 100644 tests/fortran_example/SConstruct
 create mode 100644 tests/fortran_example/openblas_fortran.cpp

diff --git a/Makefile b/Makefile
index e8cc3e06..8e23ba5b 100644
--- a/Makefile
+++ b/Makefile
@@ -226,130 +226,130 @@ blas :
 	fi; \
 	done
 
-# hpl :
-# 	ln -fs $(LIBNAME) $(LIBPREFIX).$(LIBSUFFIX)
-# 	for d in $(BLASDIRS) ../laswp exports ; \
-# 	do if test -d $$d; then \
-# 	  $(MAKE) -C $$d $(@F) || exit 1 ; \
-# 	fi; \
-# 	done
-# ifeq ($(DYNAMIC_ARCH), 1)
-# 	  $(MAKE) -C kernel commonlibs || exit 1
-# 	for d in $(DYNAMIC_CORE) ; \
-# 	do  $(MAKE) GOTOBLAS_MAKEFILE= -C kernel TARGET_CORE=$$d kernel || exit 1 ;\
-# 	done
-# endif
+hpl :
+	ln -fs $(LIBNAME) $(LIBPREFIX).$(LIBSUFFIX)
+	for d in $(BLASDIRS) ../laswp exports ; \
+	do if test -d $$d; then \
+	  $(MAKE) -C $$d $(@F) || exit 1 ; \
+	fi; \
+	done
+ifeq ($(DYNAMIC_ARCH), 1)
+	  $(MAKE) -C kernel commonlibs || exit 1
+	for d in $(DYNAMIC_CORE) ; \
+	do  $(MAKE) GOTOBLAS_MAKEFILE= -C kernel TARGET_CORE=$$d kernel || exit 1 ;\
+	done
+endif
 
-# hpl_p :
-# 	ln -fs $(LIBNAME_P) $(LIBPREFIX)_p.$(LIBSUFFIX)
-# 	for d in $(SUBDIRS) ../laswp exports ; \
-# 	do if test -d $$d; then \
-# 	  $(MAKE) -C $$d $(@F) || exit 1 ; \
-# 	fi; \
-# 	done
-
-# netlib : lapack_prebuild
-# ifneq ($(NO_LAPACK), 1)
-# 	@$(MAKE) -C $(NETLIB_LAPACK_DIR) lapacklib
-# 	@$(MAKE) -C $(NETLIB_LAPACK_DIR) tmglib
-# endif
-# ifneq ($(NO_LAPACKE), 1)
-# 	@$(MAKE) -C $(NETLIB_LAPACK_DIR) lapackelib
-# endif
+hpl_p :
+	ln -fs $(LIBNAME_P) $(LIBPREFIX)_p.$(LIBSUFFIX)
+	for d in $(SUBDIRS) ../laswp exports ; \
+	do if test -d $$d; then \
+	  $(MAKE) -C $$d $(@F) || exit 1 ; \
+	fi; \
+	done
 
-# ifeq ($(NO_LAPACK), 1)
-# re_lapack :
+netlib : lapack_prebuild
+ifneq ($(NO_LAPACK), 1)
+	@$(MAKE) -C $(NETLIB_LAPACK_DIR) lapacklib
+	@$(MAKE) -C $(NETLIB_LAPACK_DIR) tmglib
+endif
+ifneq ($(NO_LAPACKE), 1)
+	@$(MAKE) -C $(NETLIB_LAPACK_DIR) lapackelib
+endif
 
-# else
-# re_lapack :
-# 	@$(MAKE) -C relapack
-# endif
+ifeq ($(NO_LAPACK), 1)
+re_lapack :
 
-# prof_lapack : lapack_prebuild
-# 	@$(MAKE) -C $(NETLIB_LAPACK_DIR) lapack_prof
-
-# lapack_prebuild :
-# ifeq ($(NO_LAPACK), $(filter 0,$(NO_LAPACK)))
-# 	-@echo "FC          = $(FC)" > $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "override FFLAGS      = $(LAPACK_FFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "FFLAGS_DRV  = $(LAPACK_FFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "POPTS       = $(LAPACK_FPFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "FFLAGS_NOOPT       = -O0 $(LAPACK_NOOPT)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "PNOOPT      = $(LAPACK_FPFLAGS) -O0" >> $(NETLIB_LAPACK_DIR)/make.inc
-# ifeq ($(C_COMPILER)$(F_COMPILER)$(USE_OPENMP), CLANGGFORTRAN1)
-# 	-@echo "LDFLAGS     = $(FFLAGS) $(EXTRALIB) -lomp" >> $(NETLIB_LAPACK_DIR)/make.inc
-# else
-# 	-@echo "LDFLAGS     = $(FFLAGS) $(EXTRALIB)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# 	-@echo "CC          = $(CC)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "override CFLAGS      = $(LAPACK_CFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "AR          = $(AR)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "ARFLAGS     = $(ARFLAGS) -ru" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "RANLIB      = $(RANLIB)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "LAPACKLIB   = ../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "TMGLIB      = ../../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "BLASLIB     = ../../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "LAPACKELIB  = ../../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "LAPACKLIB_P = ../$(LIBNAME_P)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "SUFFIX      = $(SUFFIX)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "PSUFFIX     = $(PSUFFIX)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "CEXTRALIB   = $(EXTRALIB)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# ifeq ($(F_COMPILER), GFORTRAN)
-# 	-@echo "TIMER       = INT_ETIME" >> $(NETLIB_LAPACK_DIR)/make.inc
-# ifdef SMP
-# ifeq ($(OSNAME), WINNT)
-# 	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# else ifeq ($(OSNAME), Haiku)
-# 	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# else
-# 	-@echo "LOADER      = $(FC) -pthread" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# else
-# 	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# else
-# 	-@echo "TIMER       = NONE" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# ifeq ($(BUILD_LAPACK_DEPRECATED), 1)
-# 	-@echo "BUILD_DEPRECATED      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# ifeq ($(BUILD_SINGLE), 1)
-# 	-@echo "BUILD_SINGLE      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# ifeq ($(BUILD_DOUBLE), 1)
-# 	-@echo "BUILD_DOUBLE      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# ifeq ($(BUILD_COMPLEX), 1)
-# 	-@echo "BUILD_COMPLEX      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# ifeq ($(BUILD_COMPLEX16), 1)
-# 	-@echo "BUILD_COMPLEX16      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
-# 	-@echo "LAPACKE_WITH_TMG      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
-# 	-@cat  make.inc >> $(NETLIB_LAPACK_DIR)/make.inc
-# endif
+else
+re_lapack :
+	@$(MAKE) -C relapack
+endif
 
-# large.tgz :
-# ifeq ($(NOFORTRAN), $(filter 0,$(NOFORTRAN)))
-# 	if [ ! -a $< ]; then
-# 	-wget http://www.netlib.org/lapack/timing/large.tgz;
-# 	fi
-# endif
+prof_lapack : lapack_prebuild
+	@$(MAKE) -C $(NETLIB_LAPACK_DIR) lapack_prof
+
+lapack_prebuild :
+ifeq ($(NO_LAPACK), $(filter 0,$(NO_LAPACK)))
+	-@echo "FC          = $(FC)" > $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "override FFLAGS      = $(LAPACK_FFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "FFLAGS_DRV  = $(LAPACK_FFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "POPTS       = $(LAPACK_FPFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "FFLAGS_NOOPT       = -O0 $(LAPACK_NOOPT)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "PNOOPT      = $(LAPACK_FPFLAGS) -O0" >> $(NETLIB_LAPACK_DIR)/make.inc
+ifeq ($(C_COMPILER)$(F_COMPILER)$(USE_OPENMP), CLANGGFORTRAN1)
+	-@echo "LDFLAGS     = $(FFLAGS) $(EXTRALIB) -lomp" >> $(NETLIB_LAPACK_DIR)/make.inc
+else
+	-@echo "LDFLAGS     = $(FFLAGS) $(EXTRALIB)" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+	-@echo "CC          = $(CC)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "override CFLAGS      = $(LAPACK_CFLAGS)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "AR          = $(AR)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "ARFLAGS     = $(ARFLAGS) -ru" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "RANLIB      = $(RANLIB)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "LAPACKLIB   = ../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "TMGLIB      = ../../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "BLASLIB     = ../../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "LAPACKELIB  = ../../../$(LIBNAME)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "LAPACKLIB_P = ../$(LIBNAME_P)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "SUFFIX      = $(SUFFIX)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "PSUFFIX     = $(PSUFFIX)" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "CEXTRALIB   = $(EXTRALIB)" >> $(NETLIB_LAPACK_DIR)/make.inc
+ifeq ($(F_COMPILER), GFORTRAN)
+	-@echo "TIMER       = INT_ETIME" >> $(NETLIB_LAPACK_DIR)/make.inc
+ifdef SMP
+ifeq ($(OSNAME), WINNT)
+	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
+else ifeq ($(OSNAME), Haiku)
+	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
+else
+	-@echo "LOADER      = $(FC) -pthread" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+else
+	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+else
+	-@echo "TIMER       = NONE" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@echo "LOADER      = $(FC)" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+ifeq ($(BUILD_LAPACK_DEPRECATED), 1)
+	-@echo "BUILD_DEPRECATED      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+ifeq ($(BUILD_SINGLE), 1)
+	-@echo "BUILD_SINGLE      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+ifeq ($(BUILD_DOUBLE), 1)
+	-@echo "BUILD_DOUBLE      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+ifeq ($(BUILD_COMPLEX), 1)
+	-@echo "BUILD_COMPLEX      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+ifeq ($(BUILD_COMPLEX16), 1)
+	-@echo "BUILD_COMPLEX16      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
+	-@echo "LAPACKE_WITH_TMG      = 1" >> $(NETLIB_LAPACK_DIR)/make.inc
+	-@cat  make.inc >> $(NETLIB_LAPACK_DIR)/make.inc
+endif
 
-# timing.tgz :
-# ifeq ($(NOFORTRAN), $(filter 0,$(NOFORTRAN)))
-# 	if [ ! -a $< ]; then
-# 	-wget http://www.netlib.org/lapack/timing/timing.tgz;
-# 	fi
-# endif
+large.tgz :
+ifeq ($(NOFORTRAN), $(filter 0,$(NOFORTRAN)))
+	if [ ! -a $< ]; then
+	-wget http://www.netlib.org/lapack/timing/large.tgz;
+	fi
+endif
 
-# lapack-timing : large.tgz timing.tgz
-# ifeq ($(NOFORTRAN), $(filter 0,$(NOFORTRAN)))
-# 	(cd $(NETLIB_LAPACK_DIR); $(TAR) zxf ../timing.tgz TIMING)
-# 	(cd $(NETLIB_LAPACK_DIR)/TIMING; $(TAR) zxf ../../large.tgz )
-# 	$(MAKE) -C $(NETLIB_LAPACK_DIR)/TIMING
-# endif
+timing.tgz :
+ifeq ($(NOFORTRAN), $(filter 0,$(NOFORTRAN)))
+	if [ ! -a $< ]; then
+	-wget http://www.netlib.org/lapack/timing/timing.tgz;
+	fi
+endif
+
+lapack-timing : large.tgz timing.tgz
+ifeq ($(NOFORTRAN), $(filter 0,$(NOFORTRAN)))
+	(cd $(NETLIB_LAPACK_DIR); $(TAR) zxf ../timing.tgz TIMING)
+	(cd $(NETLIB_LAPACK_DIR)/TIMING; $(TAR) zxf ../../large.tgz )
+	$(MAKE) -C $(NETLIB_LAPACK_DIR)/TIMING
+endif
 
 
 # lapack-test :
@@ -398,7 +398,8 @@ endif
 	@touch $(NETLIB_LAPACK_DIR)/make.inc
 	@$(MAKE) -C $(NETLIB_LAPACK_DIR) clean
 	@rm -f $(NETLIB_LAPACK_DIR)/make.inc $(NETLIB_LAPACK_DIR)/lapacke/include/lapacke_mangling.h
-	@$(MAKE) -C relapack clean
+#	@$(MAKE) -C relapack clean
 	@rm -f *.grd Makefile.conf_last config_last.h
 	@(cd $(NETLIB_LAPACK_DIR)/TESTING && rm -f x* *.out testing_results.txt)
 	@echo Done.
+
diff --git a/Makefile.prebuild b/Makefile.prebuild
index 0be4f127..010f9ddb 100644
--- a/Makefile.prebuild
+++ b/Makefile.prebuild
@@ -51,6 +51,10 @@ ifeq ($(TARGET), I6500)
 TARGET_FLAGS = -mips64r6
 endif
 
+ifeq ($(TARGET), C908V)
+TARGET_FLAGS = -march=rv64imafdcv -flax-vector-conversions -mabi=lp64d
+endif
+
 ifeq ($(TARGET), C910V)
 TARGET_FLAGS = -march=rv64gcv0p7_zfh_xtheadc -mabi=lp64d
 endif
@@ -60,9 +64,9 @@ all: getarch_2nd
 	./getarch_2nd  1 >> $(TARGET_CONF)
 
 $(TARGET_CONF): c_check$(SCRIPTSUFFIX) f_check$(SCRIPTSUFFIX) getarch
-	./c_check$(SCRIPTSUFFIX) $(TARGET_MAKE) $(TARGET_CONF) "$(CC)" $(TARGET_FLAGS) $(CFLAGS)
+	./c_check$(SCRIPTSUFFIX) $(TARGET_MAKE) $(TARGET_CONF) $(CC) $(TARGET_FLAGS) $(CFLAGS)
 ifneq ($(ONLY_CBLAS), 1)
-	./f_check$(SCRIPTSUFFIX) $(TARGET_MAKE) $(TARGET_CONF) "$(FC)" $(TARGET_FLAGS)
+	./f_check$(SCRIPTSUFFIX) $(TARGET_MAKE) $(TARGET_CONF) $(FC) $(TARGET_FLAGS)
 else
 #When we only build CBLAS, we set NOFORTRAN=2
 	echo "NOFORTRAN=2" >> $(TARGET_MAKE)
@@ -77,8 +81,8 @@ endif
 
 
 getarch : getarch.c cpuid.S dummy $(CPUIDEMU)
-	avx512=$$(./c_check$(SCRIPTSUFFIX) - - "$(CC)" $(TARGET_FLAGS) $(CFLAGS) | grep NO_AVX512); \
-	rv64gv=$$(./c_check$(SCRIPTSUFFIX) - - "$(CC)" $(TARGET_FLAGS) $(CFLAGS) | grep NO_RV64GV); \
+	avx512=$$(./c_check$(SCRIPTSUFFIX) - - $(CC) $(TARGET_FLAGS) $(CFLAGS) | grep NO_AVX512); \
+	rv64gv=$$(./c_check$(SCRIPTSUFFIX) - - $(CC) $(TARGET_FLAGS) $(CFLAGS) | grep NO_RV64GV); \
 	$(HOSTCC) $(HOST_CFLAGS) $(EXFLAGS) $${avx512:+-D$${avx512}} $${rv64gv:+-D$${rv64gv}} -o $(@F) getarch.c cpuid.S $(CPUIDEMU)
 
 getarch_2nd : getarch_2nd.c $(TARGET_CONF) dummy
diff --git a/Makefile.riscv64 b/Makefile.riscv64
index ce91e03e..97111a8b 100644
--- a/Makefile.riscv64
+++ b/Makefile.riscv64
@@ -1,3 +1,9 @@
+ifeq ($(CORE), C908V)
+CCOMMON_OPT += -march=rv64imafdcv -flax-vector-conversions -mabi=lp64d
+FCOMMON_OPT += -march=rv64imafdcv -flax-vector-conversions -mabi=lp64d -static
+endif
+
+
 ifeq ($(CORE), C910V)
 CCOMMON_OPT += -march=rv64imafdcv0p7_zfh_xtheadc -mabi=lp64d -mtune=c920
 FCOMMON_OPT += -march=rv64imafdcv0p7_zfh_xtheadc -mabi=lp64d -mtune=c920 -static
diff --git a/TargetList.txt b/TargetList.txt
index deef7581..6f1ed234 100644
--- a/TargetList.txt
+++ b/TargetList.txt
@@ -65,7 +65,6 @@ MIPS1004K
 MIPS24K
 
 4.MIPS64 CPU:
-MIPS64_GENERIC
 SICORTEX
 LOONGSON3A
 LOONGSON3B
@@ -119,6 +118,7 @@ Z14
 
 10.RISC-V 64:
 RISCV64_GENERIC
+C908V
 C910V
 
 11.LOONGARCH64:
@@ -129,7 +129,3 @@ LOONGSON2K1000
 12. Elbrus E2000:
 E2K
 
-13. Alpha
-EV4
-EV5
-EV6
diff --git a/common_riscv64.h b/common_riscv64.h
index 7ddbe80a..6a8c3d76 100644
--- a/common_riscv64.h
+++ b/common_riscv64.h
@@ -88,9 +88,14 @@ static inline int blas_quickdivide(blasint x, blasint y){
 
 
 
-#define BUFFER_SIZE     ( 32 << 20)
+#define BUFFER_SIZE     ( 32 << 16)
 #define SEEK_ADDRESS
 
+#if defined(C908V)
+#include <riscv_vector.h>
+#include "riscv_vector_itr.h"
+#endif
+
 #if defined(C910V)
 #include <riscv_vector.h>
 #endif
diff --git a/cpuid_riscv64.c b/cpuid_riscv64.c
index 894d2b87..6233d571 100644
--- a/cpuid_riscv64.c
+++ b/cpuid_riscv64.c
@@ -75,6 +75,7 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 static char *cpuname[] = {
   "RISCV64_GENERIC",
+  "C908",
   "C910V"
 };
 
@@ -82,6 +83,7 @@ int detect(void){
 #ifdef __linux
   FILE *infile;
   char buffer[512],isa_buffer[512],model_buffer[512];
+  const char* check_c908_str = "T-HEAD C908";
   const char* check_c910_str = "T-HEAD C910";
   char *pmodel = NULL, *pisa = NULL;
 
@@ -103,6 +105,9 @@ int detect(void){
   if (!pmodel)
    return(CPU_GENERIC);
    
+  if (strstr(pmodel, check_c908_str) && strchr(pisa, 'v'))
+   return CPU_C908V;
+   
   if (strstr(pmodel, check_c910_str) && strchr(pisa, 'v'))
     return CPU_C910V;
 
diff --git a/getarch.c b/getarch.c
index cde5b4e8..2268885f 100644
--- a/getarch.c
+++ b/getarch.c
@@ -131,7 +131,6 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 /* #define FORCE_PPC440		*/
 /* #define FORCE_PPC440FP2	*/
 /* #define FORCE_CELL		*/
-/* #define FORCE_MIPS64_GENERIC	*/
 /* #define FORCE_SICORTEX	*/
 /* #define FORCE_LOONGSON3R3     */
 /* #define FORCE_LOONGSON3R4     */
@@ -147,9 +146,6 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 /* #define FORCE_SPARCV7	*/
 /* #define FORCE_ZARCH_GENERIC	*/
 /* #define FORCE_Z13		*/
-/* #define FORCE_EV4		*/
-/* #define FORCE_EV5		*/
-/* #define FORCE_EV6		*/
 /* #define FORCE_GENERIC	*/
 
 #ifdef FORCE_P2
@@ -919,20 +915,6 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define CORENAME  "CELL"
 #endif
 
-#ifdef FORCE_MIPS64_GENERIC
-#define FORCE
-#define ARCHITECTURE    "MIPS"
-#define SUBARCHITECTURE "MIPS64_GENERIC"
-#define SUBDIRNAME      "mips64"
-#define ARCHCONFIG   "-DMIPS64_GENERIC " \
-       "-DL1_DATA_SIZE=65536 -DL1_DATA_LINESIZE=32 " \
-       "-DL2_SIZE=1048576 -DL2_LINESIZE=32 " \
-       "-DDTB_DEFAULT_ENTRIES=64 -DDTB_SIZE=4096 -DL2_ASSOCIATIVE=8 "
-#define LIBNAME   "mips64_generic"
-#define CORENAME  "MIPS64_GENERIC"
-#else
-#endif
-
 #ifdef FORCE_SICORTEX
 #define FORCE
 #define ARCHITECTURE    "MIPS"
@@ -1619,40 +1601,29 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define CORENAME  "Z14"
 #endif
 
-#ifdef FORCE_EV4
+#ifdef FORCE_C908V
 #define FORCE
-#define ARCHITECTURE    "ALPHA"
-#define SUBARCHITECTURE "ev4"
-#define ARCHCONFIG   "-DEV4 " \
-		     "-DL1_DATA_SIZE=16384 -DL1_DATA_LINESIZE=32 " \
-		     "-DL2_SIZE=2097152 -DL2_LINESIZE=32 " \
-		     "-DDTB_DEFAULT_ENTRIES=32 -DDTB_SIZE=8192 "
-#define LIBNAME   "ev4"
-#define CORENAME  "EV4"
-#endif
-
-#ifdef FORCE_EV5
-#define FORCE
-#define ARCHITECTURE    "ALPHA"
-#define SUBARCHITECTURE "ev5"
-#define ARCHCONFIG   "-DEV5 " \
-		     "-DL1_DATA_SIZE=16384 -DL1_DATA_LINESIZE=32 " \
-		     "-DL2_SIZE=2097152 -DL2_LINESIZE=64 " \
-		     "-DDTB_DEFAULT_ENTRIES=64 -DDTB_SIZE=8192 "
-#define LIBNAME   "ev5"
-#define CORENAME  "EV5"
+#define ARCHITECTURE    "RISCV64"
+#ifdef NO_RV64GV
+#define SUBARCHITECTURE "RISCV64_GENERIC"
+#define SUBDIRNAME      "riscv64"
+#define ARCHCONFIG   "-DRISCV64_GENERIC " \
+       "-DL1_DATA_SIZE=32768 -DL1_DATA_LINESIZE=32 " \
+       "-DL2_SIZE=1048576 -DL2_LINESIZE=32 " \
+       "-DDTB_DEFAULT_ENTRIES=128 -DDTB_SIZE=4096 -DL2_ASSOCIATIVE=4 "
+#define LIBNAME   "riscv64_generic"
+#define CORENAME  "RISCV64_GENERIC"
+#else
+#define SUBARCHITECTURE "C908V"
+#define SUBDIRNAME      "riscv64"
+#define ARCHCONFIG   "-DC908V " \
+       "-DL1_DATA_SIZE=32768 -DL1_DATA_LINESIZE=32 " \
+       "-DL2_SIZE=1048576 -DL2_LINESIZE=32 " \
+       "-DDTB_DEFAULT_ENTRIES=128 -DDTB_SIZE=4096 -DL2_ASSOCIATIVE=4 "
+#define LIBNAME   "c908v"
+#define CORENAME  "C908V"
 #endif
-
-#ifdef FORCE_EV6
-#define FORCE
-#define ARCHITECTURE    "ALPHA"
-#define SUBARCHITECTURE "ev6"
-#define ARCHCONFIG   "-DEV6 " \
-		     "-DL1_DATA_SIZE=32768 -DL1_DATA_LINESIZE=64 " \
-		     "-DL2_SIZE=4194304 -DL2_LINESIZE=64 " \
-		     "-DDTB_DEFAULT_ENTRIES=64 -DDTB_SIZE=8192 "
-#define LIBNAME   "ev6"
-#define CORENAME  "EV6"
+#else
 #endif
 
 #ifdef FORCE_C910V
@@ -1831,7 +1802,7 @@ int main(int argc, char *argv[]){
 #ifdef FORCE
     printf("CORE=%s\n", CORENAME);
 #else
-#if defined(INTEL_AMD) || defined(POWER) || defined(__mips__) || defined(__arm__) || defined(__aarch64__) || defined(ZARCH) || defined(sparc) || defined(__loongarch__) || defined(__riscv) || defined(__alpha__)
+#if defined(INTEL_AMD) || defined(POWER) || defined(__mips__) || defined(__arm__) || defined(__aarch64__) || defined(ZARCH) || defined(sparc) || defined(__loongarch__) || defined(__riscv)
     printf("CORE=%s\n", get_corename());
 #endif
 #endif
diff --git a/kernel/riscv64/KERNEL.C908V b/kernel/riscv64/KERNEL.C908V
new file mode 100644
index 00000000..cce5b9b9
--- /dev/null
+++ b/kernel/riscv64/KERNEL.C908V
@@ -0,0 +1,190 @@
+SAMAXKERNEL  = amax_vector.c
+DAMAXKERNEL  = amax_vector.c
+CAMAXKERNEL  = zamax_vector.c
+ZAMAXKERNEL  = zamax_vector.c
+
+SAMINKERNEL  = amin_vector.c
+DAMINKERNEL  = amin_vector.c
+CAMINKERNEL  = zamin_vector.c
+ZAMINKERNEL  = zamin_vector.c
+
+SMAXKERNEL   = max_vector.c
+DMAXKERNEL   = max_vector.c
+
+SMINKERNEL   = min_vector.c
+DMINKERNEL   = min_vector.c
+
+ISAMAXKERNEL = iamax_vector.c
+IDAMAXKERNEL = iamax_vector.c
+ICAMAXKERNEL = izamax_vector.c
+IZAMAXKERNEL = izamax_vector.c
+
+ISAMINKERNEL = iamin_vector.c
+IDAMINKERNEL = iamin_vector.c
+ICAMINKERNEL = izamin_vector.c
+IZAMINKERNEL = izamin_vector.c
+
+ISMAXKERNEL  = imax_vector.c
+IDMAXKERNEL  = imax_vector.c
+
+ISMINKERNEL  = imin_vector.c
+IDMINKERNEL  = imin_vector.c
+
+SASUMKERNEL  = asum_vector.c
+DASUMKERNEL  = asum_vector.c
+CASUMKERNEL  = zasum_vector.c
+ZASUMKERNEL  = zasum_vector.c
+
+SSUMKERNEL  = ../arm/sum.c
+DSUMKERNEL  = ../arm/sum.c
+CSUMKERNEL  = ../arm/zsum.c
+ZSUMKERNEL  = ../arm/zsum.c
+
+SAXPYKERNEL  = axpy_vector.c
+DAXPYKERNEL  = axpy_vector.c
+CAXPYKERNEL  = zaxpy_vector.c
+ZAXPYKERNEL  = zaxpy_vector.c
+
+SAXPBYKERNEL  = axpby_vector.c
+DAXPBYKERNEL  = axpby_vector.c
+CAXPBYKERNEL  = zaxpby_vector.c
+ZAXPBYKERNEL  = zaxpby_vector.c
+
+SCOPYKERNEL  = copy_vector.c
+DCOPYKERNEL  = copy_vector.c
+CCOPYKERNEL  = zcopy_vector.c
+ZCOPYKERNEL  = zcopy_vector.c
+
+SDOTKERNEL   = dot_vector.c
+DDOTKERNEL   = dot_vector.c
+CDOTKERNEL   = zdot_vector.c
+ZDOTKERNEL   = zdot_vector.c
+
+SNRM2KERNEL  = nrm2_vector.c
+DNRM2KERNEL  = nrm2_vector.c
+CNRM2KERNEL  = znrm2_vector.c
+ZNRM2KERNEL  = znrm2_vector.c
+
+SROTKERNEL   = rot_vector.c
+DROTKERNEL   = rot_vector.c
+CROTKERNEL   = zrot_vector.c
+ZROTKERNEL   = zrot_vector.c
+
+SSCALKERNEL  = scal_vector.c
+DSCALKERNEL  = scal_vector.c
+CSCALKERNEL  = zscal_vector.c
+ZSCALKERNEL  = zscal_vector.c
+
+SSWAPKERNEL  = swap_vector.c
+DSWAPKERNEL  = swap_vector.c
+CSWAPKERNEL  = zswap_vector.c
+ZSWAPKERNEL  = zswap_vector.c
+
+SGEMVNKERNEL = gemv_n_vector.c
+DGEMVNKERNEL = gemv_n_vector.c
+CGEMVNKERNEL = zgemv_n_vector.c
+ZGEMVNKERNEL = zgemv_n_vector.c
+
+SGEMVTKERNEL = gemv_t_vector.c
+DGEMVTKERNEL = gemv_t_vector.c
+CGEMVTKERNEL = zgemv_t_vector.c
+ZGEMVTKERNEL = zgemv_t_vector.c
+
+STRMMKERNEL	= ../generic/trmmkernel_16x4.c
+DTRMMKERNEL	= ../generic/trmmkernel_8x4.c
+CTRMMKERNEL	= ../generic/ztrmmkernel_2x2.c
+ZTRMMKERNEL	= ../generic/ztrmmkernel_2x2.c
+
+SGEMMKERNEL    =  sgemm_kernel_16x4_c908v.c
+ifneq ($(SGEMM_UNROLL_M), $(SGEMM_UNROLL_N))
+SGEMMINCOPY    =  ../generic/gemm_ncopy_$(SGEMM_UNROLL_M).c
+SGEMMITCOPY    =  ../generic/gemm_tcopy_$(SGEMM_UNROLL_M).c
+SGEMMINCOPYOBJ =  sgemm_incopy$(TSUFFIX).$(SUFFIX)
+SGEMMITCOPYOBJ =  sgemm_itcopy$(TSUFFIX).$(SUFFIX)
+endif
+SGEMMONCOPY    =  ../generic/gemm_ncopy_$(SGEMM_UNROLL_N).c
+SGEMMOTCOPY    =  ../generic/gemm_tcopy_$(SGEMM_UNROLL_N).c
+SGEMMONCOPYOBJ =  sgemm_oncopy$(TSUFFIX).$(SUFFIX)
+SGEMMOTCOPYOBJ =  sgemm_otcopy$(TSUFFIX).$(SUFFIX)
+
+DGEMMKERNEL    =  dgemm_kernel_8x4_c908v.c
+ifneq ($(DGEMM_UNROLL_M), $(DGEMM_UNROLL_N))
+DGEMMINCOPY    =  ../generic/gemm_ncopy_$(DGEMM_UNROLL_M).c
+DGEMMITCOPY    =  ../generic/gemm_tcopy_$(DGEMM_UNROLL_M).c
+DGEMMINCOPYOBJ =  dgemm_incopy$(TSUFFIX).$(SUFFIX)
+DGEMMITCOPYOBJ =  dgemm_itcopy$(TSUFFIX).$(SUFFIX)
+endif
+DGEMMONCOPY    =  ../generic/gemm_ncopy_$(DGEMM_UNROLL_N).c
+DGEMMOTCOPY    =  ../generic/gemm_tcopy_$(DGEMM_UNROLL_N).c
+DGEMMONCOPYOBJ =  dgemm_oncopy$(TSUFFIX).$(SUFFIX)
+DGEMMOTCOPYOBJ =  dgemm_otcopy$(TSUFFIX).$(SUFFIX)
+
+CGEMMKERNEL    = ../generic/zgemmkernel_2x2.c
+CGEMMONCOPY    = ../generic/zgemm_ncopy_2.c
+CGEMMOTCOPY    = ../generic/zgemm_tcopy_2.c
+CGEMMONCOPYOBJ =  cgemm_oncopy.o
+CGEMMOTCOPYOBJ =  cgemm_otcopy.o
+
+ZGEMMKERNEL    = ../generic/zgemmkernel_2x2.c
+ZGEMMONCOPY    = ../generic/zgemm_ncopy_2.c
+ZGEMMOTCOPY    = ../generic/zgemm_tcopy_2.c
+ZGEMMONCOPYOBJ =  zgemm_oncopy.o
+ZGEMMOTCOPYOBJ =  zgemm_otcopy.o
+
+STRSMKERNEL_LN	=  ../generic/trsm_kernel_LN.c
+STRSMKERNEL_LT	=  ../generic/trsm_kernel_LT.c
+STRSMKERNEL_RN	=  ../generic/trsm_kernel_RN.c
+STRSMKERNEL_RT	=  ../generic/trsm_kernel_RT.c
+
+DTRSMKERNEL_LN	= ../generic/trsm_kernel_LN.c
+DTRSMKERNEL_LT	= ../generic/trsm_kernel_LT.c
+DTRSMKERNEL_RN	= ../generic/trsm_kernel_RN.c
+DTRSMKERNEL_RT	= ../generic/trsm_kernel_RT.c
+
+CTRSMKERNEL_LN	= ../generic/trsm_kernel_LN.c
+CTRSMKERNEL_LT	= ../generic/trsm_kernel_LT.c
+CTRSMKERNEL_RN	= ../generic/trsm_kernel_RN.c
+CTRSMKERNEL_RT	= ../generic/trsm_kernel_RT.c
+
+ZTRSMKERNEL_LN	= ../generic/trsm_kernel_LN.c
+ZTRSMKERNEL_LT	= ../generic/trsm_kernel_LT.c
+ZTRSMKERNEL_RN	= ../generic/trsm_kernel_RN.c
+ZTRSMKERNEL_RT	= ../generic/trsm_kernel_RT.c
+
+SSYMV_U_KERNEL =  symv_U_vector.c
+SSYMV_L_KERNEL =  symv_L_vector.c
+DSYMV_U_KERNEL =  symv_U_vector.c
+DSYMV_L_KERNEL =  symv_L_vector.c
+CSYMV_U_KERNEL =  ../generic/zsymv_k.c
+CSYMV_L_KERNEL =  ../generic/zsymv_k.c
+ZSYMV_U_KERNEL =  ../generic/zsymv_k.c
+ZSYMV_L_KERNEL =  ../generic/zsymv_k.c
+
+CHEMV_L_KERNEL =  zhemv_LM_vector.c
+CHEMV_M_KERNEL =  zhemv_LM_vector.c
+CHEMV_U_KERNEL =  zhemv_UV_vector.c
+CHEMV_V_KERNEL =  zhemv_UV_vector.c
+ZHEMV_L_KERNEL =  zhemv_LM_vector.c
+ZHEMV_M_KERNEL =  zhemv_LM_vector.c
+ZHEMV_U_KERNEL =  zhemv_UV_vector.c
+ZHEMV_V_KERNEL =  zhemv_UV_vector.c
+
+
+LSAME_KERNEL = ../generic/lsame.c
+
+SCABS_KERNEL	= ../generic/cabs.c
+DCABS_KERNEL	= ../generic/cabs.c
+QCABS_KERNEL	= ../generic/cabs.c
+
+ifndef SGEMM_BETA
+SGEMM_BETA = ../generic/gemm_beta.c
+endif
+ifndef DGEMM_BETA
+DGEMM_BETA = ../generic/gemm_beta.c
+endif
+ifndef CGEMM_BETA
+CGEMM_BETA = ../generic/zgemm_beta.c
+endif
+ifndef ZGEMM_BETA
+ZGEMM_BETA = ../generic/zgemm_beta.c
+endif
diff --git a/kernel/riscv64/dgemm_kernel_8x4_c908v.c b/kernel/riscv64/dgemm_kernel_8x4_c908v.c
new file mode 100644
index 00000000..9eafe997
--- /dev/null
+++ b/kernel/riscv64/dgemm_kernel_8x4_c908v.c
@@ -0,0 +1,978 @@
+#include "common.h"
+#include <riscv_vector.h>
+
+#define KERNEL8x4_I \
+	"addi       t1,    %[PB], 1*8  \n\t"\
+	"addi       t2,    %[PB], 2*8  \n\t"\
+	"addi       t3,    %[PB], 3*8  \n\t"\
+	"fld        ft0,  (%[PB])      \n\t"\
+	"fld        ft1,  (t1)         \n\t"\
+	"fld        ft2,  (t2)         \n\t"\
+	"fld        ft3,  (t3)         \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       t4,    %[PA], 2*8  \n\t"\
+	"addi       t5,    %[PA], 4*8  \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"addi       t6,    %[PA], 6*8  \n\t"\
+	"addi       %[PA], %[PA], 8*8  \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*8  \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"\
+	"vle32.v      v2,   (t5)         \n\t"\
+	"addi       t5,    t5,    8*8  \n\t"\
+	"vle32.v      v3,   (t6)         \n\t"\
+	"addi       t6,    t6,    8*8  \n\t"\
+	"vfmv.v.f   v10,  ft2          \n\t"\
+	"addi       %[PB], %[PB], 4*8  \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 8*8  \n\t"\
+	"vfmv.v.f   v11,  ft3          \n\t"\
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"addi       t1,   t1,     4*8  \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*8  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"addi       t2,   t2,     4*8  \n\t"\
+	"vle32.v      v6,   (t5)         \n\t"\
+	"addi       t5,    t5,    8*8  \n\t"\
+	"vfmacc.vv  v18,  v8,    v2   \n\t"\
+	"addi       t3,   t3,     4*8  \n\t"\
+	"vle32.v      v7,   (t6)         \n\t"\
+	"addi       t6,    t6,    8*8  \n\t"\
+	"vfmacc.vv  v19,  v8,    v3   \n\t"\
+	"fld        ft4,  (%[PB])   \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"fld        ft5,  (t1)        \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"fld        ft6,  (t2)        \n\t"\
+	"vfmacc.vv  v22,  v9,    v2   \n\t"\
+	"fld        ft7,  (t3)        \n\t"\
+	"vfmacc.vv  v23,  v9,    v3   \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"vfmacc.vv  v24,  v10,    v0    \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"\
+	"vfmacc.vv  v25,  v10,    v1    \n\t"\
+	"vfmv.v.f   v14,  ft6          \n\t"\
+	"vfmacc.vv  v26,  v10,    v2    \n\t"\
+	"vfmv.v.f   v15,  ft7          \n\t"\
+	"vfmacc.vv  v27,  v10,    v3    \n\t"\
+        "addi       %[PB], %[PB], 4*8  \n\t"\
+	"vfmacc.vv  v28,  v11,    v0    \n\t"\
+	"addi       t1,   t1,     4*8  \n\t"\
+	"vfmacc.vv  v29,  v11,    v1    \n\t"\
+	"addi       t2,   t2,     4*8  \n\t"\
+	"vfmacc.vv  v30,  v11,    v2    \n\t"\
+	"addi       t3,   t3,     4*8  \n\t"\
+	"vfmacc.vv  v31,  v11,    v3    \n\t"
+
+#define KERNEL8x4_M1 \
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 8*8  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*8  \n\t"\
+	"vfmacc.vv  v18,  v8,    v2   \n\t"\
+	"vle32.v      v6,   (t5)         \n\t"\
+	"addi       t5,    t5,    8*8  \n\t"\
+	"vfmacc.vv  v19,  v8,    v3   \n\t"\
+	"vle32.v      v7,   (t6)         \n\t"\
+	"addi       t6,    t6,    8*8  \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"fld        ft4,  (%[PB])      \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"fld        ft5,  (t1)        \n\t"\
+	"vfmacc.vv  v22,  v9,    v2   \n\t"\
+	"fld        ft6,  (t2)        \n\t"\
+	"vfmacc.vv  v23,  v9,    v3   \n\t"\
+	"fld        ft7,  (t3)        \n\t"\
+        "addi       %[PB], %[PB], 4*8  \n\t"\
+	"vfmacc.vv  v24,  v10,    v0   \n\t"\
+	"addi       t1,   t1,     4*8  \n\t"\
+	"vfmacc.vv  v25,  v10,    v1   \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"vfmacc.vv  v26,  v10,    v2   \n\t"\
+	"addi       t2,   t2,     4*8  \n\t"\
+	"vfmacc.vv  v27,  v10,    v3   \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"\
+	"vfmacc.vv  v28,  v11,    v0   \n\t"\
+	"addi       t3,   t3,     4*8  \n\t"\
+	"vfmacc.vv  v29,  v11,    v1   \n\t"\
+	"vfmv.v.f   v14,  ft6          \n\t"\
+	"vfmacc.vv  v30,  v11,    v2   \n\t"\
+	"vfmacc.vv  v31,  v11,    v3   \n\t"\
+	"vfmv.v.f   v15,  ft7          \n\t"
+
+#define KERNEL8x4_M2 \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 8*8  \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*8  \n\t"\
+	"vfmacc.vv  v18,  v12,    v6   \n\t"\
+	"vle32.v      v2,   (t5)         \n\t"\
+	"addi       t5,    t5,    8*8  \n\t"\
+	"vfmacc.vv  v19,  v12,    v7   \n\t"\
+	"vle32.v      v3,   (t6)         \n\t"\
+	"addi       t6,    t6,    8*8  \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"fld        ft0,  (%[PB])      \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"fld        ft1,  (t1)         \n\t"\
+	"vfmacc.vv  v22,  v13,    v6   \n\t"\
+	"fld        ft2,  (t2)         \n\t"\
+	"vfmacc.vv  v23,  v13,    v7   \n\t"\
+	"fld        ft3,  (t3)         \n\t"\
+        "addi       %[PB], %[PB], 4*8  \n\t"\
+	"vfmacc.vv  v24,  v14,    v4   \n\t"\
+	"addi       t1,   t1,     4*8  \n\t"\
+	"vfmacc.vv  v25,  v14,    v5   \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"vfmacc.vv  v26,  v14,    v6   \n\t"\
+	"addi       t2,   t2,     4*8  \n\t"\
+	"vfmacc.vv  v27,  v14,    v7   \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"\
+	"vfmacc.vv  v28,  v15,    v4   \n\t"\
+	"addi       t3,   t3,     4*8  \n\t"\
+	"vfmacc.vv  v29,  v15,    v5   \n\t"\
+	"vfmv.v.f   v10,  ft2          \n\t"\
+	"vfmacc.vv  v30,  v15,    v6   \n\t"\
+	"vfmacc.vv  v31,  v15,    v7   \n\t"\
+	"vfmv.v.f   v11,  ft3          \n\t"
+
+#define KERNEL8x4_E \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vfmacc.vv  v18,  v12,    v6   \n\t"\
+	"vfmacc.vv  v19,  v12,    v7   \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"vfmacc.vv  v22,  v13,    v6   \n\t"\
+	"vfmacc.vv  v23,  v13,    v7   \n\t"\
+	"vfmacc.vv  v24,  v14,    v4   \n\t"\
+	"vfmacc.vv  v25,  v14,    v5   \n\t"\
+	"vfmacc.vv  v26,  v14,    v6   \n\t"\
+	"vfmacc.vv  v27,  v14,    v7   \n\t"\
+	"vfmacc.vv  v28,  v15,    v4   \n\t"\
+	"vfmacc.vv  v29,  v15,    v5   \n\t"\
+	"vfmacc.vv  v30,  v15,    v6   \n\t"\
+	"vfmacc.vv  v31,  v15,    v7   \n\t"
+
+
+
+
+int CNAME(BLASLONG bm,BLASLONG bn,BLASLONG bk,FLOAT alpha,FLOAT* ba,FLOAT* bb,FLOAT* C,BLASLONG ldc
+#ifdef TRMMKERNEL
+		,BLASLONG offset
+#endif
+		)
+{
+   BLASLONG i,j,k;
+   FLOAT *C0,*C1,*C2,*C3;
+   FLOAT *ptrba,*ptrbb;
+   
+   FLOAT loadb0,loadb1,loadb2,loadb3;
+   FLOAT load0,load1,load2,load3,load4,load5,load6,load7;
+
+   FLOAT res0,res1,res2,res3;
+   FLOAT res4,res5,res6,res7;
+   FLOAT res8,res9,res10,res11;
+   FLOAT res12,res13,res14,res15;
+
+   for (j=0; j<bn/4; j+=1){
+	   C0 = C;
+	   C1 = C0+ldc;
+	   C2 = C1+ldc;
+	   C3 = C2+ldc;
+
+	   ptrba = ba;
+	   for(i=0; i<bm/8; i+=1){
+		   ptrbb = bb;
+		   //t0 for k
+		   //ft0-ft3,ft4-ft7,v8-v15 for B, t1-t3 for PB1-3
+		   //v0-v3,v4-v7 for A, t4-t6 for PA1-3
+		   //v16-v31 for temp C
+		   
+		   asm volatile(
+				"vsetvli    zero, zero, e64,m1 \n\t"
+				"fmv.w.x    ft11, zero         \n\t"
+				"mv         t0,   %[BK]        \n\t"
+				
+				"vfmv.v.f   v16,  ft11         \n\t"
+				"vfmv.v.f   v17,  ft11         \n\t"
+				"vfmv.v.f   v18,  ft11         \n\t"
+				"vfmv.v.f   v19,  ft11         \n\t"
+
+				"vfmv.v.f   v20,  ft11         \n\t"
+				"vfmv.v.f   v21,  ft11         \n\t"
+				"vfmv.v.f   v22,  ft11         \n\t"
+				"vfmv.v.f   v23,  ft11         \n\t"
+
+				"vfmv.v.f   v24,  ft11         \n\t"
+				"vfmv.v.f   v25,  ft11         \n\t"
+				"vfmv.v.f   v26,  ft11         \n\t"
+				"vfmv.v.f   v27,  ft11         \n\t"
+				
+				"vfmv.v.f   v28,  ft11         \n\t"
+				"vfmv.v.f   v29,  ft11         \n\t"
+				"vfmv.v.f   v30,  ft11         \n\t"
+				"vfmv.v.f   v31,  ft11         \n\t"
+				//unloop 8
+				"srli       t0,   %[BK], 3     \n\t"
+				"blez       t0,   M8x4_TAIL    \n\t"
+				
+				//preloop
+				KERNEL8x4_I
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				"addi       t0,   t0, -1       \n\t"
+				"blez       t0,   M8x4_MAINLOOP_TAIL    \n\t"
+				".align 4                      \n\t"
+				"M8x4_MAINLOOP:                \n\t"
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M8x4_MAINLOOP \n\t"
+				
+				"M8x4_MAINLOOP_TAIL:           \n\t"
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_E
+				
+				//tail
+				"M8x4_TAIL:                    \n\t"
+				"andi       t0,   %[BK], 7     \n\t"
+				"blez       t0,   M8x4_SAVERESULT   \n\t"
+
+				"addi       t4,    %[PA], 2*8  \n\t"
+				"addi       t5,    %[PA], 4*8  \n\t"
+				"addi       t6,    %[PA], 6*8  \n\t"
+				"addi       t1,    %[PB], 1*8  \n\t"
+				"addi       t2,    %[PB], 2*8  \n\t"
+				"addi       t3,    %[PB], 3*8  \n\t"
+
+				".align 4                      \n\t"
+				"M8x4_TAILLOOP:                \n\t"
+				"fld        ft0,  (%[PB])      \n\t"
+				"addi       %[PB], %[PB], 4*8  \n\t"
+				"vle32.v      v0,   (%[PA])      \n\t"
+				"add        %[PA], %[PA], 8*8  \n\t"
+				"vle32.v      v1,   (t4)         \n\t"
+				"addi       t4,    t4,    8*8  \n\t"
+
+				"vfmv.v.f   v8,   ft0          \n\t"
+				"fld        ft1,  (t1)         \n\t"
+				"addi       t1,   t1,     4*8  \n\t"
+				"vle32.v      v2,   (t5)         \n\t"
+				"addi       t5,    t5,    8*8  \n\t"
+				"vle32.v      v3,   (t6)         \n\t"
+				"addi       t6,    t6,    8*8  \n\t"
+
+				"vfmacc.vv  v16,  v8,    v0    \n\t"
+				"fld        ft2,  (t2)         \n\t"
+				"addi       t2,   t2,     4*8  \n\t"
+				"vfmacc.vv  v17,  v8,    v1    \n\t"
+				"vfmacc.vv  v18,  v8,    v2    \n\t"
+				"vfmv.v.f   v9,   ft1          \n\t"
+				"vfmacc.vv  v19,  v8,    v3    \n\t"
+								
+
+				"vfmacc.vv  v20,  v9,    v0    \n\t"
+				"fld        ft3,  (t3)         \n\t"
+				"addi       t3,   t3,     4*8  \n\t"
+				"vfmacc.vv  v21,  v9,    v1    \n\t"
+				"vfmacc.vv  v22,  v9,    v2    \n\t"
+				"vfmv.v.f   v10,  ft2          \n\t"
+				"vfmacc.vv  v23,  v9,    v3    \n\t"
+
+				"vfmv.v.f   v11,  ft3          \n\t"
+				"vfmacc.vv  v24,  v10,    v0    \n\t"
+				"vfmacc.vv  v25,  v10,    v1    \n\t"
+				"vfmacc.vv  v26,  v10,    v2    \n\t"
+				"vfmacc.vv  v27,  v10,    v3    \n\t"
+
+				"vfmacc.vv  v28,  v11,    v0    \n\t"
+				"vfmacc.vv  v29,  v11,    v1    \n\t"
+				"vfmacc.vv  v30,  v11,    v2    \n\t"
+				"vfmacc.vv  v31,  v11,    v3    \n\t"
+
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M8x4_TAILLOOP \n\t"
+				
+				//Save result
+				//load C
+				"M8x4_SAVERESULT:              \n\t"
+				//use v8 to store alpha
+				"vfmv.v.f   v8,   %[ALPHA]     \n\t"
+				"vle32.v      v0,   (%[C0])      \n\t"
+				"addi       t4,   %[C0], 2*8   \n\t"
+				"vle32.v      v1,   (%[C1])      \n\t"
+				"addi       t5,   %[C1], 2*8   \n\t"
+				"vle32.v      v2,   (%[C2])      \n\t"
+				"addi       t6,   %[C2], 2*8   \n\t"
+				"vle32.v      v3,   (%[C3])      \n\t"
+				"addi       t3,   %[C3], 2*8   \n\t"
+				
+				//Multiply Alpha
+				"vfmacc.vv  v0,   v8, v16 \n\t"
+				"vle32.v      v4,   (t4)          \n\t"
+				"vfmacc.vv  v1,   v8, v20 \n\t"
+				"vle32.v      v5,   (t5)          \n\t"
+				"vfmacc.vv  v2,   v8, v24 \n\t"
+				"vle32.v      v6,   (t6)          \n\t"
+				"vfmacc.vv  v3,   v8, v28 \n\t"
+				"vle32.v      v7,   (t3)          \n\t"
+
+				"vfmacc.vv  v4,   v8, v17 \n\t"
+				"vse32.v      v0,   (%[C0])      \n\t"
+				"add        %[C0], %[C0], 4*8  \n\t"
+				"vfmacc.vv  v5,   v8, v21 \n\t"
+				"vse32.v      v1,   (%[C1])      \n\t"
+				"add        %[C1], %[C1], 4*8  \n\t"
+				
+				"vfmacc.vv  v6,   v8, v25 \n\t"
+				"vse32.v      v2,   (%[C2])      \n\t"
+				"add        %[C2], %[C2], 4*8  \n\t"
+
+				"vfmacc.vv  v7,   v8, v29 \n\t"
+				"vse32.v      v3,   (%[C3])      \n\t"
+				"add        %[C3], %[C3], 4*8  \n\t"
+
+				"vle32.v      v0,   (%[C0])      \n\t"
+				"vse32.v      v4,   (t4)         \n\t"
+				"add        t4,   t4,     4*8  \n\t"
+				
+				"vle32.v      v1,   (%[C1])      \n\t"
+				"vse32.v      v5,   (t5)         \n\t"
+				"add        t5,   t5,     4*8  \n\t"
+
+				"vle32.v      v2,   (%[C2])      \n\t"
+				"vse32.v      v6,   (t6)         \n\t"
+				"add        t6,   t6,     4*8  \n\t"
+
+				"vle32.v      v3,   (%[C3])      \n\t"
+				"vse32.v      v7,   (t3)         \n\t"
+				"add        t3,   t3,     4*8  \n\t"
+
+
+				"vfmacc.vv  v0,   v8, v18 \n\t"
+				"vle32.v      v4,   (t4)          \n\t"
+				"vfmacc.vv  v1,   v8, v22 \n\t"
+				"vle32.v      v5,   (t5)          \n\t"
+				"vfmacc.vv  v2,   v8, v26 \n\t"
+				"vle32.v      v6,   (t6)          \n\t"
+				"vfmacc.vv  v3,   v8, v30 \n\t"
+				"vle32.v      v7,   (t3)          \n\t"
+
+				"vfmacc.vv  v4,   v8, v19 \n\t"
+				"vse32.v      v0,   (%[C0])      \n\t"
+				"add        %[C0], %[C0], 4*8  \n\t"
+
+				"vfmacc.vv  v5,   v8, v23 \n\t"
+				"vse32.v      v1,   (%[C1])      \n\t"
+				"add        %[C1], %[C1], 4*8  \n\t"
+
+				"vfmacc.vv  v6,   v8, v27 \n\t"
+				"vse32.v      v2,   (%[C2])      \n\t"
+				"add        %[C2], %[C2], 4*8  \n\t"
+
+				"vfmacc.vv  v7,   v8, v31 \n\t"
+				"vse32.v      v3,   (%[C3])      \n\t"
+				"add        %[C3], %[C3], 4*8  \n\t"
+
+				"vse32.v      v4,   (t4)         \n\t"
+				"vse32.v      v5,   (t5)         \n\t"
+				"vse32.v      v6,   (t6)         \n\t"
+				"vse32.v      v7,   (t3)         \n\t"
+				"M8x4_END:                     \n\t"
+				
+				:[C0]"+r"(C0),[C1]"+r"(C1),[C2]"+r"(C2),[C3]"+r"(C3),
+				 [PA]"+r"(ptrba), [PB]"+r"(ptrbb)
+				:[ALPHA]"f"(alpha), [BK]"r"(bk)
+				:"cc", "t0", "t4","t5","t6","t3","t1","t2",
+				 "ft11", "ft0", "ft1", "ft2","ft3","ft4", "ft5", "ft6","ft7",
+				 "v0", "v1", "v2", "v3","v4", "v5", "v6", "v7",
+				 "v8", "v9", "v10", "v11","v12", "v13", "v14", "v15",
+				 "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23",
+				 "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31");
+	   }
+	   if(bm&4){
+		   ptrbb = bb;
+      		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   res4 = 0;
+		   res5 = 0;
+		   res6 = 0;
+		   res7 = 0;
+		   res8 = 0;
+		   res9 = 0;
+		   res10 = 0;
+		   res11 = 0;
+		   res12 = 0;
+		   res13 = 0;
+		   res14 = 0;
+		   res15 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+			   load2 = ptrba[2];
+			   load3 = ptrba[3];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+			   res2 = res2 + load2 * loadb0;
+			   res3 = res3 + load3 * loadb0;
+
+			   res4 = res4 + load0 * loadb1;
+			   res5 = res5 + load1 * loadb1;
+			   res6 = res6 + load2 * loadb1;
+			   res7 = res7 + load3 * loadb1;
+
+			   loadb2 = ptrbb[2];
+			   loadb3 = ptrbb[3];
+			   
+   			   res8 = res8 + load0 * loadb2;
+			   res9 = res9 + load1 * loadb2;
+			   res10 = res10 + load2 * loadb2;
+			   res11 = res11 + load3 * loadb2;
+
+			   res12 = res12 + load0 * loadb3;
+			   res13 = res13 + load1 * loadb3;
+			   res14 = res14 + load2 * loadb3;
+			   res15 = res15 + load3 * loadb3;
+
+			   ptrba += 4;
+			   ptrbb += 4;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+		   res6 = res6 * alpha;
+		   res7 = res7 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+		   res10 = res10 * alpha;
+		   res11 = res11 * alpha;
+		   res12 = res12 * alpha;
+		   res13 = res13 * alpha;
+		   res14 = res14 * alpha;
+		   res15 = res15 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   
+		   C1[0] += res4;
+		   C1[1] += res5;
+		   C1[2] += res6;
+		   C1[3] += res7;
+
+   		   C2[0] += res8;
+		   C2[1] += res9;
+		   C2[2] += res10;
+		   C2[3] += res11;
+		   
+		   C3[0] += res12;
+		   C3[1] += res13;
+		   C3[2] += res14;
+		   C3[3] += res15;
+
+		   C0 += 4;
+		   C1 += 4;
+		   C2 += 4;
+		   C3 += 4;
+	   }
+   	   if(bm&2){
+		   ptrbb = bb;
+		   
+       		   res0 = 0;
+		   res1 = 0;
+		   
+		   res4 = 0;
+		   res5 = 0;
+		   
+		   res8 = 0;
+		   res9 = 0;
+		   
+		   res12 = 0;
+		   res13 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+
+			   res4 = res4 + load0 * loadb1;
+			   res5 = res5 + load1 * loadb1;
+
+			   loadb2 = ptrbb[2];
+			   loadb3 = ptrbb[3];
+			   
+   			   res8 = res8 + load0 * loadb2;
+			   res9 = res9 + load1 * loadb2;
+
+			   res12 = res12 + load0 * loadb3;
+			   res13 = res13 + load1 * loadb3;
+
+			   ptrba += 2;
+			   ptrbb += 4;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+
+		   res12 = res12 * alpha;
+		   res13 = res13 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+
+		   C1[0] += res4;
+		   C1[1] += res5;
+
+   		   C2[0] += res8;
+		   C2[1] += res9;
+		   
+		   C3[0] += res12;
+		   C3[1] += res13;
+
+		   C0 += 2;
+		   C1 += 2;
+		   C2 += 2;
+		   C3 += 2;
+	   }
+	   if(bm&1){
+		   ptrbb = bb;
+		   		   
+       		   res0 = 0;
+		   
+		   res4 = 0;
+		   
+		   res8 = 0;
+		   
+		   res12 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+				   
+			   res0 = res0 + load0 * loadb0;
+
+			   res4 = res4 + load0 * loadb1;
+
+			   loadb2 = ptrbb[2];
+			   loadb3 = ptrbb[3];
+			   
+   			   res8 = res8 + load0 * loadb2;
+
+			   res12 = res12 + load0 * loadb3;
+
+			   ptrba += 1;
+			   ptrbb += 4;
+		   }
+		   
+      		   res0 = res0 * alpha;
+
+		   res4 = res4 * alpha;
+
+       		   res8 = res8 * alpha;
+
+		   res12 = res12 * alpha;
+
+		   C0[0] += res0;
+		   C1[0] += res4;
+   		   C2[0] += res8;
+		   C3[0] += res12;
+
+		   C0 += 1;
+		   C1 += 1;
+		   C2 += 1;
+		   C3 += 1;
+	   }
+	   
+	   k = bk<<2;
+	   bb = bb+k;
+	   i = ldc<<2;
+	   C = C+i;
+   }
+   
+   if(bn&2){
+	   C0 = C;
+	   C1 = C0+ldc;
+
+	   ptrba = ba;
+	   for(i=0; i<bm/8; i+=1){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   res4 = 0;
+		   res5 = 0;
+		   res6 = 0;
+		   res7 = 0;
+		   res8 = 0;
+		   res9 = 0;
+		   res10 = 0;
+		   res11 = 0;
+		   res12 = 0;
+		   res13 = 0;
+		   res14 = 0;
+		   res15 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+			   load2 = ptrba[2];
+			   load3 = ptrba[3];
+			   load4 = ptrba[4];
+			   load5 = ptrba[5];
+			   load6 = ptrba[6];
+			   load7 = ptrba[7];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+			   res2 = res2 + load2 * loadb0;
+			   res3 = res3 + load3 * loadb0;
+
+			   res4 = res4 + load4 * loadb0;
+			   res5 = res5 + load5 * loadb0;
+			   res6 = res6 + load6 * loadb0;
+			   res7 = res7 + load7 * loadb0;
+
+   			   res8 = res8 + load0 * loadb1;
+			   res9 = res9 + load1 * loadb1;
+			   res10 = res10 + load2 * loadb1;
+			   res11 = res11 + load3 * loadb1;
+
+			   res12 = res12 + load4 * loadb1;
+			   res13 = res13 + load5 * loadb1;
+			   res14 = res14 + load6 * loadb1;
+			   res15 = res15 + load7 * loadb1;
+
+			   ptrba += 8;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+		   res6 = res6 * alpha;
+		   res7 = res7 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+		   res10 = res10 * alpha;
+		   res11 = res11 * alpha;
+		   res12 = res12 * alpha;
+		   res13 = res13 * alpha;
+		   res14 = res14 * alpha;
+		   res15 = res15 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   C0[4] += res4;
+		   C0[5] += res5;
+		   C0[6] += res6;
+		   C0[7] += res7;
+
+   		   C1[0] += res8;
+		   C1[1] += res9;
+		   C1[2] += res10;
+		   C1[3] += res11;
+		   C1[4] += res12;
+		   C1[5] += res13;
+		   C1[6] += res14;
+		   C1[7] += res15;
+
+		   C0 += 8;
+		   C1 += 8;
+	   }
+	   if(bm&4){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   
+		   res8 = 0;
+		   res9 = 0;
+		   res10 = 0;
+		   res11 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+			   load2 = ptrba[2];
+			   load3 = ptrba[3];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+			   res2 = res2 + load2 * loadb0;
+			   res3 = res3 + load3 * loadb0;
+
+   			   res8 = res8 + load0 * loadb1;
+			   res9 = res9 + load1 * loadb1;
+			   res10 = res10 + load2 * loadb1;
+			   res11 = res11 + load3 * loadb1;
+
+			   ptrba += 4;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+		   res10 = res10 * alpha;
+		   res11 = res11 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+
+   		   C1[0] += res8;
+		   C1[1] += res9;
+		   C1[2] += res10;
+		   C1[3] += res11;
+
+		   C0 += 4;
+		   C1 += 4;
+	   }
+   	   if(bm&2){
+		   ptrbb = bb;
+      		   res0 = 0;
+		   res1 = 0;
+		   
+		   res8 = 0;
+		   res9 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+
+   			   res8 = res8 + load0 * loadb1;
+			   res9 = res9 + load1 * loadb1;
+
+			   ptrba += 2;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+
+   		   C1[0] += res8;
+		   C1[1] += res9;
+		   
+		   C0 += 2;
+		   C1 += 2;
+	   }
+	   if(bm&1){
+		   ptrbb = bb;
+       		   res0 = 0;
+		   res8 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+			   load0 = ptrba[0];
+				   
+			   res0 = res0 + load0 * loadb0;
+   			   res8 = res8 + load0 * loadb1;
+			   ptrba += 1;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+       		   res8 = res8 * alpha;
+
+		   C0[0] += res0;
+   		   C1[0] += res8;
+		   
+		   C0 += 1;
+		   C1 += 1;
+	   }
+	   k = bk<<1;
+	   bb = bb+k;
+	   i = ldc<<1;
+	   C = C+i;
+   }
+
+   if (bn&1){
+	   C0 = C;
+	   ptrba = ba;
+	   for(i=0; i<bm/8; i+=1){
+		   ptrbb = bb;
+		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   res4 = 0;
+		   res5 = 0;
+		   res6 = 0;
+		   res7 = 0;
+
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   res1 = res1 + ptrba[1] * loadb0;
+			   res2 = res2 + ptrba[2] * loadb0;
+			   res3 = res3 + ptrba[3] * loadb0;
+
+			   res4 = res4 + ptrba[4] * loadb0;
+			   res5 = res5 + ptrba[5] * loadb0;
+			   res6 = res6 + ptrba[6] * loadb0;
+			   res7 = res7 + ptrba[7] * loadb0;
+			   
+			   ptrba += 8;
+			   ptrbb += 1;
+		   }
+   		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+		   res6 = res6 * alpha;
+		   res7 = res7 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   C0[4] += res4;
+		   C0[5] += res5;
+		   C0[6] += res6;
+		   C0[7] += res7;
+		   
+		   C0 += 8;
+	   }
+	   if(bm&4){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   res1 = res1 + ptrba[1] * loadb0;
+			   res2 = res2 + ptrba[2] * loadb0;
+			   res3 = res3 + ptrba[3] * loadb0;
+
+			   ptrba += 4;
+			   ptrbb += 1;
+		   }
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   
+		   C0 += 4;
+	   }
+   	   if(bm&2){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   res1 = res1 + ptrba[1] * loadb0;
+
+			   ptrba += 2;
+			   ptrbb += 1;
+		   }
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   
+		   C0 += 2;
+	   }
+	   if(bm&1){
+   		   ptrbb = bb;
+   		   res0 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   ptrba += 1;
+			   ptrbb += 1;
+		   }
+      		   res0 = res0 * alpha;
+		   C0[0] += res0;
+		   C0 += 1;
+	   }
+
+	   k = bk;
+	   bb = bb+k;
+	   C = C+ldc;
+   }
+   return 0;
+}
+
diff --git a/kernel/riscv64/sgemm_kernel_16x4_c908v.c b/kernel/riscv64/sgemm_kernel_16x4_c908v.c
new file mode 100644
index 00000000..c1452eb5
--- /dev/null
+++ b/kernel/riscv64/sgemm_kernel_16x4_c908v.c
@@ -0,0 +1,1633 @@
+#include "common.h"
+#include <riscv_vector.h>
+
+#define KERNEL16x4_I \
+	"addi       t1,    %[PB], 1*4  \n\t"\
+	"addi       t2,    %[PB], 2*4  \n\t"\
+	"addi       t3,    %[PB], 3*4  \n\t"\
+	"flw        ft0,  (%[PB])      \n\t"\
+	"flw        ft1,  (t1)         \n\t"\
+	"flw        ft2,  (t2)         \n\t"\
+	"flw        ft3,  (t3)         \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       t4,    %[PA], 4*4  \n\t"\
+	"addi       t5,    %[PA], 8*4  \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"addi       t6,    %[PA], 12*4  \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"\
+	"vle32.v      v2,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vle32.v      v3,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"vfmv.v.f   v10,  ft2          \n\t"\
+	"addi       %[PB], %[PB], 4*4  \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vfmv.v.f   v11,  ft3          \n\t"\
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"vle32.v      v6,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vfmacc.vv  v18,  v8,    v2   \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"\
+	"vle32.v      v7,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"vfmacc.vv  v19,  v8,    v3   \n\t"\
+	"flw        ft4,  (%[PB])   \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"flw        ft5,  (t1)        \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"flw        ft6,  (t2)        \n\t"\
+	"vfmacc.vv  v22,  v9,    v2   \n\t"\
+	"flw        ft7,  (t3)        \n\t"\
+	"vfmacc.vv  v23,  v9,    v3   \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"vfmacc.vv  v24,  v10,    v0    \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"\
+	"vfmacc.vv  v25,  v10,    v1    \n\t"\
+	"vfmv.v.f   v14,  ft6          \n\t"\
+	"vfmacc.vv  v26,  v10,    v2    \n\t"\
+	"vfmv.v.f   v15,  ft7          \n\t"\
+	"vfmacc.vv  v27,  v10,    v3    \n\t"\
+        "addi       %[PB], %[PB], 4*4  \n\t"\
+	"vfmacc.vv  v28,  v11,    v0    \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vfmacc.vv  v29,  v11,    v1    \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"vfmacc.vv  v30,  v11,    v2    \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"\
+	"vfmacc.vv  v31,  v11,    v3    \n\t"
+
+#define KERNEL16x4_M1 \
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmacc.vv  v18,  v8,    v2   \n\t"\
+	"vle32.v      v6,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vfmacc.vv  v19,  v8,    v3   \n\t"\
+	"vle32.v      v7,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"flw        ft4,  (%[PB])      \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"flw        ft5,  (t1)        \n\t"\
+	"vfmacc.vv  v22,  v9,    v2   \n\t"\
+	"flw        ft6,  (t2)        \n\t"\
+	"vfmacc.vv  v23,  v9,    v3   \n\t"\
+	"flw        ft7,  (t3)        \n\t"\
+        "addi       %[PB], %[PB], 4*4  \n\t"\
+	"vfmacc.vv  v24,  v10,    v0   \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vfmacc.vv  v25,  v10,    v1   \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"vfmacc.vv  v26,  v10,    v2   \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"vfmacc.vv  v27,  v10,    v3   \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"\
+	"vfmacc.vv  v28,  v11,    v0   \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"\
+	"vfmacc.vv  v29,  v11,    v1   \n\t"\
+	"vfmv.v.f   v14,  ft6          \n\t"\
+	"vfmacc.vv  v30,  v11,    v2   \n\t"\
+	"vfmacc.vv  v31,  v11,    v3   \n\t"\
+	"vfmv.v.f   v15,  ft7          \n\t"
+
+#define KERNEL16x4_M2 \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmacc.vv  v18,  v12,    v6   \n\t"\
+	"vle32.v      v2,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vfmacc.vv  v19,  v12,    v7   \n\t"\
+	"vle32.v      v3,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"flw        ft0,  (%[PB])      \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"flw        ft1,  (t1)         \n\t"\
+	"vfmacc.vv  v22,  v13,    v6   \n\t"\
+	"flw        ft2,  (t2)         \n\t"\
+	"vfmacc.vv  v23,  v13,    v7   \n\t"\
+	"flw        ft3,  (t3)         \n\t"\
+        "addi       %[PB], %[PB], 4*4  \n\t"\
+	"vfmacc.vv  v24,  v14,    v4   \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vfmacc.vv  v25,  v14,    v5   \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"vfmacc.vv  v26,  v14,    v6   \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"vfmacc.vv  v27,  v14,    v7   \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"\
+	"vfmacc.vv  v28,  v15,    v4   \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"\
+	"vfmacc.vv  v29,  v15,    v5   \n\t"\
+	"vfmv.v.f   v10,  ft2          \n\t"\
+	"vfmacc.vv  v30,  v15,    v6   \n\t"\
+	"vfmacc.vv  v31,  v15,    v7   \n\t"\
+	"vfmv.v.f   v11,  ft3          \n\t"
+
+#define KERNEL16x4_E \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vfmacc.vv  v18,  v12,    v6   \n\t"\
+	"vfmacc.vv  v19,  v12,    v7   \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"vfmacc.vv  v22,  v13,    v6   \n\t"\
+	"vfmacc.vv  v23,  v13,    v7   \n\t"\
+	"vfmacc.vv  v24,  v14,    v4   \n\t"\
+	"vfmacc.vv  v25,  v14,    v5   \n\t"\
+	"vfmacc.vv  v26,  v14,    v6   \n\t"\
+	"vfmacc.vv  v27,  v14,    v7   \n\t"\
+	"vfmacc.vv  v28,  v15,    v4   \n\t"\
+	"vfmacc.vv  v29,  v15,    v5   \n\t"\
+	"vfmacc.vv  v30,  v15,    v6   \n\t"\
+	"vfmacc.vv  v31,  v15,    v7   \n\t"
+
+
+#define KERNEL8x4_I \
+	"addi       t1,    %[PB], 1*4  \n\t"\
+	"addi       t2,    %[PB], 2*4  \n\t"\
+	"addi       t3,    %[PB], 3*4  \n\t"\
+	"flw        ft0,  (%[PB])      \n\t"\
+	"flw        ft1,  (t1)         \n\t"\
+	"flw        ft2,  (t2)         \n\t"\
+	"flw        ft3,  (t3)         \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       t4,    %[PA], 4*4  \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"addi       %[PA], %[PA], 8*4  \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*4  \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"\
+	"vfmv.v.f   v10,  ft2          \n\t"\
+	"addi       %[PB], %[PB], 4*4  \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 8*4  \n\t"\
+	"vfmv.v.f   v11,  ft3          \n\t"\
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*4  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"flw        ft4,  (%[PB])   \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"flw        ft5,  (t1)        \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"flw        ft6,  (t2)        \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"flw        ft7,  (t3)        \n\t"\
+	"vfmacc.vv  v24,  v10,    v0    \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"\
+	"vfmacc.vv  v25,  v10,    v1    \n\t"\
+	"vfmv.v.f   v14,  ft6          \n\t"\
+        "addi       %[PB], %[PB], 4*4  \n\t"\
+	"vfmv.v.f   v15,  ft7          \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vfmacc.vv  v28,  v11,    v0    \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"vfmacc.vv  v29,  v11,    v1    \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"
+
+
+#define KERNEL8x4_M1 \
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 8*4  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*4  \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"flw        ft4,  (%[PB])      \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"flw        ft5,  (t1)        \n\t"\
+        "addi       %[PB], %[PB], 4*4  \n\t"\
+	"flw        ft6,  (t2)        \n\t"\
+	"vfmacc.vv  v24,  v10,    v0   \n\t"\
+	"flw        ft7,  (t3)        \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vfmacc.vv  v25,  v10,    v1   \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"\
+	"vfmacc.vv  v28,  v11,    v0   \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"\
+	"vfmacc.vv  v29,  v11,    v1   \n\t"\
+	"vfmv.v.f   v14,  ft6          \n\t"\
+	"vfmv.v.f   v15,  ft7          \n\t"
+
+#define KERNEL8x4_M2 \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 8*4  \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    8*4  \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"flw        ft0,  (%[PB])      \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"flw        ft1,  (t1)         \n\t"\
+        "addi       %[PB], %[PB], 4*4  \n\t"\
+	"flw        ft2,  (t2)         \n\t"\
+	"vfmacc.vv  v24,  v14,    v4   \n\t"\
+	"flw        ft3,  (t3)         \n\t"\
+	"addi       t1,   t1,     4*4  \n\t"\
+	"vfmacc.vv  v25,  v14,    v5   \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"addi       t2,   t2,     4*4  \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"\
+	"vfmacc.vv  v28,  v15,    v4   \n\t"\
+	"addi       t3,   t3,     4*4  \n\t"\
+	"vfmacc.vv  v29,  v15,    v5   \n\t"\
+	"vfmv.v.f   v10,  ft2          \n\t"\
+	"vfmv.v.f   v11,  ft3          \n\t"
+
+#define KERNEL8x4_E \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"vfmacc.vv  v24,  v14,    v4   \n\t"\
+	"vfmacc.vv  v25,  v14,    v5   \n\t"\
+	"vfmacc.vv  v28,  v15,    v4   \n\t"\
+	"vfmacc.vv  v29,  v15,    v5   \n\t"
+
+
+#define KERNEL16x2_I \
+	"addi       t1,    %[PB], 1*4  \n\t"\
+	"flw        ft0,  (%[PB])      \n\t"\
+	"flw        ft1,  (t1)         \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       t4,    %[PA], 4*4  \n\t"\
+	"addi       t5,    %[PA], 8*4  \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"addi       t6,    %[PA], 12*4  \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"\
+	"vle32.v      v2,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vle32.v      v3,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"addi       %[PB], %[PB], 2*4  \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"addi       t1,   t1,     2*4  \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"vle32.v      v6,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vfmacc.vv  v18,  v8,    v2   \n\t"\
+	"vle32.v      v7,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"vfmacc.vv  v19,  v8,    v3   \n\t"\
+	"flw        ft4,  (%[PB])   \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"flw        ft5,  (t1)        \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"addi       %[PB], %[PB], 2*4  \n\t"\
+	"vfmacc.vv  v22,  v9,    v2   \n\t"\
+	"addi       t1,   t1,     2*4  \n\t"\
+	"vfmacc.vv  v23,  v9,    v3   \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"
+	
+
+#define KERNEL16x2_M1 \
+	"vfmacc.vv  v16,  v8,    v0   \n\t"\
+	"vle32.v      v4,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vfmacc.vv  v17,  v8,    v1   \n\t"\
+	"vle32.v      v5,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmacc.vv  v18,  v8,    v2   \n\t"\
+	"vle32.v      v6,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vfmacc.vv  v19,  v8,    v3   \n\t"\
+	"vle32.v      v7,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"flw        ft4,  (%[PB])      \n\t"\
+	"vfmacc.vv  v20,  v9,    v0   \n\t"\
+	"flw        ft5,  (t1)        \n\t"\
+	"vfmacc.vv  v21,  v9,    v1   \n\t"\
+	"vfmv.v.f   v12,  ft4          \n\t"\
+	"vfmacc.vv  v22,  v9,    v2   \n\t"\
+	"addi       t1,   t1,     2*4  \n\t"\
+	"vfmacc.vv  v23,  v9,    v3   \n\t"\
+	"addi       %[PB], %[PB], 2*4  \n\t"\
+	"vfmv.v.f   v13,  ft5          \n\t"
+
+
+#define KERNEL16x2_M2 \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vle32.v      v0,   (%[PA])      \n\t"\
+	"addi       %[PA], %[PA], 16*4  \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vle32.v      v1,   (t4)         \n\t"\
+	"addi       t4,    t4,    16*4  \n\t"\
+	"vfmacc.vv  v18,  v12,    v6   \n\t"\
+	"vle32.v      v2,   (t5)         \n\t"\
+	"addi       t5,    t5,    16*4  \n\t"\
+	"vfmacc.vv  v19,  v12,    v7   \n\t"\
+	"vle32.v      v3,   (t6)         \n\t"\
+	"addi       t6,    t6,    16*4  \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"flw        ft0,  (%[PB])      \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"flw        ft1,  (t1)         \n\t"\
+	"vfmacc.vv  v22,  v13,    v6   \n\t"\
+	"vfmv.v.f   v8,   ft0          \n\t"\
+	"vfmacc.vv  v23,  v13,    v7   \n\t"\
+        "addi       %[PB], %[PB], 2*4  \n\t"\
+	"addi       t1,   t1,     2*4  \n\t"\
+	"vfmv.v.f   v9,   ft1          \n\t"
+
+
+#define KERNEL16x2_E \
+	"vfmacc.vv  v16,  v12,    v4   \n\t"\
+	"vfmacc.vv  v17,  v12,    v5   \n\t"\
+	"vfmacc.vv  v18,  v12,    v6   \n\t"\
+	"vfmacc.vv  v19,  v12,    v7   \n\t"\
+	"vfmacc.vv  v20,  v13,    v4   \n\t"\
+	"vfmacc.vv  v21,  v13,    v5   \n\t"\
+	"vfmacc.vv  v22,  v13,    v6   \n\t"\
+	"vfmacc.vv  v23,  v13,    v7   \n\t"
+
+
+int CNAME(BLASLONG bm,BLASLONG bn,BLASLONG bk,FLOAT alpha,FLOAT* ba,FLOAT* bb,FLOAT* C,BLASLONG ldc
+#ifdef TRMMKERNEL
+		,BLASLONG offset
+#endif
+		)
+{
+   BLASLONG i,j,k;
+   FLOAT *C0,*C1,*C2,*C3;
+   FLOAT *ptrba,*ptrbb, *tmpc;
+   
+   FLOAT loadb0,loadb1,loadb2,loadb3;
+   FLOAT load0,load1,load2,load3,load4,load5,load6,load7;
+
+   FLOAT res0,res1,res2,res3;
+   FLOAT res4,res5,res6,res7;
+   FLOAT res8,res9,res10,res11;
+   FLOAT res12,res13,res14,res15;
+
+
+   for (j=0; j<bn/4; j+=1){
+	   C0 = C;
+	   C1 = C0+ldc;
+	   C2 = C1+ldc;
+	   C3 = C2+ldc;
+
+	   ptrba = ba;
+	   for(i=0; i<bm/16; i+=1){
+		   ptrbb = bb;
+		   //t0 for k
+		   //ft0-ft3,ft4-ft7,v8-v15 for B, t1-t3 for PB1-3
+		   //v0-v3,v4-v7 for A, t4-t6 for PA1-3
+		   //v16-v31 for temp C
+		   
+		   asm volatile(
+				"vsetvli    zero, zero, e32,m1 \n\t"
+				"fmv.w.x    ft11, zero         \n\t"
+				"mv         t0,   %[BK]        \n\t"
+				
+				"vfmv.v.f   v16,  ft11         \n\t"
+				"vfmv.v.f   v17,  ft11         \n\t"
+				"vfmv.v.f   v18,  ft11         \n\t"
+				"vfmv.v.f   v19,  ft11         \n\t"
+
+				"vfmv.v.f   v20,  ft11         \n\t"
+				"vfmv.v.f   v21,  ft11         \n\t"
+				"vfmv.v.f   v22,  ft11         \n\t"
+				"vfmv.v.f   v23,  ft11         \n\t"
+
+				"vfmv.v.f   v24,  ft11         \n\t"
+				"vfmv.v.f   v25,  ft11         \n\t"
+				"vfmv.v.f   v26,  ft11         \n\t"
+				"vfmv.v.f   v27,  ft11         \n\t"
+				
+				"vfmv.v.f   v28,  ft11         \n\t"
+				"vfmv.v.f   v29,  ft11         \n\t"
+				"vfmv.v.f   v30,  ft11         \n\t"
+				"vfmv.v.f   v31,  ft11         \n\t"
+				//unloop 8
+				"srli       t0,   %[BK], 3     \n\t"
+				"blez       t0,   M16x4_TAIL    \n\t"
+				
+				//preloop
+				KERNEL16x4_I
+				KERNEL16x4_M2
+				KERNEL16x4_M1
+				KERNEL16x4_M2
+				"addi       t0,   t0, -1       \n\t"
+				"blez       t0,   M16x4_MAINLOOP_TAIL    \n\t"
+				".align 4                      \n\t"
+				"M16x4_MAINLOOP:                \n\t"
+				KERNEL16x4_M1
+				KERNEL16x4_M2
+				KERNEL16x4_M1
+				KERNEL16x4_M2
+				KERNEL16x4_M1
+				KERNEL16x4_M2
+				KERNEL16x4_M1
+				KERNEL16x4_M2
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M16x4_MAINLOOP \n\t"
+				
+				"M16x4_MAINLOOP_TAIL:           \n\t"
+				KERNEL16x4_M1
+				KERNEL16x4_M2
+				KERNEL16x4_M1
+				KERNEL16x4_E
+				
+				//tail
+				"M16x4_TAIL:                    \n\t"
+				"andi       t0,   %[BK], 7     \n\t"
+				"blez       t0,   M16x4_SAVERESULT   \n\t"
+
+				"addi       t4,    %[PA], 4*4  \n\t"
+				"addi       t5,    %[PA], 8*4  \n\t"
+				"addi       t6,    %[PA], 12*4  \n\t"
+				"addi       t1,    %[PB], 1*4  \n\t"
+				"addi       t2,    %[PB], 2*4  \n\t"
+				"addi       t3,    %[PB], 3*4  \n\t"
+
+				".align 4                      \n\t"
+				"M16x4_TAILLOOP:                \n\t"
+				"flw        ft0,  (%[PB])      \n\t"
+				"addi       %[PB], %[PB], 4*4  \n\t"
+				"vle32.v      v0,   (%[PA])      \n\t"
+				"add        %[PA], %[PA], 16*4  \n\t"
+				"vle32.v      v1,   (t4)         \n\t"
+				"addi       t4,    t4,    16*4  \n\t"
+
+				"vfmv.v.f   v8,   ft0          \n\t"
+				"flw        ft1,  (t1)         \n\t"
+				"addi       t1,   t1,     4*4  \n\t"
+				"vle32.v      v2,   (t5)         \n\t"
+				"addi       t5,    t5,    16*4  \n\t"
+				"vle32.v      v3,   (t6)         \n\t"
+				"addi       t6,    t6,    16*4  \n\t"
+
+				"vfmacc.vv  v16,  v8,    v0    \n\t"
+				"flw        ft2,  (t2)         \n\t"
+				"addi       t2,   t2,    4*4  \n\t"
+				"vfmacc.vv  v17,  v8,    v1    \n\t"
+				"vfmacc.vv  v18,  v8,    v2    \n\t"
+				"vfmv.v.f   v9,   ft1          \n\t"
+				"vfmacc.vv  v19,  v8,    v3    \n\t"
+								
+
+				"vfmacc.vv  v20,  v9,    v0    \n\t"
+				"flw        ft3,  (t3)         \n\t"
+				"addi       t3,   t3,    4*4  \n\t"
+				"vfmacc.vv  v21,  v9,    v1    \n\t"
+				"vfmacc.vv  v22,  v9,    v2    \n\t"
+				"vfmv.v.f   v10,  ft2          \n\t"
+				"vfmacc.vv  v23,  v9,    v3    \n\t"
+
+				"vfmv.v.f   v11,  ft3          \n\t"
+				"vfmacc.vv  v24,  v10,    v0    \n\t"
+				"vfmacc.vv  v25,  v10,    v1    \n\t"
+				"vfmacc.vv  v26,  v10,    v2    \n\t"
+				"vfmacc.vv  v27,  v10,    v3    \n\t"
+
+				"vfmacc.vv  v28,  v11,    v0    \n\t"
+				"vfmacc.vv  v29,  v11,    v1    \n\t"
+				"vfmacc.vv  v30,  v11,    v2    \n\t"
+				"vfmacc.vv  v31,  v11,    v3    \n\t"
+
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M16x4_TAILLOOP \n\t"
+				
+				//Save result
+				//load C
+				"M16x4_SAVERESULT:              \n\t"
+				//use v8 to store alpha
+				"vfmv.v.f   v8,   %[ALPHA]     \n\t"
+				"vle32.v      v0,   (%[C0])      \n\t"
+				"addi       t4,   %[C0], 4*4   \n\t"
+				"vle32.v      v1,   (%[C1])      \n\t"
+				"addi       t5,   %[C1], 4*4   \n\t"
+				"vle32.v      v2,   (%[C2])      \n\t"
+				"addi       t6,   %[C2], 4*4   \n\t"
+				"vle32.v      v3,   (%[C3])      \n\t"
+				"addi       t3,   %[C3], 4*4   \n\t"
+				
+				//Multiply Alpha
+				"vfmacc.vv  v0,   v8, v16 \n\t"
+				"vle32.v      v4,   (t4)          \n\t"
+				"vfmacc.vv  v1,   v8, v20 \n\t"
+				"vle32.v      v5,   (t5)          \n\t"
+				"vfmacc.vv  v2,   v8, v24 \n\t"
+				"vle32.v      v6,   (t6)          \n\t"
+				"vfmacc.vv  v3,   v8, v28 \n\t"
+				"vle32.v      v7,   (t3)          \n\t"
+
+				"vfmacc.vv  v4,   v8, v17 \n\t"
+				"vse32.v      v0,   (%[C0])      \n\t"
+				"add        %[C0], %[C0], 8*4  \n\t"
+				"vfmacc.vv  v5,   v8, v21 \n\t"
+				"vse32.v      v1,   (%[C1])      \n\t"
+				"add        %[C1], %[C1], 8*4  \n\t"
+				
+				"vfmacc.vv  v6,   v8, v25 \n\t"
+				"vse32.v      v2,   (%[C2])      \n\t"
+				"add        %[C2], %[C2], 8*4  \n\t"
+
+				"vfmacc.vv  v7,   v8, v29 \n\t"
+				"vse32.v      v3,   (%[C3])      \n\t"
+				"add        %[C3], %[C3], 8*4  \n\t"
+
+				"vle32.v      v0,   (%[C0])      \n\t"
+				"vse32.v      v4,   (t4)         \n\t"
+				"add        t4,   t4,     8*4  \n\t"
+				
+				"vle32.v      v1,   (%[C1])      \n\t"
+				"vse32.v      v5,   (t5)         \n\t"
+				"add        t5,   t5,     8*4  \n\t"
+
+				"vle32.v      v2,   (%[C2])      \n\t"
+				"vse32.v      v6,   (t6)         \n\t"
+				"add        t6,   t6,     8*4  \n\t"
+
+				"vle32.v      v3,   (%[C3])      \n\t"
+				"vse32.v      v7,   (t3)         \n\t"
+				"add        t3,   t3,     8*4  \n\t"
+
+
+				"vfmacc.vv  v0,   v8, v18 \n\t"
+				"vle32.v      v4,   (t4)          \n\t"
+				"vfmacc.vv  v1,   v8, v22 \n\t"
+				"vle32.v      v5,   (t5)          \n\t"
+				"vfmacc.vv  v2,   v8, v26 \n\t"
+				"vle32.v      v6,   (t6)          \n\t"
+				"vfmacc.vv  v3,   v8, v30 \n\t"
+				"vle32.v      v7,   (t3)          \n\t"
+
+				"vfmacc.vv  v4,   v8, v19 \n\t"
+				"vse32.v      v0,   (%[C0])      \n\t"
+				"add        %[C0], %[C0], 8*4  \n\t"
+
+				"vfmacc.vv  v5,   v8, v23 \n\t"
+				"vse32.v      v1,   (%[C1])      \n\t"
+				"add        %[C1], %[C1], 8*4  \n\t"
+
+				"vfmacc.vv  v6,   v8, v27 \n\t"
+				"vse32.v      v2,   (%[C2])      \n\t"
+				"add        %[C2], %[C2], 8*4  \n\t"
+
+				"vfmacc.vv  v7,   v8, v31 \n\t"
+				"vse32.v      v3,   (%[C3])      \n\t"
+				"add        %[C3], %[C3], 8*4  \n\t"
+
+				"vse32.v      v4,   (t4)         \n\t"
+				"vse32.v      v5,   (t5)         \n\t"
+				"vse32.v      v6,   (t6)         \n\t"
+				"vse32.v      v7,   (t3)         \n\t"
+				"M16x4_END:                     \n\t"
+				
+				:[C0]"+r"(C0),[C1]"+r"(C1),[C2]"+r"(C2),[C3]"+r"(C3),
+				 [PA]"+r"(ptrba), [PB]"+r"(ptrbb)
+				:[ALPHA]"f"(alpha), [BK]"r"(bk)
+				:"cc", "t0", "t4","t5","t6","t3","t1","t2",
+				 "ft11", "ft0", "ft1", "ft2","ft3","ft4", "ft5", "ft6","ft7",
+				 "v0", "v1", "v2", "v3","v4", "v5", "v6", "v7",
+				 "v8", "v9", "v10", "v11","v12", "v13", "v14", "v15",
+				 "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23",
+				 "v24", "v25", "v26", "v27", "v28", "v29", "v30", "v31");
+	   }
+	   if(bm&8){
+   		   ptrbb = bb;
+		   //t0 for k
+		   //ft0-ft3,ft4-ft7,v8-v15 for B, t1-t3 for PB1-3
+		   //v0-v3,v4-v7 for A, t4-t6 for PA1-3
+		   //v16-v31 for temp C
+		   
+		   asm volatile(
+				"vsetvli    zero, zero, e32,m1 \n\t"
+				"fmv.w.x    ft11, zero         \n\t"
+				"mv         t0,   %[BK]        \n\t"
+				
+				"vfmv.v.f   v16,  ft11         \n\t"
+				"vfmv.v.f   v17,  ft11         \n\t"
+				
+				"vfmv.v.f   v20,  ft11         \n\t"
+				"vfmv.v.f   v21,  ft11         \n\t"
+				
+				"vfmv.v.f   v24,  ft11         \n\t"
+				"vfmv.v.f   v25,  ft11         \n\t"
+								
+				"vfmv.v.f   v28,  ft11         \n\t"
+				"vfmv.v.f   v29,  ft11         \n\t"
+				
+				//unloop 8
+				"srli       t0,   %[BK], 3     \n\t"
+				"blez       t0,   M8x4_TAIL    \n\t"
+				
+				//preloop
+				KERNEL8x4_I
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				"addi       t0,   t0, -1       \n\t"
+				"blez       t0,   M8x4_MAINLOOP_TAIL    \n\t"
+				".align 4                      \n\t"
+				"M8x4_MAINLOOP:                \n\t"
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M8x4_MAINLOOP \n\t"
+				
+				"M8x4_MAINLOOP_TAIL:           \n\t"
+				KERNEL8x4_M1
+				KERNEL8x4_M2
+				KERNEL8x4_M1
+				KERNEL8x4_E
+				
+				//tail
+				"M8x4_TAIL:                    \n\t"
+				"andi       t0,   %[BK], 7     \n\t"
+				"blez       t0,   M8x4_SAVERESULT   \n\t"
+
+				"addi       t4,    %[PA], 4*4  \n\t"
+				
+				"addi       t1,    %[PB], 1*4  \n\t"
+				"addi       t2,    %[PB], 2*4  \n\t"
+				"addi       t3,    %[PB], 3*4  \n\t"
+
+				".align 4                      \n\t"
+				"M8x4_TAILLOOP:                \n\t"
+				"flw        ft0,  (%[PB])      \n\t"
+				"addi       %[PB], %[PB], 4*4  \n\t"
+				"vle32.v      v0,   (%[PA])      \n\t"
+				"add        %[PA], %[PA], 8*4  \n\t"
+				"vle32.v      v1,   (t4)         \n\t"
+				"addi       t4,    t4,    8*4  \n\t"
+
+				"vfmv.v.f   v8,   ft0          \n\t"
+				"flw        ft1,  (t1)         \n\t"
+				"addi       t1,   t1,     4*4  \n\t"
+				
+				"vfmacc.vv  v16,  v8,    v0    \n\t"
+				"flw        ft2,  (t2)         \n\t"
+				"addi       t2,   t2,    4*4  \n\t"
+				"vfmacc.vv  v17,  v8,    v1    \n\t"
+				"vfmv.v.f   v9,   ft1          \n\t"
+
+				"vfmacc.vv  v20,  v9,    v0    \n\t"
+				"flw        ft3,  (t3)         \n\t"
+				"addi       t3,   t3,    4*4  \n\t"
+				"vfmacc.vv  v21,  v9,    v1    \n\t"
+				"vfmv.v.f   v10,  ft2          \n\t"
+
+				"vfmv.v.f   v11,  ft3          \n\t"
+				"vfmacc.vv  v24,  v10,    v0    \n\t"
+				"vfmacc.vv  v25,  v10,    v1    \n\t"
+
+				"vfmacc.vv  v28,  v11,    v0    \n\t"
+				"vfmacc.vv  v29,  v11,    v1    \n\t"
+
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M8x4_TAILLOOP \n\t"
+				
+				//Save result
+				//load C
+				"M8x4_SAVERESULT:              \n\t"
+				//use v8 to store alpha
+				"vfmv.v.f   v8,   %[ALPHA]     \n\t"
+				"vle32.v      v0,   (%[C0])      \n\t"
+				"addi       t4,   %[C0], 4*4   \n\t"
+				"vle32.v      v1,   (%[C1])      \n\t"
+				"addi       t5,   %[C1], 4*4   \n\t"
+				"vle32.v      v2,   (%[C2])      \n\t"
+				"addi       t6,   %[C2], 4*4   \n\t"
+				"vle32.v      v3,   (%[C3])      \n\t"
+				"addi       t3,   %[C3], 4*4   \n\t"
+				
+				//Multiply Alpha
+				"vfmacc.vv  v0,   v8, v16 \n\t"
+				"vle32.v      v4,   (t4)          \n\t"
+				"vfmacc.vv  v1,   v8, v20 \n\t"
+				"vle32.v      v5,   (t5)          \n\t"
+				"vfmacc.vv  v2,   v8, v24 \n\t"
+				"vle32.v      v6,   (t6)          \n\t"
+				"vfmacc.vv  v3,   v8, v28 \n\t"
+				"vle32.v      v7,   (t3)          \n\t"
+
+				"vfmacc.vv  v4,   v8, v17 \n\t"
+				"vse32.v      v0,   (%[C0])      \n\t"
+				"add        %[C0], %[C0], 8*4  \n\t"
+				"vfmacc.vv  v5,   v8, v21 \n\t"
+				"vse32.v      v1,   (%[C1])      \n\t"
+				"add        %[C1], %[C1], 8*4  \n\t"
+				
+				"vfmacc.vv  v6,   v8, v25 \n\t"
+				"vse32.v      v2,   (%[C2])      \n\t"
+				"add        %[C2], %[C2], 8*4  \n\t"
+
+				"vfmacc.vv  v7,   v8, v29 \n\t"
+				"vse32.v      v3,   (%[C3])      \n\t"
+				"add        %[C3], %[C3], 8*4  \n\t"
+				
+				"vse32.v      v4,   (t4)         \n\t"
+				"vse32.v      v5,   (t5)         \n\t"
+				"vse32.v      v6,   (t6)         \n\t"
+				"vse32.v      v7,   (t3)         \n\t"
+				"M8x4_END:                     \n\t"
+				
+				:[C0]"+r"(C0),[C1]"+r"(C1),[C2]"+r"(C2),[C3]"+r"(C3),
+				 [PA]"+r"(ptrba), [PB]"+r"(ptrbb)
+				:[ALPHA]"f"(alpha), [BK]"r"(bk)
+				:"cc", "t0", "t4","t5","t6","t3","t1","t2",
+				 "ft11", "ft0", "ft1", "ft2","ft3","ft4", "ft5", "ft6","ft7",
+				 "v0", "v1", "v2", "v3","v4", "v5", "v6", "v7",
+				 "v8", "v9", "v10", "v11","v12", "v13", "v14", "v15",
+				 "v16", "v17", "v20", "v21", 
+				 "v24", "v25", "v28", "v29");
+	   }
+	   if(bm&4){
+		   ptrbb = bb;
+      		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   res4 = 0;
+		   res5 = 0;
+		   res6 = 0;
+		   res7 = 0;
+		   res8 = 0;
+		   res9 = 0;
+		   res10 = 0;
+		   res11 = 0;
+		   res12 = 0;
+		   res13 = 0;
+		   res14 = 0;
+		   res15 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+			   load2 = ptrba[2];
+			   load3 = ptrba[3];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+			   res2 = res2 + load2 * loadb0;
+			   res3 = res3 + load3 * loadb0;
+
+			   res4 = res4 + load0 * loadb1;
+			   res5 = res5 + load1 * loadb1;
+			   res6 = res6 + load2 * loadb1;
+			   res7 = res7 + load3 * loadb1;
+
+			   loadb2 = ptrbb[2];
+			   loadb3 = ptrbb[3];
+			   
+   			   res8 = res8 + load0 * loadb2;
+			   res9 = res9 + load1 * loadb2;
+			   res10 = res10 + load2 * loadb2;
+			   res11 = res11 + load3 * loadb2;
+
+			   res12 = res12 + load0 * loadb3;
+			   res13 = res13 + load1 * loadb3;
+			   res14 = res14 + load2 * loadb3;
+			   res15 = res15 + load3 * loadb3;
+
+			   ptrba += 4;
+			   ptrbb += 4;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+		   res6 = res6 * alpha;
+		   res7 = res7 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+		   res10 = res10 * alpha;
+		   res11 = res11 * alpha;
+		   res12 = res12 * alpha;
+		   res13 = res13 * alpha;
+		   res14 = res14 * alpha;
+		   res15 = res15 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   
+		   C1[0] += res4;
+		   C1[1] += res5;
+		   C1[2] += res6;
+		   C1[3] += res7;
+
+   		   C2[0] += res8;
+		   C2[1] += res9;
+		   C2[2] += res10;
+		   C2[3] += res11;
+		   
+		   C3[0] += res12;
+		   C3[1] += res13;
+		   C3[2] += res14;
+		   C3[3] += res15;
+
+		   C0 += 4;
+		   C1 += 4;
+		   C2 += 4;
+		   C3 += 4;
+	   }
+   	   if(bm&2){
+		   ptrbb = bb;
+		   
+       		   res0 = 0;
+		   res1 = 0;
+		   
+		   res4 = 0;
+		   res5 = 0;
+		   
+		   res8 = 0;
+		   res9 = 0;
+		   
+		   res12 = 0;
+		   res13 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+
+			   res4 = res4 + load0 * loadb1;
+			   res5 = res5 + load1 * loadb1;
+
+			   loadb2 = ptrbb[2];
+			   loadb3 = ptrbb[3];
+			   
+   			   res8 = res8 + load0 * loadb2;
+			   res9 = res9 + load1 * loadb2;
+
+			   res12 = res12 + load0 * loadb3;
+			   res13 = res13 + load1 * loadb3;
+
+			   ptrba += 2;
+			   ptrbb += 4;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+
+		   res12 = res12 * alpha;
+		   res13 = res13 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+
+		   C1[0] += res4;
+		   C1[1] += res5;
+
+   		   C2[0] += res8;
+		   C2[1] += res9;
+		   
+		   C3[0] += res12;
+		   C3[1] += res13;
+
+		   C0 += 2;
+		   C1 += 2;
+		   C2 += 2;
+		   C3 += 2;
+	   }
+	   if(bm&1){
+		   ptrbb = bb;
+		   //t0 for k
+		   //ft0-ft3,ft4-ft7,v8-v15 for B, t1-t3 for PB1-3
+		   //v0-v3,v4-v7 for A, t4-t6 for PA1-3
+		   //v16-v31 for temp C
+
+		   FLOAT tmp[4];
+		   tmpc=tmp;
+		   //t1-t3 for PB
+		   //v0-v4 for A, v8-v11 for B
+		   //v16-v19 for C
+		   asm volatile(
+				"vsetvli    zero, zero, e32,m1 \n\t"
+				"fmv.w.x    ft11, zero         \n\t"
+				
+				"vfmv.v.f   v16,  ft11         \n\t"
+				"vfmv.v.f   v17,  ft11         \n\t"
+				"vfmv.v.f   v18,  ft11         \n\t"
+				"vfmv.v.f   v19,  ft11         \n\t"
+				//unloop 4
+
+				"srli       t0,   %[BK], 2     \n\t"
+				"blez       t0,   M1x4_TAIL    \n\t"
+
+				"addi       t1, %[PB], 4*4     \n\t"
+				"addi       t2, %[PB], 8*4     \n\t"
+				"addi       t3, %[PB], 12*4    \n\t"
+				
+				".align 4                      \n\t"
+				"M1x4_MAINLOOP:                \n\t"
+
+				"vle32.v      v4,   (%[PA])      \n\t"
+				"addi       %[PA], %[PA], 4*4  \n\t"
+				"vrgather.vi v0,   v4,   0     \n\t"
+				
+				"vle32.v      v8,   (%[PB])      \n\t"
+				"addi       %[PB], %[PB], 16*4  \n\t"
+				"vrgather.vi v1,   v4,   1     \n\t"
+				
+				"vle32.v      v9,   (t1)         \n\t"
+				"addi       t1,    t1,   16*4  \n\t"
+				"vrgather.vi v2,   v4,   2     \n\t"
+				
+				"vle32.v      v10,   (t2)         \n\t"
+				"addi       t2,    t2,   16*4  \n\t"
+				"vrgather.vi v3,   v4,   3     \n\t"
+				
+				"vle32.v      v11,   (t3)         \n\t"
+				"addi       t3,    t3,   16*4  \n\t"
+				
+				"vfmacc.vv  v16,  v8,     v0    \n\t"
+				"vfmacc.vv  v17,  v9,     v1    \n\t"
+				"vfmacc.vv  v18,  v10,    v2    \n\t"
+				"vfmacc.vv  v19,  v11,    v3    \n\t"
+				
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M1x4_MAINLOOP \n\t"
+				
+				"M1x4_TAIL:                    \n\t"
+				"andi       t0,   %[BK], 3     \n\t"
+				"blez       t0,   M1x4_SAVERESULT   \n\t"
+
+				"M1x4_TAILLOOP:                    \n\t"
+				"flw        ft0,  (%[PA])      \n\t"
+				"addi       %[PA], %[PA], 1*4  \n\t"
+				"vle32.v      v8,   (%[PB])      \n\t"
+				"addi       %[PB], %[PB], 4*4  \n\t"
+				"vfmv.v.f   v0,   ft0          \n\t"
+				"vfmacc.vv  v16,  v8,    v0    \n\t"
+				
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M1x4_TAILLOOP \n\t"
+				
+				"M1x4_SAVERESULT:              \n\t"
+				//merge v16-v19
+				"vfadd.vv  v16,  v16,   v17   \n\t"
+				"vfadd.vv  v18,  v18,   v19   \n\t"
+				"vfadd.vv  v16,  v16,   v18   \n\t"
+				
+				"vfmv.v.f   v8,   %[ALPHA]     \n\t"
+				"vfmul.vv   v16,   v8,   v16    \n\t"
+				"vse32.v      v16,   (%[TMP_C])   \n\t"
+				"M1x4_END:                     \n\t"
+				:[TMP_C]"+r"(tmpc),
+				 [PA]"+r"(ptrba), [PB]"+r"(ptrbb)
+				:[ALPHA]"f"(alpha), [BK]"r"(bk)
+				:"cc", "t0", "t3","t1","t2",
+				 "ft0", "ft11",
+				 "v0", "v1", "v2", "v3","v4",
+				 "v8", "v9", "v10", "v11",
+				 "v16", "v17","v18", "v19"
+				);
+
+		   C0[0] += tmp[0];
+		   C1[0] += tmp[1];
+   		   C2[0] += tmp[2];
+		   C3[0] += tmp[3];
+
+		   /* don't need move c point
+		   C0 += 1;
+		   C1 += 1;
+		   C2 += 1;
+		   C3 += 1;
+		   */
+	   }
+	   
+	   k = bk<<2;
+	   bb = bb+k;
+	   i = ldc<<2;
+	   C = C+i;
+   }
+   
+   if(bn&2){
+	   C0 = C;
+	   C1 = C0+ldc;
+
+	   ptrba = ba;
+	   for(i=0; i<bm/16; i+=1){
+		   ptrbb = bb;
+   		   asm volatile(
+				"vsetvli    zero, zero, e32,m1 \n\t"
+				"fmv.w.x    ft11, zero         \n\t"
+				"mv         t0,   %[BK]        \n\t"
+				
+				"vfmv.v.f   v16,  ft11         \n\t"
+				"vfmv.v.f   v17,  ft11         \n\t"
+				"vfmv.v.f   v18,  ft11         \n\t"
+				"vfmv.v.f   v19,  ft11         \n\t"
+
+				"vfmv.v.f   v20,  ft11         \n\t"
+				"vfmv.v.f   v21,  ft11         \n\t"
+				"vfmv.v.f   v22,  ft11         \n\t"
+				"vfmv.v.f   v23,  ft11         \n\t"
+
+				//unloop 8
+				"srli       t0,   %[BK], 3     \n\t"
+				"blez       t0,   M16x2_TAIL    \n\t"
+				
+				//preloop
+				KERNEL16x2_I
+				KERNEL16x2_M2
+				KERNEL16x2_M1
+				KERNEL16x2_M2
+				"addi       t0,   t0, -1       \n\t"
+				"blez       t0,   M16x2_MAINLOOP_TAIL    \n\t"
+				".align 4                      \n\t"
+				"M16x2_MAINLOOP:                \n\t"
+				KERNEL16x2_M1
+				KERNEL16x2_M2
+				KERNEL16x2_M1
+				KERNEL16x2_M2
+				KERNEL16x2_M1
+				KERNEL16x2_M2
+				KERNEL16x2_M1
+				KERNEL16x2_M2
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M16x2_MAINLOOP \n\t"
+				
+				"M16x2_MAINLOOP_TAIL:           \n\t"
+				KERNEL16x2_M1
+				KERNEL16x2_M2
+				KERNEL16x2_M1
+				KERNEL16x2_E
+				
+				//tail
+				"M16x2_TAIL:                    \n\t"
+				"andi       t0,   %[BK], 7     \n\t"
+				"blez       t0,   M16x2_SAVERESULT   \n\t"
+
+				"addi       t4,    %[PA], 4*4  \n\t"
+				"addi       t5,    %[PA], 8*4  \n\t"
+				"addi       t6,    %[PA], 12*4  \n\t"
+				"addi       t1,    %[PB], 1*4  \n\t"
+
+				".align 4                      \n\t"
+				"M16x2_TAILLOOP:                \n\t"
+				"flw        ft0,  (%[PB])      \n\t"
+				"addi       %[PB], %[PB], 2*4  \n\t"
+				"vle32.v      v0,   (%[PA])      \n\t"
+				"add        %[PA], %[PA], 16*4  \n\t"
+				"vle32.v      v1,   (t4)         \n\t"
+				"addi       t4,    t4,    16*4  \n\t"
+
+				"vfmv.v.f   v8,   ft0          \n\t"
+				"flw        ft1,  (t1)         \n\t"
+				"addi       t1,   t1,     2*4  \n\t"
+				"vle32.v      v2,   (t5)         \n\t"
+				"addi       t5,    t5,    16*4  \n\t"
+				"vle32.v      v3,   (t6)         \n\t"
+				"addi       t6,    t6,    16*4  \n\t"
+
+				"vfmv.v.f   v9,   ft1          \n\t"				
+				"vfmacc.vv  v16,  v8,    v0    \n\t"
+				"vfmacc.vv  v17,  v8,    v1    \n\t"
+				"vfmacc.vv  v18,  v8,    v2    \n\t"
+				"vfmacc.vv  v19,  v8,    v3    \n\t"
+
+				"vfmacc.vv  v20,  v9,    v0    \n\t"
+				"vfmacc.vv  v21,  v9,    v1    \n\t"
+				"vfmacc.vv  v22,  v9,    v2    \n\t"
+				"vfmacc.vv  v23,  v9,    v3    \n\t"
+
+				"addi       t0,   t0, -1       \n\t"
+				"bgtz       t0,   M16x2_TAILLOOP \n\t"
+				
+				//Save result
+				//load C
+				"M16x2_SAVERESULT:              \n\t"
+				//use v8 to store alpha
+				"vfmv.v.f   v8,   %[ALPHA]     \n\t"
+				"vle32.v      v0,   (%[C0])      \n\t"
+				"addi       t4,   %[C0], 4*4   \n\t"
+				"vle32.v      v1,   (%[C1])      \n\t"
+				"addi       t5,   %[C1], 4*4   \n\t"
+				
+				//Multiply Alpha
+				"vfmacc.vv  v0,   v8, v16 \n\t"
+				"vle32.v      v4,   (t4)          \n\t"
+				"vfmacc.vv  v1,   v8, v20 \n\t"
+				"vle32.v      v5,   (t5)          \n\t"
+				
+				"vfmacc.vv  v4,   v8, v17 \n\t"
+				"vse32.v      v0,   (%[C0])      \n\t"
+				"add        %[C0], %[C0], 8*4  \n\t"
+				"vfmacc.vv  v5,   v8, v21 \n\t"
+				"vse32.v      v1,   (%[C1])      \n\t"
+				"add        %[C1], %[C1], 8*4  \n\t"
+				
+				"vle32.v      v0,   (%[C0])      \n\t"
+				"vse32.v      v4,   (t4)         \n\t"
+				"add        t4,   t4,     8*4  \n\t"
+				
+				"vle32.v      v1,   (%[C1])      \n\t"
+				"vse32.v      v5,   (t5)         \n\t"
+				"add        t5,   t5,     8*4  \n\t"
+
+				"vfmacc.vv  v0,   v8, v18 \n\t"
+				"vle32.v      v4,   (t4)          \n\t"
+				"vfmacc.vv  v1,   v8, v22 \n\t"
+				"vle32.v      v5,   (t5)          \n\t"
+
+				"vfmacc.vv  v4,   v8, v19 \n\t"
+				"vse32.v      v0,   (%[C0])      \n\t"
+				"add        %[C0], %[C0], 8*4  \n\t"
+
+				"vfmacc.vv  v5,   v8, v23 \n\t"
+				"vse32.v      v1,   (%[C1])      \n\t"
+				"add        %[C1], %[C1], 8*4  \n\t"
+
+				"vse32.v      v4,   (t4)         \n\t"
+				"vse32.v      v5,   (t5)         \n\t"
+				"M16x2_END:                     \n\t"
+				
+				:[C0]"+r"(C0),[C1]"+r"(C1),
+				 [PA]"+r"(ptrba), [PB]"+r"(ptrbb)
+				:[ALPHA]"f"(alpha), [BK]"r"(bk)
+				:"cc", "t0", "t4","t5","t6","t3","t1","t2",
+				 "ft11", "ft0", "ft1", "ft2","ft3","ft4", "ft5", "ft6","ft7",
+				 "v0", "v1", "v2", "v3","v4", "v5", "v6", "v7",
+				 "v8", "v9", "v10", "v11","v12", "v13", "v14", "v15",
+				 "v16", "v17", "v18", "v19", "v20", "v21", "v22", "v23");
+
+	   }
+	   if(bm&8){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   res4 = 0;
+		   res5 = 0;
+		   res6 = 0;
+		   res7 = 0;
+		   res8 = 0;
+		   res9 = 0;
+		   res10 = 0;
+		   res11 = 0;
+		   res12 = 0;
+		   res13 = 0;
+		   res14 = 0;
+		   res15 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+			   load2 = ptrba[2];
+			   load3 = ptrba[3];
+			   load4 = ptrba[4];
+			   load5 = ptrba[5];
+			   load6 = ptrba[6];
+			   load7 = ptrba[7];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+			   res2 = res2 + load2 * loadb0;
+			   res3 = res3 + load3 * loadb0;
+
+			   res4 = res4 + load4 * loadb0;
+			   res5 = res5 + load5 * loadb0;
+			   res6 = res6 + load6 * loadb0;
+			   res7 = res7 + load7 * loadb0;
+
+   			   res8 = res8 + load0 * loadb1;
+			   res9 = res9 + load1 * loadb1;
+			   res10 = res10 + load2 * loadb1;
+			   res11 = res11 + load3 * loadb1;
+
+			   res12 = res12 + load4 * loadb1;
+			   res13 = res13 + load5 * loadb1;
+			   res14 = res14 + load6 * loadb1;
+			   res15 = res15 + load7 * loadb1;
+
+			   ptrba += 8;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+		   res6 = res6 * alpha;
+		   res7 = res7 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+		   res10 = res10 * alpha;
+		   res11 = res11 * alpha;
+		   res12 = res12 * alpha;
+		   res13 = res13 * alpha;
+		   res14 = res14 * alpha;
+		   res15 = res15 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   C0[4] += res4;
+		   C0[5] += res5;
+		   C0[6] += res6;
+		   C0[7] += res7;
+
+   		   C1[0] += res8;
+		   C1[1] += res9;
+		   C1[2] += res10;
+		   C1[3] += res11;
+		   C1[4] += res12;
+		   C1[5] += res13;
+		   C1[6] += res14;
+		   C1[7] += res15;
+
+		   C0 += 8;
+		   C1 += 8;
+	   }
+	   if(bm&4){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   
+		   res8 = 0;
+		   res9 = 0;
+		   res10 = 0;
+		   res11 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+			   load2 = ptrba[2];
+			   load3 = ptrba[3];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+			   res2 = res2 + load2 * loadb0;
+			   res3 = res3 + load3 * loadb0;
+
+   			   res8 = res8 + load0 * loadb1;
+			   res9 = res9 + load1 * loadb1;
+			   res10 = res10 + load2 * loadb1;
+			   res11 = res11 + load3 * loadb1;
+
+			   ptrba += 4;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+		   res10 = res10 * alpha;
+		   res11 = res11 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+
+   		   C1[0] += res8;
+		   C1[1] += res9;
+		   C1[2] += res10;
+		   C1[3] += res11;
+
+		   C0 += 4;
+		   C1 += 4;
+	   }
+   	   if(bm&2){
+		   ptrbb = bb;
+      		   res0 = 0;
+		   res1 = 0;
+		   
+		   res8 = 0;
+		   res9 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+
+			   load0 = ptrba[0];
+			   load1 = ptrba[1];
+				   
+			   res0 = res0 + load0 * loadb0;
+			   res1 = res1 + load1 * loadb0;
+
+   			   res8 = res8 + load0 * loadb1;
+			   res9 = res9 + load1 * loadb1;
+
+			   ptrba += 2;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+
+       		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+
+   		   C1[0] += res8;
+		   C1[1] += res9;
+		   
+		   C0 += 2;
+		   C1 += 2;
+	   }
+	   if(bm&1){
+		   ptrbb = bb;
+       		   res0 = 0;
+		   res8 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   loadb1 = ptrbb[1];
+			   load0 = ptrba[0];
+				   
+			   res0 = res0 + load0 * loadb0;
+   			   res8 = res8 + load0 * loadb1;
+			   ptrba += 1;
+			   ptrbb += 2;
+		   }
+		   
+      		   res0 = res0 * alpha;
+       		   res8 = res8 * alpha;
+
+		   C0[0] += res0;
+   		   C1[0] += res8;
+		   
+		   C0 += 1;
+		   C1 += 1;
+	   }
+	   k = bk<<1;
+	   bb = bb+k;
+	   i = ldc<<1;
+	   C = C+i;
+   }
+
+   if (bn&1){
+	   C0 = C;
+	   ptrba = ba;
+	   for(i=0; i<bm/16; i+=1){
+	 	   ptrbb = bb;
+		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   res4 = 0;
+		   res5 = 0;
+		   res6 = 0;
+		   res7 = 0;
+		   
+		   res8 = 0;
+		   res9 = 0;
+		   res10 = 0;
+		   res11 = 0;
+		   res12 = 0;
+		   res13 = 0;
+		   res14 = 0;
+		   res15 = 0;
+		   
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   res1 = res1 + ptrba[1] * loadb0;
+			   res2 = res2 + ptrba[2] * loadb0;
+			   res3 = res3 + ptrba[3] * loadb0;
+
+			   res4 = res4 + ptrba[4] * loadb0;
+			   res5 = res5 + ptrba[5] * loadb0;
+			   res6 = res6 + ptrba[6] * loadb0;
+			   res7 = res7 + ptrba[7] * loadb0;
+			   
+			   res8 = res8 + ptrba[8] * loadb0;
+			   res9 = res9 + ptrba[9] * loadb0;
+			   res10 = res10 + ptrba[10] * loadb0;
+			   res11 = res11 + ptrba[11] * loadb0;
+
+			   res12 = res12 + ptrba[12] * loadb0;
+			   res13 = res13 + ptrba[13] * loadb0;
+			   res14 = res14 + ptrba[14] * loadb0;
+			   res15 = res15 + ptrba[15] * loadb0;
+			   
+			   ptrba += 16;
+			   ptrbb += 1;
+		   }
+   		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+		   res6 = res6 * alpha;
+		   res7 = res7 * alpha;
+		   
+   		   res8 = res8 * alpha;
+		   res9 = res9 * alpha;
+		   res10 = res10 * alpha;
+		   res11 = res11 * alpha;
+		   res12 = res12 * alpha;
+		   res13 = res13 * alpha;
+		   res14 = res14 * alpha;
+		   res15 = res15 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   C0[4] += res4;
+		   C0[5] += res5;
+		   C0[6] += res6;
+		   C0[7] += res7;
+		   
+		   C0[8] += res8;
+		   C0[9] += res9;
+		   C0[10] += res10;
+		   C0[11] += res11;
+		   C0[12] += res12;
+		   C0[13] += res13;
+		   C0[14] += res14;
+		   C0[15] += res15;
+		   
+		   C0 += 16;
+
+	   }
+	   
+	   if(bm&8){
+		   ptrbb = bb;
+		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   res4 = 0;
+		   res5 = 0;
+		   res6 = 0;
+		   res7 = 0;
+
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   res1 = res1 + ptrba[1] * loadb0;
+			   res2 = res2 + ptrba[2] * loadb0;
+			   res3 = res3 + ptrba[3] * loadb0;
+
+			   res4 = res4 + ptrba[4] * loadb0;
+			   res5 = res5 + ptrba[5] * loadb0;
+			   res6 = res6 + ptrba[6] * loadb0;
+			   res7 = res7 + ptrba[7] * loadb0;
+			   
+			   ptrba += 8;
+			   ptrbb += 1;
+		   }
+   		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+		   res4 = res4 * alpha;
+		   res5 = res5 * alpha;
+		   res6 = res6 * alpha;
+		   res7 = res7 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   C0[4] += res4;
+		   C0[5] += res5;
+		   C0[6] += res6;
+		   C0[7] += res7;
+		   
+		   C0 += 8;
+	   }
+	   if(bm&4){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   res2 = 0;
+		   res3 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   res1 = res1 + ptrba[1] * loadb0;
+			   res2 = res2 + ptrba[2] * loadb0;
+			   res3 = res3 + ptrba[3] * loadb0;
+
+			   ptrba += 4;
+			   ptrbb += 1;
+		   }
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+		   res2 = res2 * alpha;
+		   res3 = res3 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   C0[2] += res2;
+		   C0[3] += res3;
+		   
+		   C0 += 4;
+	   }
+   	   if(bm&2){
+		   ptrbb = bb;
+   		   res0 = 0;
+		   res1 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   res1 = res1 + ptrba[1] * loadb0;
+
+			   ptrba += 2;
+			   ptrbb += 1;
+		   }
+      		   res0 = res0 * alpha;
+		   res1 = res1 * alpha;
+
+		   C0[0] += res0;
+		   C0[1] += res1;
+		   
+		   C0 += 2;
+	   }
+	   if(bm&1){
+   		   ptrbb = bb;
+   		   res0 = 0;
+		   for(k=0; k<bk; k+=1){
+			   loadb0 = ptrbb[0];
+			   res0 = res0 + ptrba[0] * loadb0;
+			   ptrba += 1;
+			   ptrbb += 1;
+		   }
+      		   res0 = res0 * alpha;
+		   C0[0] += res0;
+		   C0 += 1;
+	   }
+
+	   k = bk;
+	   bb = bb+k;
+	   C = C+ldc;
+   }
+   return 0;
+}
+
diff --git a/param.h b/param.h
index 514b13a3..275a73ae 100644
--- a/param.h
+++ b/param.h
@@ -79,8 +79,6 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define SBGEMM_DEFAULT_P 256
 #define SBGEMM_DEFAULT_R 256
 #define SBGEMM_DEFAULT_Q 256
-#define SBGEMM_ALIGN_K 1  // must be 2^x
-
 #ifdef OPTERON
 
 #define SNUMOPT		4
@@ -2947,7 +2945,7 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 #define SYMV_P	16
 #endif
 
-#if defined(MIPS64_GENERIC) || defined(P5600) || defined(MIPS1004K) || defined(MIPS24K) || defined(I6400) || defined(P6600) || defined(I6500)
+#if defined(P5600) || defined(MIPS1004K) || defined(MIPS24K) || defined(I6400) || defined(P6600) || defined(I6500)
 #define SNUMOPT  2
 #define DNUMOPT  2
 
@@ -3038,6 +3036,45 @@ USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 
 #endif
 
+#ifdef C908V
+#define GEMM_DEFAULT_OFFSET_A 0
+#define GEMM_DEFAULT_OFFSET_B 0
+#define GEMM_DEFAULT_ALIGN 0x03fffUL
+
+#define SGEMM_DEFAULT_UNROLL_M  16
+#define SGEMM_DEFAULT_UNROLL_N  4
+
+#define DGEMM_DEFAULT_UNROLL_M  8
+#define DGEMM_DEFAULT_UNROLL_N  4
+
+#define CGEMM_DEFAULT_UNROLL_M  2
+#define CGEMM_DEFAULT_UNROLL_N  2
+
+#define ZGEMM_DEFAULT_UNROLL_M  2
+#define ZGEMM_DEFAULT_UNROLL_N  2
+
+#define SGEMM_DEFAULT_P	160
+#define DGEMM_DEFAULT_P	160
+#define CGEMM_DEFAULT_P 96
+#define ZGEMM_DEFAULT_P 64
+
+#define SGEMM_DEFAULT_Q 240
+#define DGEMM_DEFAULT_Q 128
+#define CGEMM_DEFAULT_Q 120
+#define ZGEMM_DEFAULT_Q 120
+
+#define SGEMM_DEFAULT_R 12288
+#define DGEMM_DEFAULT_R 8192
+#define CGEMM_DEFAULT_R 4096
+#define ZGEMM_DEFAULT_R 4096
+
+#define SYMV_P	16
+
+#define GEMM_DEFAULT_OFFSET_A 0
+#define GEMM_DEFAULT_OFFSET_B 0
+
+#endif
+
 #ifdef C910V
 #define GEMM_DEFAULT_OFFSET_A 0
 #define GEMM_DEFAULT_OFFSET_B 0
@@ -3396,9 +3433,6 @@ is a big desktop or server with abundant cache rather than a phone or embedded d
 
 #elif defined(NEOVERSEN2)
 
-#undef SBGEMM_ALIGN_K
-#define SBGEMM_ALIGN_K 4
-
 #undef SBGEMM_DEFAULT_UNROLL_M
 #undef SBGEMM_DEFAULT_UNROLL_N
 #define SBGEMM_DEFAULT_UNROLL_M 8
diff --git a/riscv_vector_itr.h b/riscv_vector_itr.h
new file mode 100644
index 00000000..bd1323e5
--- /dev/null
+++ b/riscv_vector_itr.h
@@ -0,0 +1,5135 @@
+/* DO NOT EDIT, please edit generator instead.
+   This file was generated by gen-vector-iterator with the command:
+   $ ./gen-vector-iterator -usr-c > riscv_vector_itr.h  */
+// #ifndef _GCC_RISCV_VECTOR_H
+
+// #error "Never included riscv_vector_itr.h, plz include riscv_vector.h"
+
+// #endif
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector and integer types.  */
+#define _RVV_INT_ITERATOR(MACRO) \
+  MACRO (8, f8, 64, int8_t) \
+  MACRO (8, f4, 32, int8_t) \
+  MACRO (8, f2, 16, int8_t) \
+  MACRO (8, 1, 8, int8_t) \
+  MACRO (8, 2, 4, int8_t) \
+  MACRO (8, 4, 2, int8_t) \
+  MACRO (8, 8, 1, int8_t) \
+  MACRO (16, f4, 64, int16_t) \
+  MACRO (16, f2, 32, int16_t) \
+  MACRO (16, 1, 16, int16_t) \
+  MACRO (16, 2, 8, int16_t) \
+  MACRO (16, 4, 4, int16_t) \
+  MACRO (16, 8, 2, int16_t) \
+  MACRO (32, f2, 64, int32_t) \
+  MACRO (32, 1, 32, int32_t) \
+  MACRO (32, 2, 16, int32_t) \
+  MACRO (32, 4, 8, int32_t) \
+  MACRO (32, 8, 4, int32_t) \
+  MACRO (64, 1, 64, int64_t) \
+  MACRO (64, 2, 32, int64_t) \
+  MACRO (64, 4, 16, int64_t) \
+  MACRO (64, 8, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, f8, 64, int8_t, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding widening vector type.  */
+#define _RVV_WINT_ITERATOR(MACRO) \
+  MACRO (8, f8, 64, int8_t, 16, f4, int16_t) \
+  MACRO (8, f4, 32, int8_t, 16, f2, int16_t) \
+  MACRO (8, f2, 16, int8_t, 16, 1, int16_t) \
+  MACRO (8, 1, 8, int8_t, 16, 2, int16_t) \
+  MACRO (8, 2, 4, int8_t, 16, 4, int16_t) \
+  MACRO (8, 4, 2, int8_t, 16, 8, int16_t) \
+  MACRO (16, f4, 64, int16_t, 32, f2, int32_t) \
+  MACRO (16, f2, 32, int16_t, 32, 1, int32_t) \
+  MACRO (16, 1, 16, int16_t, 32, 2, int32_t) \
+  MACRO (16, 2, 8, int16_t, 32, 4, int32_t) \
+  MACRO (16, 4, 4, int16_t, 32, 8, int32_t) \
+  MACRO (32, f2, 64, int32_t, 64, 1, int64_t) \
+  MACRO (32, 1, 32, int32_t, 64, 2, int64_t) \
+  MACRO (32, 2, 16, int32_t, 64, 4, int64_t) \
+  MACRO (32, 4, 8, int32_t, 64, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, f8, 64, int8_t, 16, f4, int16_t, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 16, f2, int16_t, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 16, 2, int16_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 4, int16_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 32, f2, int32_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 2, int32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 4, int32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 8, int32_t, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 2, int64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 4, int64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding widening vector type but with LMUL 1.  */
+#define _RVV_WRED_INT_ITERATOR(MACRO) \
+  MACRO (8, f8, 64, int8_t, 16, 1, int16_t) \
+  MACRO (8, f4, 32, int8_t, 16, 1, int16_t) \
+  MACRO (8, f2, 16, int8_t, 16, 1, int16_t) \
+  MACRO (8, 1, 8, int8_t, 16, 1, int16_t) \
+  MACRO (8, 2, 4, int8_t, 16, 1, int16_t) \
+  MACRO (8, 4, 2, int8_t, 16, 1, int16_t) \
+  MACRO (8, 8, 1, int8_t, 16, 1, int16_t) \
+  MACRO (16, f4, 64, int16_t, 32, 1, int32_t) \
+  MACRO (16, f2, 32, int16_t, 32, 1, int32_t) \
+  MACRO (16, 1, 16, int16_t, 32, 1, int32_t) \
+  MACRO (16, 2, 8, int16_t, 32, 1, int32_t) \
+  MACRO (16, 4, 4, int16_t, 32, 1, int32_t) \
+  MACRO (16, 8, 2, int16_t, 32, 1, int32_t) \
+  MACRO (32, f2, 64, int32_t, 64, 1, int64_t) \
+  MACRO (32, 1, 32, int32_t, 64, 1, int64_t) \
+  MACRO (32, 2, 16, int32_t, 64, 1, int64_t) \
+  MACRO (32, 4, 8, int32_t, 64, 1, int64_t) \
+  MACRO (32, 8, 4, int32_t, 64, 1, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WRED_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, f8, 64, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 16, 1, int16_t, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 64, 1, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding quad widening vector type.  */
+#define _RVV_QINT_ITERATOR(MACRO) \
+  MACRO (8, f8, 64, int8_t, 32, f2, int32_t) \
+  MACRO (8, f4, 32, int8_t, 32, 1, int32_t) \
+  MACRO (8, f2, 16, int8_t, 32, 2, int32_t) \
+  MACRO (8, 1, 8, int8_t, 32, 4, int32_t) \
+  MACRO (8, 2, 4, int8_t, 32, 8, int32_t) \
+  MACRO (16, f4, 64, int16_t, 64, 1, int64_t) \
+  MACRO (16, f2, 32, int16_t, 64, 2, int64_t) \
+  MACRO (16, 1, 16, int16_t, 64, 4, int64_t) \
+  MACRO (16, 2, 8, int16_t, 64, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_QINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, f8, 64, int8_t, 32, f2, int32_t, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 32, 1, int32_t, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 32, 2, int32_t, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 32, 4, int32_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 32, 8, int32_t, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 64, 2, int64_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 64, 4, int64_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 64, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer modes, and info for
+   corresponding eightfold widening vector type.  */
+#define _RVV_EINT_ITERATOR(MACRO) \
+  MACRO (8, f8, 64, int8_t, 64, 1, int64_t) \
+  MACRO (8, f4, 32, int8_t, 64, 2, int64_t) \
+  MACRO (8, f2, 16, int8_t, 64, 4, int64_t) \
+  MACRO (8, 1, 8, int8_t, 64, 8, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_EINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, f8, 64, int8_t, 64, 1, int64_t, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 64, 2, int64_t, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 64, 4, int64_t, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 64, 8, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector and floating point modes.  */
+#define _RVV_FLOAT_ITERATOR(MACRO) \
+  MACRO (16, f4, 64, __float16_t) \
+  MACRO (16, f2, 32, __float16_t) \
+  MACRO (16, 1, 16, __float16_t) \
+  MACRO (16, 2, 8, __float16_t) \
+  MACRO (16, 4, 4, __float16_t) \
+  MACRO (16, 8, 2, __float16_t) \
+  MACRO (32, f2, 64, __float32_t) \
+  MACRO (32, 1, 32, __float32_t) \
+  MACRO (32, 2, 16, __float32_t) \
+  MACRO (32, 4, 8, __float32_t) \
+  MACRO (32, 8, 4, __float32_t) \
+  MACRO (64, 1, 64, __float64_t) \
+  MACRO (64, 2, 32, __float64_t) \
+  MACRO (64, 4, 16, __float64_t) \
+  MACRO (64, 8, 8, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, f4, 64, __float16_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating modes, and info for
+   corresponding widening vector type.  */
+#define _RVV_WFLOAT_ITERATOR(MACRO) \
+  MACRO (16, f4, 64, __float16_t, 32, f2, __float32_t) \
+  MACRO (16, f2, 32, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 1, 16, __float16_t, 32, 2, __float32_t) \
+  MACRO (16, 2, 8, __float16_t, 32, 4, __float32_t) \
+  MACRO (16, 4, 4, __float16_t, 32, 8, __float32_t) \
+  MACRO (32, f2, 64, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 1, 32, __float32_t, 64, 2, __float64_t) \
+  MACRO (32, 2, 16, __float32_t, 64, 4, __float64_t) \
+  MACRO (32, 4, 8, __float32_t, 64, 8, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WFLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, f4, 64, __float16_t, 32, f2, __float32_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 32, 2, __float32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 32, 4, __float32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 32, 8, __float32_t, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 64, 2, __float64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 64, 4, __float64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 64, 8, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating types, and info for
+   corresponding widening vector type but with LMUL 1.  */
+#define _RVV_WRED_FLOAT_ITERATOR(MACRO) \
+  MACRO (16, f4, 64, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, f2, 32, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 1, 16, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 2, 8, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 4, 4, __float16_t, 32, 1, __float32_t) \
+  MACRO (16, 8, 2, __float16_t, 32, 1, __float32_t) \
+  MACRO (32, f2, 64, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 1, 32, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 2, 16, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 4, 8, __float32_t, 64, 1, __float64_t) \
+  MACRO (32, 8, 4, __float32_t, 64, 1, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_WRED_FLOAT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, f4, 64, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 32, 1, __float32_t, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 64, 1, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding integer and vector type.  */
+#define _RVV_FLOAT_INT_ITERATOR(MACRO) \
+  MACRO (16, f4, 64, __float16_t, int16_t) \
+  MACRO (16, f2, 32, __float16_t, int16_t) \
+  MACRO (16, 1, 16, __float16_t, int16_t) \
+  MACRO (16, 2, 8, __float16_t, int16_t) \
+  MACRO (16, 4, 4, __float16_t, int16_t) \
+  MACRO (16, 8, 2, __float16_t, int16_t) \
+  MACRO (32, f2, 64, __float32_t, int32_t) \
+  MACRO (32, 1, 32, __float32_t, int32_t) \
+  MACRO (32, 2, 16, __float32_t, int32_t) \
+  MACRO (32, 4, 8, __float32_t, int32_t) \
+  MACRO (32, 8, 4, __float32_t, int32_t) \
+  MACRO (64, 1, 64, __float64_t, int64_t) \
+  MACRO (64, 2, 32, __float64_t, int64_t) \
+  MACRO (64, 4, 16, __float64_t, int64_t) \
+  MACRO (64, 8, 8, __float64_t, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_INT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, f4, 64, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, int16_t, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, int64_t, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding integer and vector type.  */
+#define _RVV_INT_INDEX_ITERATOR(MACRO) \
+  MACRO (8, f8, 64, int8_t, 8, f8) \
+  MACRO (8, f8, 64, int8_t, 16, f4) \
+  MACRO (8, f8, 64, int8_t, 32, f2) \
+  MACRO (8, f8, 64, int8_t, 64, 1) \
+  MACRO (8, f4, 32, int8_t, 8, f4) \
+  MACRO (8, f4, 32, int8_t, 16, f2) \
+  MACRO (8, f4, 32, int8_t, 32, 1) \
+  MACRO (8, f4, 32, int8_t, 64, 2) \
+  MACRO (8, f2, 16, int8_t, 8, f2) \
+  MACRO (8, f2, 16, int8_t, 16, 1) \
+  MACRO (8, f2, 16, int8_t, 32, 2) \
+  MACRO (8, f2, 16, int8_t, 64, 4) \
+  MACRO (8, 1, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 8, int8_t, 64, 8) \
+  MACRO (8, 2, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 4, int8_t, 32, 8) \
+  MACRO (8, 4, 2, int8_t, 8, 4) \
+  MACRO (8, 4, 2, int8_t, 16, 8) \
+  MACRO (8, 8, 1, int8_t, 8, 8) \
+  MACRO (16, f4, 64, int16_t, 8, f8) \
+  MACRO (16, f4, 64, int16_t, 16, f4) \
+  MACRO (16, f4, 64, int16_t, 32, f2) \
+  MACRO (16, f4, 64, int16_t, 64, 1) \
+  MACRO (16, f2, 32, int16_t, 8, f4) \
+  MACRO (16, f2, 32, int16_t, 16, f2) \
+  MACRO (16, f2, 32, int16_t, 32, 1) \
+  MACRO (16, f2, 32, int16_t, 64, 2) \
+  MACRO (16, 1, 16, int16_t, 8, f2) \
+  MACRO (16, 1, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 16, int16_t, 64, 4) \
+  MACRO (16, 2, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 8, int16_t, 64, 8) \
+  MACRO (16, 4, 4, int16_t, 8, 2) \
+  MACRO (16, 4, 4, int16_t, 16, 4) \
+  MACRO (16, 4, 4, int16_t, 32, 8) \
+  MACRO (16, 8, 2, int16_t, 8, 4) \
+  MACRO (16, 8, 2, int16_t, 16, 8) \
+  MACRO (32, f2, 64, int32_t, 8, f8) \
+  MACRO (32, f2, 64, int32_t, 16, f4) \
+  MACRO (32, f2, 64, int32_t, 32, f2) \
+  MACRO (32, f2, 64, int32_t, 64, 1) \
+  MACRO (32, 1, 32, int32_t, 8, f4) \
+  MACRO (32, 1, 32, int32_t, 16, f2) \
+  MACRO (32, 1, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 32, int32_t, 64, 2) \
+  MACRO (32, 2, 16, int32_t, 8, f2) \
+  MACRO (32, 2, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 16, int32_t, 64, 4) \
+  MACRO (32, 4, 8, int32_t, 8, 1) \
+  MACRO (32, 4, 8, int32_t, 16, 2) \
+  MACRO (32, 4, 8, int32_t, 32, 4) \
+  MACRO (32, 4, 8, int32_t, 64, 8) \
+  MACRO (32, 8, 4, int32_t, 8, 2) \
+  MACRO (32, 8, 4, int32_t, 16, 4) \
+  MACRO (32, 8, 4, int32_t, 32, 8) \
+  MACRO (64, 1, 64, int64_t, 8, f8) \
+  MACRO (64, 1, 64, int64_t, 16, f4) \
+  MACRO (64, 1, 64, int64_t, 32, f2) \
+  MACRO (64, 1, 64, int64_t, 64, 1) \
+  MACRO (64, 2, 32, int64_t, 8, f4) \
+  MACRO (64, 2, 32, int64_t, 16, f2) \
+  MACRO (64, 2, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 32, int64_t, 64, 2) \
+  MACRO (64, 4, 16, int64_t, 8, f2) \
+  MACRO (64, 4, 16, int64_t, 16, 1) \
+  MACRO (64, 4, 16, int64_t, 32, 2) \
+  MACRO (64, 4, 16, int64_t, 64, 4) \
+  MACRO (64, 8, 8, int64_t, 8, 1) \
+  MACRO (64, 8, 8, int64_t, 16, 2) \
+  MACRO (64, 8, 8, int64_t, 32, 4) \
+  MACRO (64, 8, 8, int64_t, 64, 8) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, f8, 64, int8_t, 8, f8, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 16, f4, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 32, f2, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 64, 1, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 8, f4, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 16, f2, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 32, 1, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 64, 2, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 8, f2, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 16, 1, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 32, 2, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 64, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 8, f8, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 32, f2, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 64, 1, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 8, f4, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 32, 1, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 64, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 8, f2, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 8, f8, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 16, f4, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 64, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 8, f4, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 16, f2, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 8, f2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 8, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 8, f8, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 16, f4, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 32, f2, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 8, f4, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 16, f2, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 8, f2, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 8, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 16, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 8, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point types, and info for
+   corresponding floating point and vector type.  */
+#define _RVV_FLOAT_INDEX_ITERATOR(MACRO) \
+  MACRO (16, f4, 64, __float16_t, 8, f8) \
+  MACRO (16, f4, 64, __float16_t, 16, f4) \
+  MACRO (16, f4, 64, __float16_t, 32, f2) \
+  MACRO (16, f4, 64, __float16_t, 64, 1) \
+  MACRO (16, f2, 32, __float16_t, 8, f4) \
+  MACRO (16, f2, 32, __float16_t, 16, f2) \
+  MACRO (16, f2, 32, __float16_t, 32, 1) \
+  MACRO (16, f2, 32, __float16_t, 64, 2) \
+  MACRO (16, 1, 16, __float16_t, 8, f2) \
+  MACRO (16, 1, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 16, __float16_t, 64, 4) \
+  MACRO (16, 2, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 8, __float16_t, 64, 8) \
+  MACRO (16, 4, 4, __float16_t, 8, 2) \
+  MACRO (16, 4, 4, __float16_t, 16, 4) \
+  MACRO (16, 4, 4, __float16_t, 32, 8) \
+  MACRO (16, 8, 2, __float16_t, 8, 4) \
+  MACRO (16, 8, 2, __float16_t, 16, 8) \
+  MACRO (32, f2, 64, __float32_t, 8, f8) \
+  MACRO (32, f2, 64, __float32_t, 16, f4) \
+  MACRO (32, f2, 64, __float32_t, 32, f2) \
+  MACRO (32, f2, 64, __float32_t, 64, 1) \
+  MACRO (32, 1, 32, __float32_t, 8, f4) \
+  MACRO (32, 1, 32, __float32_t, 16, f2) \
+  MACRO (32, 1, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 32, __float32_t, 64, 2) \
+  MACRO (32, 2, 16, __float32_t, 8, f2) \
+  MACRO (32, 2, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 16, __float32_t, 64, 4) \
+  MACRO (32, 4, 8, __float32_t, 8, 1) \
+  MACRO (32, 4, 8, __float32_t, 16, 2) \
+  MACRO (32, 4, 8, __float32_t, 32, 4) \
+  MACRO (32, 4, 8, __float32_t, 64, 8) \
+  MACRO (32, 8, 4, __float32_t, 8, 2) \
+  MACRO (32, 8, 4, __float32_t, 16, 4) \
+  MACRO (32, 8, 4, __float32_t, 32, 8) \
+  MACRO (64, 1, 64, __float64_t, 8, f8) \
+  MACRO (64, 1, 64, __float64_t, 16, f4) \
+  MACRO (64, 1, 64, __float64_t, 32, f2) \
+  MACRO (64, 1, 64, __float64_t, 64, 1) \
+  MACRO (64, 2, 32, __float64_t, 8, f4) \
+  MACRO (64, 2, 32, __float64_t, 16, f2) \
+  MACRO (64, 2, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 32, __float64_t, 64, 2) \
+  MACRO (64, 4, 16, __float64_t, 8, f2) \
+  MACRO (64, 4, 16, __float64_t, 16, 1) \
+  MACRO (64, 4, 16, __float64_t, 32, 2) \
+  MACRO (64, 4, 16, __float64_t, 64, 4) \
+  MACRO (64, 8, 8, __float64_t, 8, 1) \
+  MACRO (64, 8, 8, __float64_t, 16, 2) \
+  MACRO (64, 8, 8, __float64_t, 32, 4) \
+  MACRO (64, 8, 8, __float64_t, 64, 8) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, f4, 64, __float16_t, 8, f8, __VA_ARGS__) \
+  MACRO (16, f4, 64, __float16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, f4, 64, __float16_t, 32, f2, __VA_ARGS__) \
+  MACRO (16, f4, 64, __float16_t, 64, 1, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 8, f4, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 32, 1, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 64, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 8, f2, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 8, f8, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 16, f4, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 64, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 8, f4, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 16, f2, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 8, f2, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 8, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 8, f8, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 16, f4, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 32, f2, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 8, f4, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 16, f2, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 8, f2, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 8, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 16, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 8, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding integer and vector type.  */
+#define _RVV_INT_REINT_ITERATOR(MACRO) \
+  MACRO (8, f8, 64, int8_t, 8, f4) \
+  MACRO (8, f8, 64, int8_t, 8, f2) \
+  MACRO (8, f8, 64, int8_t, 8, 1) \
+  MACRO (8, f8, 64, int8_t, 8, 2) \
+  MACRO (8, f8, 64, int8_t, 8, 4) \
+  MACRO (8, f8, 64, int8_t, 8, 8) \
+  MACRO (8, f4, 32, int8_t, 16, f4) \
+  MACRO (8, f4, 32, int8_t, 8, f8) \
+  MACRO (8, f4, 32, int8_t, 8, f2) \
+  MACRO (8, f4, 32, int8_t, 8, 1) \
+  MACRO (8, f4, 32, int8_t, 8, 2) \
+  MACRO (8, f4, 32, int8_t, 8, 4) \
+  MACRO (8, f4, 32, int8_t, 8, 8) \
+  MACRO (8, f2, 16, int8_t, 16, f2) \
+  MACRO (8, f2, 16, int8_t, 32, f2) \
+  MACRO (8, f2, 16, int8_t, 8, f8) \
+  MACRO (8, f2, 16, int8_t, 8, f4) \
+  MACRO (8, f2, 16, int8_t, 8, 1) \
+  MACRO (8, f2, 16, int8_t, 8, 2) \
+  MACRO (8, f2, 16, int8_t, 8, 4) \
+  MACRO (8, f2, 16, int8_t, 8, 8) \
+  MACRO (8, 1, 8, int8_t, 16, 1) \
+  MACRO (8, 1, 8, int8_t, 32, 1) \
+  MACRO (8, 1, 8, int8_t, 64, 1) \
+  MACRO (8, 1, 8, int8_t, 8, f8) \
+  MACRO (8, 1, 8, int8_t, 8, f4) \
+  MACRO (8, 1, 8, int8_t, 8, f2) \
+  MACRO (8, 1, 8, int8_t, 8, 2) \
+  MACRO (8, 1, 8, int8_t, 8, 4) \
+  MACRO (8, 1, 8, int8_t, 8, 8) \
+  MACRO (8, 2, 4, int8_t, 16, 2) \
+  MACRO (8, 2, 4, int8_t, 32, 2) \
+  MACRO (8, 2, 4, int8_t, 64, 2) \
+  MACRO (8, 2, 4, int8_t, 8, f8) \
+  MACRO (8, 2, 4, int8_t, 8, f4) \
+  MACRO (8, 2, 4, int8_t, 8, f2) \
+  MACRO (8, 2, 4, int8_t, 8, 1) \
+  MACRO (8, 2, 4, int8_t, 8, 4) \
+  MACRO (8, 2, 4, int8_t, 8, 8) \
+  MACRO (8, 4, 2, int8_t, 16, 4) \
+  MACRO (8, 4, 2, int8_t, 32, 4) \
+  MACRO (8, 4, 2, int8_t, 64, 4) \
+  MACRO (8, 4, 2, int8_t, 8, f8) \
+  MACRO (8, 4, 2, int8_t, 8, f4) \
+  MACRO (8, 4, 2, int8_t, 8, f2) \
+  MACRO (8, 4, 2, int8_t, 8, 1) \
+  MACRO (8, 4, 2, int8_t, 8, 2) \
+  MACRO (8, 4, 2, int8_t, 8, 8) \
+  MACRO (8, 8, 1, int8_t, 16, 8) \
+  MACRO (8, 8, 1, int8_t, 32, 8) \
+  MACRO (8, 8, 1, int8_t, 64, 8) \
+  MACRO (8, 8, 1, int8_t, 8, f8) \
+  MACRO (8, 8, 1, int8_t, 8, f4) \
+  MACRO (8, 8, 1, int8_t, 8, f2) \
+  MACRO (8, 8, 1, int8_t, 8, 1) \
+  MACRO (8, 8, 1, int8_t, 8, 2) \
+  MACRO (8, 8, 1, int8_t, 8, 4) \
+  MACRO (16, f4, 64, int16_t, 8, f4) \
+  MACRO (16, f4, 64, int16_t, 16, f2) \
+  MACRO (16, f4, 64, int16_t, 16, 1) \
+  MACRO (16, f4, 64, int16_t, 16, 2) \
+  MACRO (16, f4, 64, int16_t, 16, 4) \
+  MACRO (16, f4, 64, int16_t, 16, 8) \
+  MACRO (16, f2, 32, int16_t, 8, f2) \
+  MACRO (16, f2, 32, int16_t, 32, f2) \
+  MACRO (16, f2, 32, int16_t, 16, f4) \
+  MACRO (16, f2, 32, int16_t, 16, 1) \
+  MACRO (16, f2, 32, int16_t, 16, 2) \
+  MACRO (16, f2, 32, int16_t, 16, 4) \
+  MACRO (16, f2, 32, int16_t, 16, 8) \
+  MACRO (16, 1, 16, int16_t, 8, 1) \
+  MACRO (16, 1, 16, int16_t, 32, 1) \
+  MACRO (16, 1, 16, int16_t, 64, 1) \
+  MACRO (16, 1, 16, int16_t, 16, f4) \
+  MACRO (16, 1, 16, int16_t, 16, f2) \
+  MACRO (16, 1, 16, int16_t, 16, 2) \
+  MACRO (16, 1, 16, int16_t, 16, 4) \
+  MACRO (16, 1, 16, int16_t, 16, 8) \
+  MACRO (16, 2, 8, int16_t, 8, 2) \
+  MACRO (16, 2, 8, int16_t, 32, 2) \
+  MACRO (16, 2, 8, int16_t, 64, 2) \
+  MACRO (16, 2, 8, int16_t, 16, f4) \
+  MACRO (16, 2, 8, int16_t, 16, f2) \
+  MACRO (16, 2, 8, int16_t, 16, 1) \
+  MACRO (16, 2, 8, int16_t, 16, 4) \
+  MACRO (16, 2, 8, int16_t, 16, 8) \
+  MACRO (16, 4, 4, int16_t, 8, 4) \
+  MACRO (16, 4, 4, int16_t, 32, 4) \
+  MACRO (16, 4, 4, int16_t, 64, 4) \
+  MACRO (16, 4, 4, int16_t, 16, f4) \
+  MACRO (16, 4, 4, int16_t, 16, f2) \
+  MACRO (16, 4, 4, int16_t, 16, 1) \
+  MACRO (16, 4, 4, int16_t, 16, 2) \
+  MACRO (16, 4, 4, int16_t, 16, 8) \
+  MACRO (16, 8, 2, int16_t, 8, 8) \
+  MACRO (16, 8, 2, int16_t, 32, 8) \
+  MACRO (16, 8, 2, int16_t, 64, 8) \
+  MACRO (16, 8, 2, int16_t, 16, f4) \
+  MACRO (16, 8, 2, int16_t, 16, f2) \
+  MACRO (16, 8, 2, int16_t, 16, 1) \
+  MACRO (16, 8, 2, int16_t, 16, 2) \
+  MACRO (16, 8, 2, int16_t, 16, 4) \
+  MACRO (32, f2, 64, int32_t, 8, f2) \
+  MACRO (32, f2, 64, int32_t, 16, f2) \
+  MACRO (32, f2, 64, int32_t, 32, 1) \
+  MACRO (32, f2, 64, int32_t, 32, 2) \
+  MACRO (32, f2, 64, int32_t, 32, 4) \
+  MACRO (32, f2, 64, int32_t, 32, 8) \
+  MACRO (32, 1, 32, int32_t, 8, 1) \
+  MACRO (32, 1, 32, int32_t, 16, 1) \
+  MACRO (32, 1, 32, int32_t, 64, 1) \
+  MACRO (32, 1, 32, int32_t, 32, f2) \
+  MACRO (32, 1, 32, int32_t, 32, 2) \
+  MACRO (32, 1, 32, int32_t, 32, 4) \
+  MACRO (32, 1, 32, int32_t, 32, 8) \
+  MACRO (32, 2, 16, int32_t, 8, 2) \
+  MACRO (32, 2, 16, int32_t, 16, 2) \
+  MACRO (32, 2, 16, int32_t, 64, 2) \
+  MACRO (32, 2, 16, int32_t, 32, f2) \
+  MACRO (32, 2, 16, int32_t, 32, 1) \
+  MACRO (32, 2, 16, int32_t, 32, 4) \
+  MACRO (32, 2, 16, int32_t, 32, 8) \
+  MACRO (32, 4, 8, int32_t, 8, 4) \
+  MACRO (32, 4, 8, int32_t, 16, 4) \
+  MACRO (32, 4, 8, int32_t, 64, 4) \
+  MACRO (32, 4, 8, int32_t, 32, f2) \
+  MACRO (32, 4, 8, int32_t, 32, 1) \
+  MACRO (32, 4, 8, int32_t, 32, 2) \
+  MACRO (32, 4, 8, int32_t, 32, 8) \
+  MACRO (32, 8, 4, int32_t, 8, 8) \
+  MACRO (32, 8, 4, int32_t, 16, 8) \
+  MACRO (32, 8, 4, int32_t, 64, 8) \
+  MACRO (32, 8, 4, int32_t, 32, f2) \
+  MACRO (32, 8, 4, int32_t, 32, 1) \
+  MACRO (32, 8, 4, int32_t, 32, 2) \
+  MACRO (32, 8, 4, int32_t, 32, 4) \
+  MACRO (64, 1, 64, int64_t, 8, 1) \
+  MACRO (64, 1, 64, int64_t, 16, 1) \
+  MACRO (64, 1, 64, int64_t, 32, 1) \
+  MACRO (64, 1, 64, int64_t, 64, 2) \
+  MACRO (64, 1, 64, int64_t, 64, 4) \
+  MACRO (64, 1, 64, int64_t, 64, 8) \
+  MACRO (64, 2, 32, int64_t, 8, 2) \
+  MACRO (64, 2, 32, int64_t, 16, 2) \
+  MACRO (64, 2, 32, int64_t, 32, 2) \
+  MACRO (64, 2, 32, int64_t, 64, 1) \
+  MACRO (64, 2, 32, int64_t, 64, 4) \
+  MACRO (64, 2, 32, int64_t, 64, 8) \
+  MACRO (64, 4, 16, int64_t, 8, 4) \
+  MACRO (64, 4, 16, int64_t, 16, 4) \
+  MACRO (64, 4, 16, int64_t, 32, 4) \
+  MACRO (64, 4, 16, int64_t, 64, 1) \
+  MACRO (64, 4, 16, int64_t, 64, 2) \
+  MACRO (64, 4, 16, int64_t, 64, 8) \
+  MACRO (64, 8, 8, int64_t, 8, 8) \
+  MACRO (64, 8, 8, int64_t, 16, 8) \
+  MACRO (64, 8, 8, int64_t, 32, 8) \
+  MACRO (64, 8, 8, int64_t, 64, 1) \
+  MACRO (64, 8, 8, int64_t, 64, 2) \
+  MACRO (64, 8, 8, int64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_REINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, f8, 64, int8_t, 8, f4, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 8, f2, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, f8, 64, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 16, f4, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 8, f8, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 8, f2, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, f4, 32, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 16, f2, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 32, f2, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 8, f8, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 8, f4, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, f2, 16, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 16, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 32, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 64, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, f8, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, f4, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, f2, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 32, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 64, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, f8, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, f4, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, f2, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 2, 4, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 64, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, f8, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, f4, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, f2, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 4, 2, int8_t, 8, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 16, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, f8, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, f4, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, f2, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 8, 1, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 8, f4, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, f4, 64, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 8, f2, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 32, f2, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, f2, 32, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 32, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 64, 1, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 1, 16, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 64, 2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 8, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, int16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 8, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 8, 2, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 8, f2, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 16, f2, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, f2, 64, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 64, 1, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 1, 32, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 8, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 2, 16, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 8, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, int32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 8, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 16, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 8, 1, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, int64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 8, 2, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 16, 2, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 2, 32, int64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 8, 4, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 16, 4, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, int64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 8, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 16, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 32, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, int64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point types, and info for
+   corresponding floating and vector type.  */
+#define _RVV_FLOAT_REINT_ITERATOR(MACRO) \
+  MACRO (16, f4, 64, __float16_t, 16, f2) \
+  MACRO (16, f4, 64, __float16_t, 16, 1) \
+  MACRO (16, f4, 64, __float16_t, 16, 2) \
+  MACRO (16, f4, 64, __float16_t, 16, 4) \
+  MACRO (16, f4, 64, __float16_t, 16, 8) \
+  MACRO (16, f2, 32, __float16_t, 16, f4) \
+  MACRO (16, f2, 32, __float16_t, 16, 1) \
+  MACRO (16, f2, 32, __float16_t, 16, 2) \
+  MACRO (16, f2, 32, __float16_t, 16, 4) \
+  MACRO (16, f2, 32, __float16_t, 16, 8) \
+  MACRO (16, 1, 16, __float16_t, 16, f4) \
+  MACRO (16, 1, 16, __float16_t, 16, f2) \
+  MACRO (16, 1, 16, __float16_t, 16, 2) \
+  MACRO (16, 1, 16, __float16_t, 16, 4) \
+  MACRO (16, 1, 16, __float16_t, 16, 8) \
+  MACRO (16, 2, 8, __float16_t, 16, f4) \
+  MACRO (16, 2, 8, __float16_t, 16, f2) \
+  MACRO (16, 2, 8, __float16_t, 16, 1) \
+  MACRO (16, 2, 8, __float16_t, 16, 4) \
+  MACRO (16, 2, 8, __float16_t, 16, 8) \
+  MACRO (16, 4, 4, __float16_t, 16, f4) \
+  MACRO (16, 4, 4, __float16_t, 16, f2) \
+  MACRO (16, 4, 4, __float16_t, 16, 1) \
+  MACRO (16, 4, 4, __float16_t, 16, 2) \
+  MACRO (16, 4, 4, __float16_t, 16, 8) \
+  MACRO (16, 8, 2, __float16_t, 16, f4) \
+  MACRO (16, 8, 2, __float16_t, 16, f2) \
+  MACRO (16, 8, 2, __float16_t, 16, 1) \
+  MACRO (16, 8, 2, __float16_t, 16, 2) \
+  MACRO (16, 8, 2, __float16_t, 16, 4) \
+  MACRO (32, f2, 64, __float32_t, 32, 1) \
+  MACRO (32, f2, 64, __float32_t, 32, 2) \
+  MACRO (32, f2, 64, __float32_t, 32, 4) \
+  MACRO (32, f2, 64, __float32_t, 32, 8) \
+  MACRO (32, 1, 32, __float32_t, 32, f2) \
+  MACRO (32, 1, 32, __float32_t, 32, 2) \
+  MACRO (32, 1, 32, __float32_t, 32, 4) \
+  MACRO (32, 1, 32, __float32_t, 32, 8) \
+  MACRO (32, 2, 16, __float32_t, 32, f2) \
+  MACRO (32, 2, 16, __float32_t, 32, 1) \
+  MACRO (32, 2, 16, __float32_t, 32, 4) \
+  MACRO (32, 2, 16, __float32_t, 32, 8) \
+  MACRO (32, 4, 8, __float32_t, 32, f2) \
+  MACRO (32, 4, 8, __float32_t, 32, 1) \
+  MACRO (32, 4, 8, __float32_t, 32, 2) \
+  MACRO (32, 4, 8, __float32_t, 32, 8) \
+  MACRO (32, 8, 4, __float32_t, 32, f2) \
+  MACRO (32, 8, 4, __float32_t, 32, 1) \
+  MACRO (32, 8, 4, __float32_t, 32, 2) \
+  MACRO (32, 8, 4, __float32_t, 32, 4) \
+  MACRO (64, 1, 64, __float64_t, 64, 2) \
+  MACRO (64, 1, 64, __float64_t, 64, 4) \
+  MACRO (64, 1, 64, __float64_t, 64, 8) \
+  MACRO (64, 2, 32, __float64_t, 64, 1) \
+  MACRO (64, 2, 32, __float64_t, 64, 4) \
+  MACRO (64, 2, 32, __float64_t, 64, 8) \
+  MACRO (64, 4, 16, __float64_t, 64, 1) \
+  MACRO (64, 4, 16, __float64_t, 64, 2) \
+  MACRO (64, 4, 16, __float64_t, 64, 8) \
+  MACRO (64, 8, 8, __float64_t, 64, 1) \
+  MACRO (64, 8, 8, __float64_t, 64, 2) \
+  MACRO (64, 8, 8, __float64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_REINT_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, f4, 64, __float16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, f4, 64, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, f4, 64, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, f4, 64, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, f4, 64, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, f2, 32, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 1, 16, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 2, 8, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 4, 4, __float16_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, f4, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, f2, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 8, 2, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, f2, 64, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 1, 32, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 2, 16, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 4, 8, __float32_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, f2, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 8, 4, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 64, __float64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 2, 32, __float64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 16, __float64_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 8, 8, __float64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, integer types, and info for
+   corresponding widening vector LMUL.  */
+#define _RVV_INT_WLMUL_ITERATOR(MACRO) \
+  MACRO (8, 2, 1, 8, int8_t) \
+  MACRO (8, 4, 2, 4, int8_t) \
+  MACRO (8, 8, 4, 2, int8_t) \
+  MACRO (16, 2, 1, 16, int16_t) \
+  MACRO (16, 4, 2, 8, int16_t) \
+  MACRO (16, 8, 4, 4, int16_t) \
+  MACRO (32, 2, 1, 32, int32_t) \
+  MACRO (32, 4, 2, 16, int32_t) \
+  MACRO (32, 8, 4, 8, int32_t) \
+  MACRO (64, 2, 1, 64, int64_t) \
+  MACRO (64, 4, 2, 32, int64_t) \
+  MACRO (64, 8, 4, 16, int64_t) \
+  MACRO (8, 4, 1, 8, int8_t) \
+  MACRO (8, 8, 2, 4, int8_t) \
+  MACRO (16, 4, 1, 16, int16_t) \
+  MACRO (16, 8, 2, 8, int16_t) \
+  MACRO (32, 4, 1, 32, int32_t) \
+  MACRO (32, 8, 2, 16, int32_t) \
+  MACRO (64, 4, 1, 64, int64_t) \
+  MACRO (64, 8, 2, 32, int64_t) \
+  MACRO (8, 8, 1, 8, int8_t) \
+  MACRO (16, 8, 1, 16, int16_t) \
+  MACRO (32, 8, 1, 32, int32_t) \
+  MACRO (64, 8, 1, 64, int64_t) \
+
+/* An iterator to call a macro with every supported SEW, LMUL and MLEN value,
+   along with its corresponding vector, float types, and info for
+   corresponding widening vector LMUL.  */
+#define _RVV_FLOAT_WLMUL_ITERATOR(MACRO) \
+  MACRO (16, 2, 1, 16, __float16_t) \
+  MACRO (16, 4, 2, 8, __float16_t) \
+  MACRO (16, 8, 4, 4, __float16_t) \
+  MACRO (32, 2, 1, 32, __float32_t) \
+  MACRO (32, 4, 2, 16, __float32_t) \
+  MACRO (32, 8, 4, 8, __float32_t) \
+  MACRO (64, 2, 1, 64, __float64_t) \
+  MACRO (64, 4, 2, 32, __float64_t) \
+  MACRO (64, 8, 4, 16, __float64_t) \
+  MACRO (16, 4, 1, 16, __float16_t) \
+  MACRO (16, 8, 2, 8, __float16_t) \
+  MACRO (32, 4, 1, 32, __float32_t) \
+  MACRO (32, 8, 2, 16, __float32_t) \
+  MACRO (64, 4, 1, 64, __float64_t) \
+  MACRO (64, 8, 2, 32, __float64_t) \
+  MACRO (16, 8, 1, 16, __float16_t) \
+  MACRO (32, 8, 1, 32, __float32_t) \
+  MACRO (64, 8, 1, 64, __float64_t) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, int8_t) \
+  MACRO (8, 1, 3, 8, int8_t) \
+  MACRO (8, 1, 4, 8, int8_t) \
+  MACRO (8, 1, 5, 8, int8_t) \
+  MACRO (8, 1, 6, 8, int8_t) \
+  MACRO (8, 1, 7, 8, int8_t) \
+  MACRO (8, 1, 8, 8, int8_t) \
+  MACRO (8, 2, 2, 4, int8_t) \
+  MACRO (8, 2, 3, 4, int8_t) \
+  MACRO (8, 2, 4, 4, int8_t) \
+  MACRO (8, 4, 2, 2, int8_t) \
+  MACRO (16, 1, 2, 16, int16_t) \
+  MACRO (16, 1, 3, 16, int16_t) \
+  MACRO (16, 1, 4, 16, int16_t) \
+  MACRO (16, 1, 5, 16, int16_t) \
+  MACRO (16, 1, 6, 16, int16_t) \
+  MACRO (16, 1, 7, 16, int16_t) \
+  MACRO (16, 1, 8, 16, int16_t) \
+  MACRO (16, 2, 2, 8, int16_t) \
+  MACRO (16, 2, 3, 8, int16_t) \
+  MACRO (16, 2, 4, 8, int16_t) \
+  MACRO (16, 4, 2, 4, int16_t) \
+  MACRO (32, 1, 2, 32, int32_t) \
+  MACRO (32, 1, 3, 32, int32_t) \
+  MACRO (32, 1, 4, 32, int32_t) \
+  MACRO (32, 1, 5, 32, int32_t) \
+  MACRO (32, 1, 6, 32, int32_t) \
+  MACRO (32, 1, 7, 32, int32_t) \
+  MACRO (32, 1, 8, 32, int32_t) \
+  MACRO (32, 2, 2, 16, int32_t) \
+  MACRO (32, 2, 3, 16, int32_t) \
+  MACRO (32, 2, 4, 16, int32_t) \
+  MACRO (32, 4, 2, 8, int32_t) \
+  MACRO (64, 1, 2, 64, int64_t) \
+  MACRO (64, 1, 3, 64, int64_t) \
+  MACRO (64, 1, 4, 64, int64_t) \
+  MACRO (64, 1, 5, 64, int64_t) \
+  MACRO (64, 1, 6, 64, int64_t) \
+  MACRO (64, 1, 7, 64, int64_t) \
+  MACRO (64, 1, 8, 64, int64_t) \
+  MACRO (64, 2, 2, 32, int64_t) \
+  MACRO (64, 2, 3, 32, int64_t) \
+  MACRO (64, 2, 4, 32, int64_t) \
+  MACRO (64, 4, 2, 16, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 2, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 2, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 2, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 3, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 3, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 3, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 3, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 4, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 4, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 4, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 4, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 5, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 5, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 5, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 5, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 6, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 6, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 6, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 6, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 7, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 7, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 7, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 7, 8, int8_t, 64, 8) \
+  MACRO (8, 1, 8, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 8, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 8, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 8, 8, int8_t, 64, 8) \
+  MACRO (8, 2, 2, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 2, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 2, 4, int8_t, 32, 8) \
+  MACRO (8, 2, 3, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 3, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 3, 4, int8_t, 32, 8) \
+  MACRO (8, 2, 4, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 4, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 4, 4, int8_t, 32, 8) \
+  MACRO (8, 4, 2, 2, int8_t, 8, 4) \
+  MACRO (8, 4, 2, 2, int8_t, 16, 8) \
+  MACRO (16, 1, 2, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 2, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 2, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 3, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 3, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 3, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 4, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 4, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 4, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 5, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 5, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 5, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 6, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 6, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 6, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 7, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 7, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 7, 16, int16_t, 64, 4) \
+  MACRO (16, 1, 8, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 8, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 8, 16, int16_t, 64, 4) \
+  MACRO (16, 2, 2, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 2, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 2, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 2, 8, int16_t, 64, 8) \
+  MACRO (16, 2, 3, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 3, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 3, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 3, 8, int16_t, 64, 8) \
+  MACRO (16, 2, 4, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 4, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 4, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 4, 8, int16_t, 64, 8) \
+  MACRO (16, 4, 2, 4, int16_t, 8, 2) \
+  MACRO (16, 4, 2, 4, int16_t, 16, 4) \
+  MACRO (16, 4, 2, 4, int16_t, 32, 8) \
+  MACRO (32, 1, 2, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 2, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 3, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 3, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 4, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 4, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 5, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 5, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 6, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 6, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 7, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 7, 32, int32_t, 64, 2) \
+  MACRO (32, 1, 8, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 8, 32, int32_t, 64, 2) \
+  MACRO (32, 2, 2, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 2, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 2, 16, int32_t, 64, 4) \
+  MACRO (32, 2, 3, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 3, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 3, 16, int32_t, 64, 4) \
+  MACRO (32, 2, 4, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 4, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 4, 16, int32_t, 64, 4) \
+  MACRO (32, 4, 2, 8, int32_t, 8, 1) \
+  MACRO (32, 4, 2, 8, int32_t, 16, 2) \
+  MACRO (32, 4, 2, 8, int32_t, 32, 4) \
+  MACRO (32, 4, 2, 8, int32_t, 64, 8) \
+  MACRO (64, 1, 2, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 3, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 4, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 5, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 6, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 7, 64, int64_t, 64, 1) \
+  MACRO (64, 1, 8, 64, int64_t, 64, 1) \
+  MACRO (64, 2, 2, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 2, 32, int64_t, 64, 2) \
+  MACRO (64, 2, 3, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 3, 32, int64_t, 64, 2) \
+  MACRO (64, 2, 4, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 4, 32, int64_t, 64, 2) \
+  MACRO (64, 4, 2, 16, int64_t, 16, 1) \
+  MACRO (64, 4, 2, 16, int64_t, 32, 2) \
+  MACRO (64, 4, 2, 16, int64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF2_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, int8_t) \
+  MACRO (8, 2, 2, 4, int8_t) \
+  MACRO (8, 4, 2, 2, int8_t) \
+  MACRO (16, 1, 2, 16, int16_t) \
+  MACRO (16, 2, 2, 8, int16_t) \
+  MACRO (16, 4, 2, 4, int16_t) \
+  MACRO (32, 1, 2, 32, int32_t) \
+  MACRO (32, 2, 2, 16, int32_t) \
+  MACRO (32, 4, 2, 8, int32_t) \
+  MACRO (64, 1, 2, 64, int64_t) \
+  MACRO (64, 2, 2, 32, int64_t) \
+  MACRO (64, 4, 2, 16, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF2_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF3_ITERATOR(MACRO) \
+  MACRO (8, 1, 3, 8, int8_t) \
+  MACRO (8, 2, 3, 4, int8_t) \
+  MACRO (16, 1, 3, 16, int16_t) \
+  MACRO (16, 2, 3, 8, int16_t) \
+  MACRO (32, 1, 3, 32, int32_t) \
+  MACRO (32, 2, 3, 16, int32_t) \
+  MACRO (64, 1, 3, 64, int64_t) \
+  MACRO (64, 2, 3, 32, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF3_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 3, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF4_ITERATOR(MACRO) \
+  MACRO (8, 1, 4, 8, int8_t) \
+  MACRO (8, 2, 4, 4, int8_t) \
+  MACRO (16, 1, 4, 16, int16_t) \
+  MACRO (16, 2, 4, 8, int16_t) \
+  MACRO (32, 1, 4, 32, int32_t) \
+  MACRO (32, 2, 4, 16, int32_t) \
+  MACRO (64, 1, 4, 64, int64_t) \
+  MACRO (64, 2, 4, 32, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF4_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 4, 8, int8_t, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, int64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF5_ITERATOR(MACRO) \
+  MACRO (8, 1, 5, 8, int8_t) \
+  MACRO (16, 1, 5, 16, int16_t) \
+  MACRO (32, 1, 5, 32, int32_t) \
+  MACRO (64, 1, 5, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF5_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 5, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF6_ITERATOR(MACRO) \
+  MACRO (8, 1, 6, 8, int8_t) \
+  MACRO (16, 1, 6, 16, int16_t) \
+  MACRO (32, 1, 6, 32, int32_t) \
+  MACRO (64, 1, 6, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF6_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 6, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF7_ITERATOR(MACRO) \
+  MACRO (8, 1, 7, 8, int8_t) \
+  MACRO (16, 1, 7, 16, int16_t) \
+  MACRO (32, 1, 7, 32, int32_t) \
+  MACRO (64, 1, 7, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF7_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 7, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF8_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, 8, int8_t) \
+  MACRO (16, 1, 8, 16, int16_t) \
+  MACRO (32, 1, 8, 32, int32_t) \
+  MACRO (64, 1, 8, 64, int64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF8_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, 8, int8_t, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, int64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF2_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 2, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 2, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 2, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 2, 8, int8_t, 64, 8) \
+  MACRO (8, 2, 2, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 2, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 2, 4, int8_t, 32, 8) \
+  MACRO (8, 4, 2, 2, int8_t, 8, 4) \
+  MACRO (8, 4, 2, 2, int8_t, 16, 8) \
+  MACRO (16, 1, 2, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 2, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 2, 16, int16_t, 64, 4) \
+  MACRO (16, 2, 2, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 2, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 2, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 2, 8, int16_t, 64, 8) \
+  MACRO (16, 4, 2, 4, int16_t, 8, 2) \
+  MACRO (16, 4, 2, 4, int16_t, 16, 4) \
+  MACRO (16, 4, 2, 4, int16_t, 32, 8) \
+  MACRO (32, 1, 2, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 2, 32, int32_t, 64, 2) \
+  MACRO (32, 2, 2, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 2, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 2, 16, int32_t, 64, 4) \
+  MACRO (32, 4, 2, 8, int32_t, 8, 1) \
+  MACRO (32, 4, 2, 8, int32_t, 16, 2) \
+  MACRO (32, 4, 2, 8, int32_t, 32, 4) \
+  MACRO (32, 4, 2, 8, int32_t, 64, 8) \
+  MACRO (64, 1, 2, 64, int64_t, 64, 1) \
+  MACRO (64, 2, 2, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 2, 32, int64_t, 64, 2) \
+  MACRO (64, 4, 2, 16, int64_t, 16, 1) \
+  MACRO (64, 4, 2, 16, int64_t, 32, 2) \
+  MACRO (64, 4, 2, 16, int64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF2_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 2, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 2, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 2, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, 8, 4, __VA_ARGS__) \
+  MACRO (8, 4, 2, 2, int8_t, 16, 8, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, int16_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, int32_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, int64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, int64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF3_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 3, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 3, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 3, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 3, 8, int8_t, 64, 8) \
+  MACRO (8, 2, 3, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 3, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 3, 4, int8_t, 32, 8) \
+  MACRO (16, 1, 3, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 3, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 3, 16, int16_t, 64, 4) \
+  MACRO (16, 2, 3, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 3, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 3, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 3, 8, int16_t, 64, 8) \
+  MACRO (32, 1, 3, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 3, 32, int32_t, 64, 2) \
+  MACRO (32, 2, 3, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 3, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 3, 16, int32_t, 64, 4) \
+  MACRO (64, 1, 3, 64, int64_t, 64, 1) \
+  MACRO (64, 2, 3, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 3, 32, int64_t, 64, 2) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF3_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 3, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 3, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 3, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, int64_t, 64, 2, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF4_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 4, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 4, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 4, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 4, 8, int8_t, 64, 8) \
+  MACRO (8, 2, 4, 4, int8_t, 8, 2) \
+  MACRO (8, 2, 4, 4, int8_t, 16, 4) \
+  MACRO (8, 2, 4, 4, int8_t, 32, 8) \
+  MACRO (16, 1, 4, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 4, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 4, 16, int16_t, 64, 4) \
+  MACRO (16, 2, 4, 8, int16_t, 8, 1) \
+  MACRO (16, 2, 4, 8, int16_t, 16, 2) \
+  MACRO (16, 2, 4, 8, int16_t, 32, 4) \
+  MACRO (16, 2, 4, 8, int16_t, 64, 8) \
+  MACRO (32, 1, 4, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 4, 32, int32_t, 64, 2) \
+  MACRO (32, 2, 4, 16, int32_t, 16, 1) \
+  MACRO (32, 2, 4, 16, int32_t, 32, 2) \
+  MACRO (32, 2, 4, 16, int32_t, 64, 4) \
+  MACRO (64, 1, 4, 64, int64_t, 64, 1) \
+  MACRO (64, 2, 4, 32, int64_t, 32, 1) \
+  MACRO (64, 2, 4, 32, int64_t, 64, 2) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF4_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 4, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 4, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 8, 2, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 16, 4, __VA_ARGS__) \
+  MACRO (8, 2, 4, 4, int8_t, 32, 8, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, int16_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, int32_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, int64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, int64_t, 64, 2, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF5_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 5, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 5, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 5, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 5, 8, int8_t, 64, 8) \
+  MACRO (16, 1, 5, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 5, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 5, 16, int16_t, 64, 4) \
+  MACRO (32, 1, 5, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 5, 32, int32_t, 64, 2) \
+  MACRO (64, 1, 5, 64, int64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF5_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 5, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 5, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, int64_t, 64, 1, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF6_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 6, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 6, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 6, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 6, 8, int8_t, 64, 8) \
+  MACRO (16, 1, 6, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 6, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 6, 16, int16_t, 64, 4) \
+  MACRO (32, 1, 6, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 6, 32, int32_t, 64, 2) \
+  MACRO (64, 1, 6, 64, int64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF6_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 6, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 6, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, int64_t, 64, 1, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF7_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 7, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 7, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 7, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 7, 8, int8_t, 64, 8) \
+  MACRO (16, 1, 7, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 7, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 7, 16, int16_t, 64, 4) \
+  MACRO (32, 1, 7, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 7, 32, int32_t, 64, 2) \
+  MACRO (64, 1, 7, 64, int64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF7_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 7, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 7, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, int64_t, 64, 1, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_INT_TUPLE_NF8_INDEX_ITERATOR(MACRO) \
+  MACRO (8, 1, 8, 8, int8_t, 8, 1) \
+  MACRO (8, 1, 8, 8, int8_t, 16, 2) \
+  MACRO (8, 1, 8, 8, int8_t, 32, 4) \
+  MACRO (8, 1, 8, 8, int8_t, 64, 8) \
+  MACRO (16, 1, 8, 16, int16_t, 16, 1) \
+  MACRO (16, 1, 8, 16, int16_t, 32, 2) \
+  MACRO (16, 1, 8, 16, int16_t, 64, 4) \
+  MACRO (32, 1, 8, 32, int32_t, 32, 1) \
+  MACRO (32, 1, 8, 32, int32_t, 64, 2) \
+  MACRO (64, 1, 8, 64, int64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_INT_TUPLE_NF8_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (8, 1, 8, 8, int8_t, 8, 1, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 16, 2, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 32, 4, __VA_ARGS__) \
+  MACRO (8, 1, 8, 8, int8_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, int16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, int32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, int64_t, 64, 1, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, __float16_t) \
+  MACRO (16, 1, 3, 16, __float16_t) \
+  MACRO (16, 1, 4, 16, __float16_t) \
+  MACRO (16, 1, 5, 16, __float16_t) \
+  MACRO (16, 1, 6, 16, __float16_t) \
+  MACRO (16, 1, 7, 16, __float16_t) \
+  MACRO (16, 1, 8, 16, __float16_t) \
+  MACRO (16, 2, 2, 8, __float16_t) \
+  MACRO (16, 2, 3, 8, __float16_t) \
+  MACRO (16, 2, 4, 8, __float16_t) \
+  MACRO (16, 4, 2, 4, __float16_t) \
+  MACRO (32, 1, 2, 32, __float32_t) \
+  MACRO (32, 1, 3, 32, __float32_t) \
+  MACRO (32, 1, 4, 32, __float32_t) \
+  MACRO (32, 1, 5, 32, __float32_t) \
+  MACRO (32, 1, 6, 32, __float32_t) \
+  MACRO (32, 1, 7, 32, __float32_t) \
+  MACRO (32, 1, 8, 32, __float32_t) \
+  MACRO (32, 2, 2, 16, __float32_t) \
+  MACRO (32, 2, 3, 16, __float32_t) \
+  MACRO (32, 2, 4, 16, __float32_t) \
+  MACRO (32, 4, 2, 8, __float32_t) \
+  MACRO (64, 1, 2, 64, __float64_t) \
+  MACRO (64, 1, 3, 64, __float64_t) \
+  MACRO (64, 1, 4, 64, __float64_t) \
+  MACRO (64, 1, 5, 64, __float64_t) \
+  MACRO (64, 1, 6, 64, __float64_t) \
+  MACRO (64, 1, 7, 64, __float64_t) \
+  MACRO (64, 1, 8, 64, __float64_t) \
+  MACRO (64, 2, 2, 32, __float64_t) \
+  MACRO (64, 2, 3, 32, __float64_t) \
+  MACRO (64, 2, 4, 32, __float64_t) \
+  MACRO (64, 4, 2, 16, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 2, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 2, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 3, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 3, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 3, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 4, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 4, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 4, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 5, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 5, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 5, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 6, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 6, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 6, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 7, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 7, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 7, 16, __float16_t, 64, 4) \
+  MACRO (16, 1, 8, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 8, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 8, 16, __float16_t, 64, 4) \
+  MACRO (16, 2, 2, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 2, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 2, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 2, 8, __float16_t, 64, 8) \
+  MACRO (16, 2, 3, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 3, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 3, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 3, 8, __float16_t, 64, 8) \
+  MACRO (16, 2, 4, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 4, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 4, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 4, 8, __float16_t, 64, 8) \
+  MACRO (16, 4, 2, 4, __float16_t, 8, 2) \
+  MACRO (16, 4, 2, 4, __float16_t, 16, 4) \
+  MACRO (16, 4, 2, 4, __float16_t, 32, 8) \
+  MACRO (32, 1, 2, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 2, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 3, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 3, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 4, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 4, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 5, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 5, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 6, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 6, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 7, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 7, 32, __float32_t, 64, 2) \
+  MACRO (32, 1, 8, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 8, 32, __float32_t, 64, 2) \
+  MACRO (32, 2, 2, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 2, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 2, 16, __float32_t, 64, 4) \
+  MACRO (32, 2, 3, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 3, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 3, 16, __float32_t, 64, 4) \
+  MACRO (32, 2, 4, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 4, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 4, 16, __float32_t, 64, 4) \
+  MACRO (32, 4, 2, 8, __float32_t, 8, 1) \
+  MACRO (32, 4, 2, 8, __float32_t, 16, 2) \
+  MACRO (32, 4, 2, 8, __float32_t, 32, 4) \
+  MACRO (32, 4, 2, 8, __float32_t, 64, 8) \
+  MACRO (64, 1, 2, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 3, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 4, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 5, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 6, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 7, 64, __float64_t, 64, 1) \
+  MACRO (64, 1, 8, 64, __float64_t, 64, 1) \
+  MACRO (64, 2, 2, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 2, 32, __float64_t, 64, 2) \
+  MACRO (64, 2, 3, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 3, 32, __float64_t, 64, 2) \
+  MACRO (64, 2, 4, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 4, 32, __float64_t, 64, 2) \
+  MACRO (64, 4, 2, 16, __float64_t, 16, 1) \
+  MACRO (64, 4, 2, 16, __float64_t, 32, 2) \
+  MACRO (64, 4, 2, 16, __float64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF2_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, __float16_t) \
+  MACRO (16, 2, 2, 8, __float16_t) \
+  MACRO (16, 4, 2, 4, __float16_t) \
+  MACRO (32, 1, 2, 32, __float32_t) \
+  MACRO (32, 2, 2, 16, __float32_t) \
+  MACRO (32, 4, 2, 8, __float32_t) \
+  MACRO (64, 1, 2, 64, __float64_t) \
+  MACRO (64, 2, 2, 32, __float64_t) \
+  MACRO (64, 4, 2, 16, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF2_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF3_ITERATOR(MACRO) \
+  MACRO (16, 1, 3, 16, __float16_t) \
+  MACRO (16, 2, 3, 8, __float16_t) \
+  MACRO (32, 1, 3, 32, __float32_t) \
+  MACRO (32, 2, 3, 16, __float32_t) \
+  MACRO (64, 1, 3, 64, __float64_t) \
+  MACRO (64, 2, 3, 32, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF3_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 3, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF4_ITERATOR(MACRO) \
+  MACRO (16, 1, 4, 16, __float16_t) \
+  MACRO (16, 2, 4, 8, __float16_t) \
+  MACRO (32, 1, 4, 32, __float32_t) \
+  MACRO (32, 2, 4, 16, __float32_t) \
+  MACRO (64, 1, 4, 64, __float64_t) \
+  MACRO (64, 2, 4, 32, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF4_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 4, 16, __float16_t, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, __float64_t, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF5_ITERATOR(MACRO) \
+  MACRO (16, 1, 5, 16, __float16_t) \
+  MACRO (32, 1, 5, 32, __float32_t) \
+  MACRO (64, 1, 5, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF5_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 5, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF6_ITERATOR(MACRO) \
+  MACRO (16, 1, 6, 16, __float16_t) \
+  MACRO (32, 1, 6, 32, __float32_t) \
+  MACRO (64, 1, 6, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF6_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 6, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF7_ITERATOR(MACRO) \
+  MACRO (16, 1, 7, 16, __float16_t) \
+  MACRO (32, 1, 7, 32, __float32_t) \
+  MACRO (64, 1, 7, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF7_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 7, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF8_ITERATOR(MACRO) \
+  MACRO (16, 1, 8, 16, __float16_t) \
+  MACRO (32, 1, 8, 32, __float32_t) \
+  MACRO (64, 1, 8, 64, __float64_t) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF8_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 8, 16, __float16_t, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, __float64_t, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF2_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 2, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 2, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 2, 16, __float16_t, 64, 4) \
+  MACRO (16, 2, 2, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 2, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 2, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 2, 8, __float16_t, 64, 8) \
+  MACRO (16, 4, 2, 4, __float16_t, 8, 2) \
+  MACRO (16, 4, 2, 4, __float16_t, 16, 4) \
+  MACRO (16, 4, 2, 4, __float16_t, 32, 8) \
+  MACRO (32, 1, 2, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 2, 32, __float32_t, 64, 2) \
+  MACRO (32, 2, 2, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 2, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 2, 16, __float32_t, 64, 4) \
+  MACRO (32, 4, 2, 8, __float32_t, 8, 1) \
+  MACRO (32, 4, 2, 8, __float32_t, 16, 2) \
+  MACRO (32, 4, 2, 8, __float32_t, 32, 4) \
+  MACRO (32, 4, 2, 8, __float32_t, 64, 8) \
+  MACRO (64, 1, 2, 64, __float64_t, 64, 1) \
+  MACRO (64, 2, 2, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 2, 32, __float64_t, 64, 2) \
+  MACRO (64, 4, 2, 16, __float64_t, 16, 1) \
+  MACRO (64, 4, 2, 16, __float64_t, 32, 2) \
+  MACRO (64, 4, 2, 16, __float64_t, 64, 4) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF2_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 2, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 2, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 2, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 8, 2, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 16, 4, __VA_ARGS__) \
+  MACRO (16, 4, 2, 4, __float16_t, 32, 8, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 2, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 2, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 8, 1, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 16, 2, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 32, 4, __VA_ARGS__) \
+  MACRO (32, 4, 2, 8, __float32_t, 64, 8, __VA_ARGS__) \
+  MACRO (64, 1, 2, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 2, 32, __float64_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 16, 1, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 32, 2, __VA_ARGS__) \
+  MACRO (64, 4, 2, 16, __float64_t, 64, 4, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF3_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 3, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 3, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 3, 16, __float16_t, 64, 4) \
+  MACRO (16, 2, 3, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 3, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 3, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 3, 8, __float16_t, 64, 8) \
+  MACRO (32, 1, 3, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 3, 32, __float32_t, 64, 2) \
+  MACRO (32, 2, 3, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 3, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 3, 16, __float32_t, 64, 4) \
+  MACRO (64, 1, 3, 64, __float64_t, 64, 1) \
+  MACRO (64, 2, 3, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 3, 32, __float64_t, 64, 2) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF3_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 3, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 3, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 3, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 3, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 3, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 3, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 3, 32, __float64_t, 64, 2, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF4_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 4, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 4, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 4, 16, __float16_t, 64, 4) \
+  MACRO (16, 2, 4, 8, __float16_t, 8, 1) \
+  MACRO (16, 2, 4, 8, __float16_t, 16, 2) \
+  MACRO (16, 2, 4, 8, __float16_t, 32, 4) \
+  MACRO (16, 2, 4, 8, __float16_t, 64, 8) \
+  MACRO (32, 1, 4, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 4, 32, __float32_t, 64, 2) \
+  MACRO (32, 2, 4, 16, __float32_t, 16, 1) \
+  MACRO (32, 2, 4, 16, __float32_t, 32, 2) \
+  MACRO (32, 2, 4, 16, __float32_t, 64, 4) \
+  MACRO (64, 1, 4, 64, __float64_t, 64, 1) \
+  MACRO (64, 2, 4, 32, __float64_t, 32, 1) \
+  MACRO (64, 2, 4, 32, __float64_t, 64, 2) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF4_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 4, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 4, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 8, 1, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 16, 2, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 32, 4, __VA_ARGS__) \
+  MACRO (16, 2, 4, 8, __float16_t, 64, 8, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 4, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 16, 1, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 32, 2, __VA_ARGS__) \
+  MACRO (32, 2, 4, 16, __float32_t, 64, 4, __VA_ARGS__) \
+  MACRO (64, 1, 4, 64, __float64_t, 64, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, 32, 1, __VA_ARGS__) \
+  MACRO (64, 2, 4, 32, __float64_t, 64, 2, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF5_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 5, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 5, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 5, 16, __float16_t, 64, 4) \
+  MACRO (32, 1, 5, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 5, 32, __float32_t, 64, 2) \
+  MACRO (64, 1, 5, 64, __float64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF5_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 5, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 5, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 5, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 5, 64, __float64_t, 64, 1, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF6_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 6, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 6, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 6, 16, __float16_t, 64, 4) \
+  MACRO (32, 1, 6, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 6, 32, __float32_t, 64, 2) \
+  MACRO (64, 1, 6, 64, __float64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF6_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 6, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 6, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 6, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 6, 64, __float64_t, 64, 1, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF7_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 7, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 7, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 7, 16, __float16_t, 64, 4) \
+  MACRO (32, 1, 7, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 7, 32, __float32_t, 64, 2) \
+  MACRO (64, 1, 7, 64, __float64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF7_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 7, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 7, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 7, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 7, 64, __float64_t, 64, 1, __VA_ARGS__) \
+
+/* An iterator to call a macro with every supported NF, SEW, LMUL and MLEN value,
+   along with its corresponding vector, floating point modes, and info for
+   corresponding floating point and vector tuple type.  */
+#define _RVV_FLOAT_TUPLE_NF8_INDEX_ITERATOR(MACRO) \
+  MACRO (16, 1, 8, 16, __float16_t, 16, 1) \
+  MACRO (16, 1, 8, 16, __float16_t, 32, 2) \
+  MACRO (16, 1, 8, 16, __float16_t, 64, 4) \
+  MACRO (32, 1, 8, 32, __float32_t, 32, 1) \
+  MACRO (32, 1, 8, 32, __float32_t, 64, 2) \
+  MACRO (64, 1, 8, 64, __float64_t, 64, 1) \
+
+/* Same as above but with an extra argument.  */
+#define _RVV_FLOAT_TUPLE_NF8_INDEX_ITERATOR_ARG(MACRO, ...) \
+  MACRO (16, 1, 8, 16, __float16_t, 16, 1, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 32, 2, __VA_ARGS__) \
+  MACRO (16, 1, 8, 16, __float16_t, 64, 4, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, 32, 1, __VA_ARGS__) \
+  MACRO (32, 1, 8, 32, __float32_t, 64, 2, __VA_ARGS__) \
+  MACRO (64, 1, 8, 64, __float64_t, 64, 1, __VA_ARGS__) \
+
+//#if __riscv_v == 7000
+/* Wrapper only.  */
+#define vle_v_i8m1	vle8_v_i8m1
+#define vle_v_u8m1	vle8_v_u8m1
+#define vle_v_i8m1_m	vle8_v_i8m1_m
+#define vle_v_u8m1_m	vle8_v_u8m1_m
+#define vle_v_i8m2	vle8_v_i8m2
+#define vle_v_u8m2	vle8_v_u8m2
+#define vle_v_i8m2_m	vle8_v_i8m2_m
+#define vle_v_u8m2_m	vle8_v_u8m2_m
+#define vle_v_i8m4	vle8_v_i8m4
+#define vle_v_u8m4	vle8_v_u8m4
+#define vle_v_i8m4_m	vle8_v_i8m4_m
+#define vle_v_u8m4_m	vle8_v_u8m4_m
+#define vle_v_i8m8	vle8_v_i8m8
+#define vle_v_u8m8	vle8_v_u8m8
+#define vle_v_i8m8_m	vle8_v_i8m8_m
+#define vle_v_u8m8_m	vle8_v_u8m8_m
+#define vle_v_i16m1	vle16_v_i16m1
+#define vle_v_u16m1	vle16_v_u16m1
+#define vle_v_i16m1_m	vle16_v_i16m1_m
+#define vle_v_u16m1_m	vle16_v_u16m1_m
+#define vle_v_i16m2	vle16_v_i16m2
+#define vle_v_u16m2	vle16_v_u16m2
+#define vle_v_i16m2_m	vle16_v_i16m2_m
+#define vle_v_u16m2_m	vle16_v_u16m2_m
+#define vle_v_i16m4	vle16_v_i16m4
+#define vle_v_u16m4	vle16_v_u16m4
+#define vle_v_i16m4_m	vle16_v_i16m4_m
+#define vle_v_u16m4_m	vle16_v_u16m4_m
+#define vle_v_i16m8	vle16_v_i16m8
+#define vle_v_u16m8	vle16_v_u16m8
+#define vle_v_i16m8_m	vle16_v_i16m8_m
+#define vle_v_u16m8_m	vle16_v_u16m8_m
+#define vle_v_i32m1	vle32_v_i32m1
+#define vle_v_u32m1	vle32_v_u32m1
+#define vle_v_i32m1_m	vle32_v_i32m1_m
+#define vle_v_u32m1_m	vle32_v_u32m1_m
+#define vle_v_i32m2	vle32_v_i32m2
+#define vle_v_u32m2	vle32_v_u32m2
+#define vle_v_i32m2_m	vle32_v_i32m2_m
+#define vle_v_u32m2_m	vle32_v_u32m2_m
+#define vle_v_i32m4	vle32_v_i32m4
+#define vle_v_u32m4	vle32_v_u32m4
+#define vle_v_i32m4_m	vle32_v_i32m4_m
+#define vle_v_u32m4_m	vle32_v_u32m4_m
+#define vle_v_i32m8	vle32_v_i32m8
+#define vle_v_u32m8	vle32_v_u32m8
+#define vle_v_i32m8_m	vle32_v_i32m8_m
+#define vle_v_u32m8_m	vle32_v_u32m8_m
+#define vle_v_i64m1	vle64_v_i64m1
+#define vle_v_u64m1	vle64_v_u64m1
+#define vle_v_i64m1_m	vle64_v_i64m1_m
+#define vle_v_u64m1_m	vle64_v_u64m1_m
+#define vle_v_i64m2	vle64_v_i64m2
+#define vle_v_u64m2	vle64_v_u64m2
+#define vle_v_i64m2_m	vle64_v_i64m2_m
+#define vle_v_u64m2_m	vle64_v_u64m2_m
+#define vle_v_i64m4	vle64_v_i64m4
+#define vle_v_u64m4	vle64_v_u64m4
+#define vle_v_i64m4_m	vle64_v_i64m4_m
+#define vle_v_u64m4_m	vle64_v_u64m4_m
+#define vle_v_i64m8	vle64_v_i64m8
+#define vle_v_u64m8	vle64_v_u64m8
+#define vle_v_i64m8_m	vle64_v_i64m8_m
+#define vle_v_u64m8_m	vle64_v_u64m8_m
+#define vle_v_f16m1	vle16_v_f16m1
+#define vle_v_f16m1_m	vle16_v_f16m1_m
+#define vle_v_f16m2	vle16_v_f16m2
+#define vle_v_f16m2_m	vle16_v_f16m2_m
+#define vle_v_f16m4	vle16_v_f16m4
+#define vle_v_f16m4_m	vle16_v_f16m4_m
+#define vle_v_f16m8	vle16_v_f16m8
+#define vle_v_f16m8_m	vle16_v_f16m8_m
+#define vle_v_f32m1	vle32_v_f32m1
+#define vle_v_f32m1_m	vle32_v_f32m1_m
+#define vle_v_f32m2	vle32_v_f32m2
+#define vle_v_f32m2_m	vle32_v_f32m2_m
+#define vle_v_f32m4	vle32_v_f32m4
+#define vle_v_f32m4_m	vle32_v_f32m4_m
+#define vle_v_f32m8	vle32_v_f32m8
+#define vle_v_f32m8_m	vle32_v_f32m8_m
+#define vle_v_f64m1	vle64_v_f64m1
+#define vle_v_f64m1_m	vle64_v_f64m1_m
+#define vle_v_f64m2	vle64_v_f64m2
+#define vle_v_f64m2_m	vle64_v_f64m2_m
+#define vle_v_f64m4	vle64_v_f64m4
+#define vle_v_f64m4_m	vle64_v_f64m4_m
+#define vle_v_f64m8	vle64_v_f64m8
+#define vle_v_f64m8_m	vle64_v_f64m8_m
+
+/* Wrapper only.  */
+#define vse_v_i8m1	vse8_v_i8m1
+#define vse_v_u8m1	vse8_v_u8m1
+#define vse_v_i8m1_m	vse8_v_i8m1_m
+#define vse_v_u8m1_m	vse8_v_u8m1_m
+#define vse_v_i8m2	vse8_v_i8m2
+#define vse_v_u8m2	vse8_v_u8m2
+#define vse_v_i8m2_m	vse8_v_i8m2_m
+#define vse_v_u8m2_m	vse8_v_u8m2_m
+#define vse_v_i8m4	vse8_v_i8m4
+#define vse_v_u8m4	vse8_v_u8m4
+#define vse_v_i8m4_m	vse8_v_i8m4_m
+#define vse_v_u8m4_m	vse8_v_u8m4_m
+#define vse_v_i8m8	vse8_v_i8m8
+#define vse_v_u8m8	vse8_v_u8m8
+#define vse_v_i8m8_m	vse8_v_i8m8_m
+#define vse_v_u8m8_m	vse8_v_u8m8_m
+#define vse_v_i16m1	vse16_v_i16m1
+#define vse_v_u16m1	vse16_v_u16m1
+#define vse_v_i16m1_m	vse16_v_i16m1_m
+#define vse_v_u16m1_m	vse16_v_u16m1_m
+#define vse_v_i16m2	vse16_v_i16m2
+#define vse_v_u16m2	vse16_v_u16m2
+#define vse_v_i16m2_m	vse16_v_i16m2_m
+#define vse_v_u16m2_m	vse16_v_u16m2_m
+#define vse_v_i16m4	vse16_v_i16m4
+#define vse_v_u16m4	vse16_v_u16m4
+#define vse_v_i16m4_m	vse16_v_i16m4_m
+#define vse_v_u16m4_m	vse16_v_u16m4_m
+#define vse_v_i16m8	vse16_v_i16m8
+#define vse_v_u16m8	vse16_v_u16m8
+#define vse_v_i16m8_m	vse16_v_i16m8_m
+#define vse_v_u16m8_m	vse16_v_u16m8_m
+#define vse_v_i32m1	vse32_v_i32m1
+#define vse_v_u32m1	vse32_v_u32m1
+#define vse_v_i32m1_m	vse32_v_i32m1_m
+#define vse_v_u32m1_m	vse32_v_u32m1_m
+#define vse_v_i32m2	vse32_v_i32m2
+#define vse_v_u32m2	vse32_v_u32m2
+#define vse_v_i32m2_m	vse32_v_i32m2_m
+#define vse_v_u32m2_m	vse32_v_u32m2_m
+#define vse_v_i32m4	vse32_v_i32m4
+#define vse_v_u32m4	vse32_v_u32m4
+#define vse_v_i32m4_m	vse32_v_i32m4_m
+#define vse_v_u32m4_m	vse32_v_u32m4_m
+#define vse_v_i32m8	vse32_v_i32m8
+#define vse_v_u32m8	vse32_v_u32m8
+#define vse_v_i32m8_m	vse32_v_i32m8_m
+#define vse_v_u32m8_m	vse32_v_u32m8_m
+#define vse_v_i64m1	vse64_v_i64m1
+#define vse_v_u64m1	vse64_v_u64m1
+#define vse_v_i64m1_m	vse64_v_i64m1_m
+#define vse_v_u64m1_m	vse64_v_u64m1_m
+#define vse_v_i64m2	vse64_v_i64m2
+#define vse_v_u64m2	vse64_v_u64m2
+#define vse_v_i64m2_m	vse64_v_i64m2_m
+#define vse_v_u64m2_m	vse64_v_u64m2_m
+#define vse_v_i64m4	vse64_v_i64m4
+#define vse_v_u64m4	vse64_v_u64m4
+#define vse_v_i64m4_m	vse64_v_i64m4_m
+#define vse_v_u64m4_m	vse64_v_u64m4_m
+#define vse_v_i64m8	vse64_v_i64m8
+#define vse_v_u64m8	vse64_v_u64m8
+#define vse_v_i64m8_m	vse64_v_i64m8_m
+#define vse_v_u64m8_m	vse64_v_u64m8_m
+#define vse_v_f16m1	vse16_v_f16m1
+#define vse_v_f16m1_m	vse16_v_f16m1_m
+#define vse_v_f16m2	vse16_v_f16m2
+#define vse_v_f16m2_m	vse16_v_f16m2_m
+#define vse_v_f16m4	vse16_v_f16m4
+#define vse_v_f16m4_m	vse16_v_f16m4_m
+#define vse_v_f16m8	vse16_v_f16m8
+#define vse_v_f16m8_m	vse16_v_f16m8_m
+#define vse_v_f32m1	vse32_v_f32m1
+#define vse_v_f32m1_m	vse32_v_f32m1_m
+#define vse_v_f32m2	vse32_v_f32m2
+#define vse_v_f32m2_m	vse32_v_f32m2_m
+#define vse_v_f32m4	vse32_v_f32m4
+#define vse_v_f32m4_m	vse32_v_f32m4_m
+#define vse_v_f32m8	vse32_v_f32m8
+#define vse_v_f32m8_m	vse32_v_f32m8_m
+#define vse_v_f64m1	vse64_v_f64m1
+#define vse_v_f64m1_m	vse64_v_f64m1_m
+#define vse_v_f64m2	vse64_v_f64m2
+#define vse_v_f64m2_m	vse64_v_f64m2_m
+#define vse_v_f64m4	vse64_v_f64m4
+#define vse_v_f64m4_m	vse64_v_f64m4_m
+#define vse_v_f64m8	vse64_v_f64m8
+#define vse_v_f64m8_m	vse64_v_f64m8_m
+
+/* Wrapper only.  */
+#define vlse_v_i8m1	vlse8_v_i8m1
+#define vlse_v_u8m1	vlse8_v_u8m1
+#define vlse_v_i8m1_m	vlse8_v_i8m1_m
+#define vlse_v_u8m1_m	vlse8_v_u8m1_m
+#define vlse_v_i8m2	vlse8_v_i8m2
+#define vlse_v_u8m2	vlse8_v_u8m2
+#define vlse_v_i8m2_m	vlse8_v_i8m2_m
+#define vlse_v_u8m2_m	vlse8_v_u8m2_m
+#define vlse_v_i8m4	vlse8_v_i8m4
+#define vlse_v_u8m4	vlse8_v_u8m4
+#define vlse_v_i8m4_m	vlse8_v_i8m4_m
+#define vlse_v_u8m4_m	vlse8_v_u8m4_m
+#define vlse_v_i8m8	vlse8_v_i8m8
+#define vlse_v_u8m8	vlse8_v_u8m8
+#define vlse_v_i8m8_m	vlse8_v_i8m8_m
+#define vlse_v_u8m8_m	vlse8_v_u8m8_m
+#define vlse_v_i16m1	vlse16_v_i16m1
+#define vlse_v_u16m1	vlse16_v_u16m1
+#define vlse_v_i16m1_m	vlse16_v_i16m1_m
+#define vlse_v_u16m1_m	vlse16_v_u16m1_m
+#define vlse_v_i16m2	vlse16_v_i16m2
+#define vlse_v_u16m2	vlse16_v_u16m2
+#define vlse_v_i16m2_m	vlse16_v_i16m2_m
+#define vlse_v_u16m2_m	vlse16_v_u16m2_m
+#define vlse_v_i16m4	vlse16_v_i16m4
+#define vlse_v_u16m4	vlse16_v_u16m4
+#define vlse_v_i16m4_m	vlse16_v_i16m4_m
+#define vlse_v_u16m4_m	vlse16_v_u16m4_m
+#define vlse_v_i16m8	vlse16_v_i16m8
+#define vlse_v_u16m8	vlse16_v_u16m8
+#define vlse_v_i16m8_m	vlse16_v_i16m8_m
+#define vlse_v_u16m8_m	vlse16_v_u16m8_m
+#define vlse_v_i32m1	vlse32_v_i32m1
+#define vlse_v_u32m1	vlse32_v_u32m1
+#define vlse_v_i32m1_m	vlse32_v_i32m1_m
+#define vlse_v_u32m1_m	vlse32_v_u32m1_m
+#define vlse_v_i32m2	vlse32_v_i32m2
+#define vlse_v_u32m2	vlse32_v_u32m2
+#define vlse_v_i32m2_m	vlse32_v_i32m2_m
+#define vlse_v_u32m2_m	vlse32_v_u32m2_m
+#define vlse_v_i32m4	vlse32_v_i32m4
+#define vlse_v_u32m4	vlse32_v_u32m4
+#define vlse_v_i32m4_m	vlse32_v_i32m4_m
+#define vlse_v_u32m4_m	vlse32_v_u32m4_m
+#define vlse_v_i32m8	vlse32_v_i32m8
+#define vlse_v_u32m8	vlse32_v_u32m8
+#define vlse_v_i32m8_m	vlse32_v_i32m8_m
+#define vlse_v_u32m8_m	vlse32_v_u32m8_m
+#define vlse_v_i64m1	vlse64_v_i64m1
+#define vlse_v_u64m1	vlse64_v_u64m1
+#define vlse_v_i64m1_m	vlse64_v_i64m1_m
+#define vlse_v_u64m1_m	vlse64_v_u64m1_m
+#define vlse_v_i64m2	vlse64_v_i64m2
+#define vlse_v_u64m2	vlse64_v_u64m2
+#define vlse_v_i64m2_m	vlse64_v_i64m2_m
+#define vlse_v_u64m2_m	vlse64_v_u64m2_m
+#define vlse_v_i64m4	vlse64_v_i64m4
+#define vlse_v_u64m4	vlse64_v_u64m4
+#define vlse_v_i64m4_m	vlse64_v_i64m4_m
+#define vlse_v_u64m4_m	vlse64_v_u64m4_m
+#define vlse_v_i64m8	vlse64_v_i64m8
+#define vlse_v_u64m8	vlse64_v_u64m8
+#define vlse_v_i64m8_m	vlse64_v_i64m8_m
+#define vlse_v_u64m8_m	vlse64_v_u64m8_m
+#define vlse_v_f16m1	vlse16_v_f16m1
+#define vlse_v_f16m1_m	vlse16_v_f16m1_m
+#define vlse_v_f16m2	vlse16_v_f16m2
+#define vlse_v_f16m2_m	vlse16_v_f16m2_m
+#define vlse_v_f16m4	vlse16_v_f16m4
+#define vlse_v_f16m4_m	vlse16_v_f16m4_m
+#define vlse_v_f16m8	vlse16_v_f16m8
+#define vlse_v_f16m8_m	vlse16_v_f16m8_m
+#define vlse_v_f32m1	vlse32_v_f32m1
+#define vlse_v_f32m1_m	vlse32_v_f32m1_m
+#define vlse_v_f32m2	vlse32_v_f32m2
+#define vlse_v_f32m2_m	vlse32_v_f32m2_m
+#define vlse_v_f32m4	vlse32_v_f32m4
+#define vlse_v_f32m4_m	vlse32_v_f32m4_m
+#define vlse_v_f32m8	vlse32_v_f32m8
+#define vlse_v_f32m8_m	vlse32_v_f32m8_m
+#define vlse_v_f64m1	vlse64_v_f64m1
+#define vlse_v_f64m1_m	vlse64_v_f64m1_m
+#define vlse_v_f64m2	vlse64_v_f64m2
+#define vlse_v_f64m2_m	vlse64_v_f64m2_m
+#define vlse_v_f64m4	vlse64_v_f64m4
+#define vlse_v_f64m4_m	vlse64_v_f64m4_m
+#define vlse_v_f64m8	vlse64_v_f64m8
+#define vlse_v_f64m8_m	vlse64_v_f64m8_m
+
+/* Wrapper only.  */
+#define vsse_v_i8m1	vsse8_v_i8m1
+#define vsse_v_u8m1	vsse8_v_u8m1
+#define vsse_v_i8m1_m	vsse8_v_i8m1_m
+#define vsse_v_u8m1_m	vsse8_v_u8m1_m
+#define vsse_v_i8m2	vsse8_v_i8m2
+#define vsse_v_u8m2	vsse8_v_u8m2
+#define vsse_v_i8m2_m	vsse8_v_i8m2_m
+#define vsse_v_u8m2_m	vsse8_v_u8m2_m
+#define vsse_v_i8m4	vsse8_v_i8m4
+#define vsse_v_u8m4	vsse8_v_u8m4
+#define vsse_v_i8m4_m	vsse8_v_i8m4_m
+#define vsse_v_u8m4_m	vsse8_v_u8m4_m
+#define vsse_v_i8m8	vsse8_v_i8m8
+#define vsse_v_u8m8	vsse8_v_u8m8
+#define vsse_v_i8m8_m	vsse8_v_i8m8_m
+#define vsse_v_u8m8_m	vsse8_v_u8m8_m
+#define vsse_v_i16m1	vsse16_v_i16m1
+#define vsse_v_u16m1	vsse16_v_u16m1
+#define vsse_v_i16m1_m	vsse16_v_i16m1_m
+#define vsse_v_u16m1_m	vsse16_v_u16m1_m
+#define vsse_v_i16m2	vsse16_v_i16m2
+#define vsse_v_u16m2	vsse16_v_u16m2
+#define vsse_v_i16m2_m	vsse16_v_i16m2_m
+#define vsse_v_u16m2_m	vsse16_v_u16m2_m
+#define vsse_v_i16m4	vsse16_v_i16m4
+#define vsse_v_u16m4	vsse16_v_u16m4
+#define vsse_v_i16m4_m	vsse16_v_i16m4_m
+#define vsse_v_u16m4_m	vsse16_v_u16m4_m
+#define vsse_v_i16m8	vsse16_v_i16m8
+#define vsse_v_u16m8	vsse16_v_u16m8
+#define vsse_v_i16m8_m	vsse16_v_i16m8_m
+#define vsse_v_u16m8_m	vsse16_v_u16m8_m
+#define vsse_v_i32m1	vsse32_v_i32m1
+#define vsse_v_u32m1	vsse32_v_u32m1
+#define vsse_v_i32m1_m	vsse32_v_i32m1_m
+#define vsse_v_u32m1_m	vsse32_v_u32m1_m
+#define vsse_v_i32m2	vsse32_v_i32m2
+#define vsse_v_u32m2	vsse32_v_u32m2
+#define vsse_v_i32m2_m	vsse32_v_i32m2_m
+#define vsse_v_u32m2_m	vsse32_v_u32m2_m
+#define vsse_v_i32m4	vsse32_v_i32m4
+#define vsse_v_u32m4	vsse32_v_u32m4
+#define vsse_v_i32m4_m	vsse32_v_i32m4_m
+#define vsse_v_u32m4_m	vsse32_v_u32m4_m
+#define vsse_v_i32m8	vsse32_v_i32m8
+#define vsse_v_u32m8	vsse32_v_u32m8
+#define vsse_v_i32m8_m	vsse32_v_i32m8_m
+#define vsse_v_u32m8_m	vsse32_v_u32m8_m
+#define vsse_v_i64m1	vsse64_v_i64m1
+#define vsse_v_u64m1	vsse64_v_u64m1
+#define vsse_v_i64m1_m	vsse64_v_i64m1_m
+#define vsse_v_u64m1_m	vsse64_v_u64m1_m
+#define vsse_v_i64m2	vsse64_v_i64m2
+#define vsse_v_u64m2	vsse64_v_u64m2
+#define vsse_v_i64m2_m	vsse64_v_i64m2_m
+#define vsse_v_u64m2_m	vsse64_v_u64m2_m
+#define vsse_v_i64m4	vsse64_v_i64m4
+#define vsse_v_u64m4	vsse64_v_u64m4
+#define vsse_v_i64m4_m	vsse64_v_i64m4_m
+#define vsse_v_u64m4_m	vsse64_v_u64m4_m
+#define vsse_v_i64m8	vsse64_v_i64m8
+#define vsse_v_u64m8	vsse64_v_u64m8
+#define vsse_v_i64m8_m	vsse64_v_i64m8_m
+#define vsse_v_u64m8_m	vsse64_v_u64m8_m
+#define vsse_v_f16m1	vsse16_v_f16m1
+#define vsse_v_f16m1_m	vsse16_v_f16m1_m
+#define vsse_v_f16m2	vsse16_v_f16m2
+#define vsse_v_f16m2_m	vsse16_v_f16m2_m
+#define vsse_v_f16m4	vsse16_v_f16m4
+#define vsse_v_f16m4_m	vsse16_v_f16m4_m
+#define vsse_v_f16m8	vsse16_v_f16m8
+#define vsse_v_f16m8_m	vsse16_v_f16m8_m
+#define vsse_v_f32m1	vsse32_v_f32m1
+#define vsse_v_f32m1_m	vsse32_v_f32m1_m
+#define vsse_v_f32m2	vsse32_v_f32m2
+#define vsse_v_f32m2_m	vsse32_v_f32m2_m
+#define vsse_v_f32m4	vsse32_v_f32m4
+#define vsse_v_f32m4_m	vsse32_v_f32m4_m
+#define vsse_v_f32m8	vsse32_v_f32m8
+#define vsse_v_f32m8_m	vsse32_v_f32m8_m
+#define vsse_v_f64m1	vsse64_v_f64m1
+#define vsse_v_f64m1_m	vsse64_v_f64m1_m
+#define vsse_v_f64m2	vsse64_v_f64m2
+#define vsse_v_f64m2_m	vsse64_v_f64m2_m
+#define vsse_v_f64m4	vsse64_v_f64m4
+#define vsse_v_f64m4_m	vsse64_v_f64m4_m
+#define vsse_v_f64m8	vsse64_v_f64m8
+#define vsse_v_f64m8_m	vsse64_v_f64m8_m
+
+/* Wrapper only.  */
+#define vlxe_v_i8m1	vloxei8_v_i8m1
+#define vlxe_v_u8m1	vloxei8_v_u8m1
+#define vlxe_v_i8m1_m	vloxei8_v_i8m1_m
+#define vlxe_v_u8m1_m	vloxei8_v_u8m1_m
+#define vlxe_v_i8m2	vloxei8_v_i8m2
+#define vlxe_v_u8m2	vloxei8_v_u8m2
+#define vlxe_v_i8m2_m	vloxei8_v_i8m2_m
+#define vlxe_v_u8m2_m	vloxei8_v_u8m2_m
+#define vlxe_v_i8m4	vloxei8_v_i8m4
+#define vlxe_v_u8m4	vloxei8_v_u8m4
+#define vlxe_v_i8m4_m	vloxei8_v_i8m4_m
+#define vlxe_v_u8m4_m	vloxei8_v_u8m4_m
+#define vlxe_v_i8m8	vloxei8_v_i8m8
+#define vlxe_v_u8m8	vloxei8_v_u8m8
+#define vlxe_v_i8m8_m	vloxei8_v_i8m8_m
+#define vlxe_v_u8m8_m	vloxei8_v_u8m8_m
+#define vlxe_v_i16m1	vloxei16_v_i16m1
+#define vlxe_v_u16m1	vloxei16_v_u16m1
+#define vlxe_v_i16m1_m	vloxei16_v_i16m1_m
+#define vlxe_v_u16m1_m	vloxei16_v_u16m1_m
+#define vlxe_v_i16m2	vloxei16_v_i16m2
+#define vlxe_v_u16m2	vloxei16_v_u16m2
+#define vlxe_v_i16m2_m	vloxei16_v_i16m2_m
+#define vlxe_v_u16m2_m	vloxei16_v_u16m2_m
+#define vlxe_v_i16m4	vloxei16_v_i16m4
+#define vlxe_v_u16m4	vloxei16_v_u16m4
+#define vlxe_v_i16m4_m	vloxei16_v_i16m4_m
+#define vlxe_v_u16m4_m	vloxei16_v_u16m4_m
+#define vlxe_v_i16m8	vloxei16_v_i16m8
+#define vlxe_v_u16m8	vloxei16_v_u16m8
+#define vlxe_v_i16m8_m	vloxei16_v_i16m8_m
+#define vlxe_v_u16m8_m	vloxei16_v_u16m8_m
+#define vlxe_v_i32m1	vloxei32_v_i32m1
+#define vlxe_v_u32m1	vloxei32_v_u32m1
+#define vlxe_v_i32m1_m	vloxei32_v_i32m1_m
+#define vlxe_v_u32m1_m	vloxei32_v_u32m1_m
+#define vlxe_v_i32m2	vloxei32_v_i32m2
+#define vlxe_v_u32m2	vloxei32_v_u32m2
+#define vlxe_v_i32m2_m	vloxei32_v_i32m2_m
+#define vlxe_v_u32m2_m	vloxei32_v_u32m2_m
+#define vlxe_v_i32m4	vloxei32_v_i32m4
+#define vlxe_v_u32m4	vloxei32_v_u32m4
+#define vlxe_v_i32m4_m	vloxei32_v_i32m4_m
+#define vlxe_v_u32m4_m	vloxei32_v_u32m4_m
+#define vlxe_v_i32m8	vloxei32_v_i32m8
+#define vlxe_v_u32m8	vloxei32_v_u32m8
+#define vlxe_v_i32m8_m	vloxei32_v_i32m8_m
+#define vlxe_v_u32m8_m	vloxei32_v_u32m8_m
+#define vlxe_v_i64m1	vloxei64_v_i64m1
+#define vlxe_v_u64m1	vloxei64_v_u64m1
+#define vlxe_v_i64m1_m	vloxei64_v_i64m1_m
+#define vlxe_v_u64m1_m	vloxei64_v_u64m1_m
+#define vlxe_v_i64m2	vloxei64_v_i64m2
+#define vlxe_v_u64m2	vloxei64_v_u64m2
+#define vlxe_v_i64m2_m	vloxei64_v_i64m2_m
+#define vlxe_v_u64m2_m	vloxei64_v_u64m2_m
+#define vlxe_v_i64m4	vloxei64_v_i64m4
+#define vlxe_v_u64m4	vloxei64_v_u64m4
+#define vlxe_v_i64m4_m	vloxei64_v_i64m4_m
+#define vlxe_v_u64m4_m	vloxei64_v_u64m4_m
+#define vlxe_v_i64m8	vloxei64_v_i64m8
+#define vlxe_v_u64m8	vloxei64_v_u64m8
+#define vlxe_v_i64m8_m	vloxei64_v_i64m8_m
+#define vlxe_v_u64m8_m	vloxei64_v_u64m8_m
+#define vlxe_v_f16m1	vloxei16_v_f16m1
+#define vlxe_v_f16m1_m	vloxei16_v_f16m1_m
+#define vlxe_v_f16m2	vloxei16_v_f16m2
+#define vlxe_v_f16m2_m	vloxei16_v_f16m2_m
+#define vlxe_v_f16m4	vloxei16_v_f16m4
+#define vlxe_v_f16m4_m	vloxei16_v_f16m4_m
+#define vlxe_v_f16m8	vloxei16_v_f16m8
+#define vlxe_v_f16m8_m	vloxei16_v_f16m8_m
+#define vlxe_v_f32m1	vloxei32_v_f32m1
+#define vlxe_v_f32m1_m	vloxei32_v_f32m1_m
+#define vlxe_v_f32m2	vloxei32_v_f32m2
+#define vlxe_v_f32m2_m	vloxei32_v_f32m2_m
+#define vlxe_v_f32m4	vloxei32_v_f32m4
+#define vlxe_v_f32m4_m	vloxei32_v_f32m4_m
+#define vlxe_v_f32m8	vloxei32_v_f32m8
+#define vlxe_v_f32m8_m	vloxei32_v_f32m8_m
+#define vlxe_v_f64m1	vloxei64_v_f64m1
+#define vlxe_v_f64m1_m	vloxei64_v_f64m1_m
+#define vlxe_v_f64m2	vloxei64_v_f64m2
+#define vlxe_v_f64m2_m	vloxei64_v_f64m2_m
+#define vlxe_v_f64m4	vloxei64_v_f64m4
+#define vlxe_v_f64m4_m	vloxei64_v_f64m4_m
+#define vlxe_v_f64m8	vloxei64_v_f64m8
+#define vlxe_v_f64m8_m	vloxei64_v_f64m8_m
+
+/* Wrapper only.  */
+#define vsxe_v_i8m1	vsoxei8_v_i8m1
+#define vsxe_v_u8m1	vsoxei8_v_u8m1
+#define vsxe_v_i8m1_m	vsoxei8_v_i8m1_m
+#define vsxe_v_u8m1_m	vsoxei8_v_u8m1_m
+#define vsxe_v_i8m2	vsoxei8_v_i8m2
+#define vsxe_v_u8m2	vsoxei8_v_u8m2
+#define vsxe_v_i8m2_m	vsoxei8_v_i8m2_m
+#define vsxe_v_u8m2_m	vsoxei8_v_u8m2_m
+#define vsxe_v_i8m4	vsoxei8_v_i8m4
+#define vsxe_v_u8m4	vsoxei8_v_u8m4
+#define vsxe_v_i8m4_m	vsoxei8_v_i8m4_m
+#define vsxe_v_u8m4_m	vsoxei8_v_u8m4_m
+#define vsxe_v_i8m8	vsoxei8_v_i8m8
+#define vsxe_v_u8m8	vsoxei8_v_u8m8
+#define vsxe_v_i8m8_m	vsoxei8_v_i8m8_m
+#define vsxe_v_u8m8_m	vsoxei8_v_u8m8_m
+#define vsxe_v_i16m1	vsoxei16_v_i16m1
+#define vsxe_v_u16m1	vsoxei16_v_u16m1
+#define vsxe_v_i16m1_m	vsoxei16_v_i16m1_m
+#define vsxe_v_u16m1_m	vsoxei16_v_u16m1_m
+#define vsxe_v_i16m2	vsoxei16_v_i16m2
+#define vsxe_v_u16m2	vsoxei16_v_u16m2
+#define vsxe_v_i16m2_m	vsoxei16_v_i16m2_m
+#define vsxe_v_u16m2_m	vsoxei16_v_u16m2_m
+#define vsxe_v_i16m4	vsoxei16_v_i16m4
+#define vsxe_v_u16m4	vsoxei16_v_u16m4
+#define vsxe_v_i16m4_m	vsoxei16_v_i16m4_m
+#define vsxe_v_u16m4_m	vsoxei16_v_u16m4_m
+#define vsxe_v_i16m8	vsoxei16_v_i16m8
+#define vsxe_v_u16m8	vsoxei16_v_u16m8
+#define vsxe_v_i16m8_m	vsoxei16_v_i16m8_m
+#define vsxe_v_u16m8_m	vsoxei16_v_u16m8_m
+#define vsxe_v_i32m1	vsoxei32_v_i32m1
+#define vsxe_v_u32m1	vsoxei32_v_u32m1
+#define vsxe_v_i32m1_m	vsoxei32_v_i32m1_m
+#define vsxe_v_u32m1_m	vsoxei32_v_u32m1_m
+#define vsxe_v_i32m2	vsoxei32_v_i32m2
+#define vsxe_v_u32m2	vsoxei32_v_u32m2
+#define vsxe_v_i32m2_m	vsoxei32_v_i32m2_m
+#define vsxe_v_u32m2_m	vsoxei32_v_u32m2_m
+#define vsxe_v_i32m4	vsoxei32_v_i32m4
+#define vsxe_v_u32m4	vsoxei32_v_u32m4
+#define vsxe_v_i32m4_m	vsoxei32_v_i32m4_m
+#define vsxe_v_u32m4_m	vsoxei32_v_u32m4_m
+#define vsxe_v_i32m8	vsoxei32_v_i32m8
+#define vsxe_v_u32m8	vsoxei32_v_u32m8
+#define vsxe_v_i32m8_m	vsoxei32_v_i32m8_m
+#define vsxe_v_u32m8_m	vsoxei32_v_u32m8_m
+#define vsxe_v_i64m1	vsoxei64_v_i64m1
+#define vsxe_v_u64m1	vsoxei64_v_u64m1
+#define vsxe_v_i64m1_m	vsoxei64_v_i64m1_m
+#define vsxe_v_u64m1_m	vsoxei64_v_u64m1_m
+#define vsxe_v_i64m2	vsoxei64_v_i64m2
+#define vsxe_v_u64m2	vsoxei64_v_u64m2
+#define vsxe_v_i64m2_m	vsoxei64_v_i64m2_m
+#define vsxe_v_u64m2_m	vsoxei64_v_u64m2_m
+#define vsxe_v_i64m4	vsoxei64_v_i64m4
+#define vsxe_v_u64m4	vsoxei64_v_u64m4
+#define vsxe_v_i64m4_m	vsoxei64_v_i64m4_m
+#define vsxe_v_u64m4_m	vsoxei64_v_u64m4_m
+#define vsxe_v_i64m8	vsoxei64_v_i64m8
+#define vsxe_v_u64m8	vsoxei64_v_u64m8
+#define vsxe_v_i64m8_m	vsoxei64_v_i64m8_m
+#define vsxe_v_u64m8_m	vsoxei64_v_u64m8_m
+#define vsxe_v_f16m1	vsoxei16_v_f16m1
+#define vsxe_v_f16m1_m	vsoxei16_v_f16m1_m
+#define vsxe_v_f16m2	vsoxei16_v_f16m2
+#define vsxe_v_f16m2_m	vsoxei16_v_f16m2_m
+#define vsxe_v_f16m4	vsoxei16_v_f16m4
+#define vsxe_v_f16m4_m	vsoxei16_v_f16m4_m
+#define vsxe_v_f16m8	vsoxei16_v_f16m8
+#define vsxe_v_f16m8_m	vsoxei16_v_f16m8_m
+#define vsxe_v_f32m1	vsoxei32_v_f32m1
+#define vsxe_v_f32m1_m	vsoxei32_v_f32m1_m
+#define vsxe_v_f32m2	vsoxei32_v_f32m2
+#define vsxe_v_f32m2_m	vsoxei32_v_f32m2_m
+#define vsxe_v_f32m4	vsoxei32_v_f32m4
+#define vsxe_v_f32m4_m	vsoxei32_v_f32m4_m
+#define vsxe_v_f32m8	vsoxei32_v_f32m8
+#define vsxe_v_f32m8_m	vsoxei32_v_f32m8_m
+#define vsxe_v_f64m1	vsoxei64_v_f64m1
+#define vsxe_v_f64m1_m	vsoxei64_v_f64m1_m
+#define vsxe_v_f64m2	vsoxei64_v_f64m2
+#define vsxe_v_f64m2_m	vsoxei64_v_f64m2_m
+#define vsxe_v_f64m4	vsoxei64_v_f64m4
+#define vsxe_v_f64m4_m	vsoxei64_v_f64m4_m
+#define vsxe_v_f64m8	vsoxei64_v_f64m8
+#define vsxe_v_f64m8_m	vsoxei64_v_f64m8_m
+
+/* Wrapper only.  */
+#define vsuxe_v_i8m1	vsuxei8_v_i8m1
+#define vsuxe_v_u8m1	vsuxei8_v_u8m1
+#define vsuxe_v_i8m1_m	vsuxei8_v_i8m1_m
+#define vsuxe_v_u8m1_m	vsuxei8_v_u8m1_m
+#define vsuxe_v_i8m2	vsuxei8_v_i8m2
+#define vsuxe_v_u8m2	vsuxei8_v_u8m2
+#define vsuxe_v_i8m2_m	vsuxei8_v_i8m2_m
+#define vsuxe_v_u8m2_m	vsuxei8_v_u8m2_m
+#define vsuxe_v_i8m4	vsuxei8_v_i8m4
+#define vsuxe_v_u8m4	vsuxei8_v_u8m4
+#define vsuxe_v_i8m4_m	vsuxei8_v_i8m4_m
+#define vsuxe_v_u8m4_m	vsuxei8_v_u8m4_m
+#define vsuxe_v_i8m8	vsuxei8_v_i8m8
+#define vsuxe_v_u8m8	vsuxei8_v_u8m8
+#define vsuxe_v_i8m8_m	vsuxei8_v_i8m8_m
+#define vsuxe_v_u8m8_m	vsuxei8_v_u8m8_m
+#define vsuxe_v_i16m1	vsuxei16_v_i16m1
+#define vsuxe_v_u16m1	vsuxei16_v_u16m1
+#define vsuxe_v_i16m1_m	vsuxei16_v_i16m1_m
+#define vsuxe_v_u16m1_m	vsuxei16_v_u16m1_m
+#define vsuxe_v_i16m2	vsuxei16_v_i16m2
+#define vsuxe_v_u16m2	vsuxei16_v_u16m2
+#define vsuxe_v_i16m2_m	vsuxei16_v_i16m2_m
+#define vsuxe_v_u16m2_m	vsuxei16_v_u16m2_m
+#define vsuxe_v_i16m4	vsuxei16_v_i16m4
+#define vsuxe_v_u16m4	vsuxei16_v_u16m4
+#define vsuxe_v_i16m4_m	vsuxei16_v_i16m4_m
+#define vsuxe_v_u16m4_m	vsuxei16_v_u16m4_m
+#define vsuxe_v_i16m8	vsuxei16_v_i16m8
+#define vsuxe_v_u16m8	vsuxei16_v_u16m8
+#define vsuxe_v_i16m8_m	vsuxei16_v_i16m8_m
+#define vsuxe_v_u16m8_m	vsuxei16_v_u16m8_m
+#define vsuxe_v_i32m1	vsuxei32_v_i32m1
+#define vsuxe_v_u32m1	vsuxei32_v_u32m1
+#define vsuxe_v_i32m1_m	vsuxei32_v_i32m1_m
+#define vsuxe_v_u32m1_m	vsuxei32_v_u32m1_m
+#define vsuxe_v_i32m2	vsuxei32_v_i32m2
+#define vsuxe_v_u32m2	vsuxei32_v_u32m2
+#define vsuxe_v_i32m2_m	vsuxei32_v_i32m2_m
+#define vsuxe_v_u32m2_m	vsuxei32_v_u32m2_m
+#define vsuxe_v_i32m4	vsuxei32_v_i32m4
+#define vsuxe_v_u32m4	vsuxei32_v_u32m4
+#define vsuxe_v_i32m4_m	vsuxei32_v_i32m4_m
+#define vsuxe_v_u32m4_m	vsuxei32_v_u32m4_m
+#define vsuxe_v_i32m8	vsuxei32_v_i32m8
+#define vsuxe_v_u32m8	vsuxei32_v_u32m8
+#define vsuxe_v_i32m8_m	vsuxei32_v_i32m8_m
+#define vsuxe_v_u32m8_m	vsuxei32_v_u32m8_m
+#define vsuxe_v_i64m1	vsuxei64_v_i64m1
+#define vsuxe_v_u64m1	vsuxei64_v_u64m1
+#define vsuxe_v_i64m1_m	vsuxei64_v_i64m1_m
+#define vsuxe_v_u64m1_m	vsuxei64_v_u64m1_m
+#define vsuxe_v_i64m2	vsuxei64_v_i64m2
+#define vsuxe_v_u64m2	vsuxei64_v_u64m2
+#define vsuxe_v_i64m2_m	vsuxei64_v_i64m2_m
+#define vsuxe_v_u64m2_m	vsuxei64_v_u64m2_m
+#define vsuxe_v_i64m4	vsuxei64_v_i64m4
+#define vsuxe_v_u64m4	vsuxei64_v_u64m4
+#define vsuxe_v_i64m4_m	vsuxei64_v_i64m4_m
+#define vsuxe_v_u64m4_m	vsuxei64_v_u64m4_m
+#define vsuxe_v_i64m8	vsuxei64_v_i64m8
+#define vsuxe_v_u64m8	vsuxei64_v_u64m8
+#define vsuxe_v_i64m8_m	vsuxei64_v_i64m8_m
+#define vsuxe_v_u64m8_m	vsuxei64_v_u64m8_m
+#define vsuxe_v_f16m1	vsuxei16_v_f16m1
+#define vsuxe_v_f16m1_m	vsuxei16_v_f16m1_m
+#define vsuxe_v_f16m2	vsuxei16_v_f16m2
+#define vsuxe_v_f16m2_m	vsuxei16_v_f16m2_m
+#define vsuxe_v_f16m4	vsuxei16_v_f16m4
+#define vsuxe_v_f16m4_m	vsuxei16_v_f16m4_m
+#define vsuxe_v_f16m8	vsuxei16_v_f16m8
+#define vsuxe_v_f16m8_m	vsuxei16_v_f16m8_m
+#define vsuxe_v_f32m1	vsuxei32_v_f32m1
+#define vsuxe_v_f32m1_m	vsuxei32_v_f32m1_m
+#define vsuxe_v_f32m2	vsuxei32_v_f32m2
+#define vsuxe_v_f32m2_m	vsuxei32_v_f32m2_m
+#define vsuxe_v_f32m4	vsuxei32_v_f32m4
+#define vsuxe_v_f32m4_m	vsuxei32_v_f32m4_m
+#define vsuxe_v_f32m8	vsuxei32_v_f32m8
+#define vsuxe_v_f32m8_m	vsuxei32_v_f32m8_m
+#define vsuxe_v_f64m1	vsuxei64_v_f64m1
+#define vsuxe_v_f64m1_m	vsuxei64_v_f64m1_m
+#define vsuxe_v_f64m2	vsuxei64_v_f64m2
+#define vsuxe_v_f64m2_m	vsuxei64_v_f64m2_m
+#define vsuxe_v_f64m4	vsuxei64_v_f64m4
+#define vsuxe_v_f64m4_m	vsuxei64_v_f64m4_m
+#define vsuxe_v_f64m8	vsuxei64_v_f64m8
+#define vsuxe_v_f64m8_m	vsuxei64_v_f64m8_m
+
+/* Wrapper only.  */
+#define vleff_v_i8m1	vle8ff_v_i8m1
+#define vleff_v_u8m1	vle8ff_v_u8m1
+#define vleff_v_i8m1_m	vle8ff_v_i8m1_m
+#define vleff_v_u8m1_m	vle8ff_v_u8m1_m
+#define vleff_v_i8m2	vle8ff_v_i8m2
+#define vleff_v_u8m2	vle8ff_v_u8m2
+#define vleff_v_i8m2_m	vle8ff_v_i8m2_m
+#define vleff_v_u8m2_m	vle8ff_v_u8m2_m
+#define vleff_v_i8m4	vle8ff_v_i8m4
+#define vleff_v_u8m4	vle8ff_v_u8m4
+#define vleff_v_i8m4_m	vle8ff_v_i8m4_m
+#define vleff_v_u8m4_m	vle8ff_v_u8m4_m
+#define vleff_v_i8m8	vle8ff_v_i8m8
+#define vleff_v_u8m8	vle8ff_v_u8m8
+#define vleff_v_i8m8_m	vle8ff_v_i8m8_m
+#define vleff_v_u8m8_m	vle8ff_v_u8m8_m
+#define vleff_v_i16m1	vle16ff_v_i16m1
+#define vleff_v_u16m1	vle16ff_v_u16m1
+#define vleff_v_i16m1_m	vle16ff_v_i16m1_m
+#define vleff_v_u16m1_m	vle16ff_v_u16m1_m
+#define vleff_v_i16m2	vle16ff_v_i16m2
+#define vleff_v_u16m2	vle16ff_v_u16m2
+#define vleff_v_i16m2_m	vle16ff_v_i16m2_m
+#define vleff_v_u16m2_m	vle16ff_v_u16m2_m
+#define vleff_v_i16m4	vle16ff_v_i16m4
+#define vleff_v_u16m4	vle16ff_v_u16m4
+#define vleff_v_i16m4_m	vle16ff_v_i16m4_m
+#define vleff_v_u16m4_m	vle16ff_v_u16m4_m
+#define vleff_v_i16m8	vle16ff_v_i16m8
+#define vleff_v_u16m8	vle16ff_v_u16m8
+#define vleff_v_i16m8_m	vle16ff_v_i16m8_m
+#define vleff_v_u16m8_m	vle16ff_v_u16m8_m
+#define vleff_v_i32m1	vle32ff_v_i32m1
+#define vleff_v_u32m1	vle32ff_v_u32m1
+#define vleff_v_i32m1_m	vle32ff_v_i32m1_m
+#define vleff_v_u32m1_m	vle32ff_v_u32m1_m
+#define vleff_v_i32m2	vle32ff_v_i32m2
+#define vleff_v_u32m2	vle32ff_v_u32m2
+#define vleff_v_i32m2_m	vle32ff_v_i32m2_m
+#define vleff_v_u32m2_m	vle32ff_v_u32m2_m
+#define vleff_v_i32m4	vle32ff_v_i32m4
+#define vleff_v_u32m4	vle32ff_v_u32m4
+#define vleff_v_i32m4_m	vle32ff_v_i32m4_m
+#define vleff_v_u32m4_m	vle32ff_v_u32m4_m
+#define vleff_v_i32m8	vle32ff_v_i32m8
+#define vleff_v_u32m8	vle32ff_v_u32m8
+#define vleff_v_i32m8_m	vle32ff_v_i32m8_m
+#define vleff_v_u32m8_m	vle32ff_v_u32m8_m
+#define vleff_v_i64m1	vle64ff_v_i64m1
+#define vleff_v_u64m1	vle64ff_v_u64m1
+#define vleff_v_i64m1_m	vle64ff_v_i64m1_m
+#define vleff_v_u64m1_m	vle64ff_v_u64m1_m
+#define vleff_v_i64m2	vle64ff_v_i64m2
+#define vleff_v_u64m2	vle64ff_v_u64m2
+#define vleff_v_i64m2_m	vle64ff_v_i64m2_m
+#define vleff_v_u64m2_m	vle64ff_v_u64m2_m
+#define vleff_v_i64m4	vle64ff_v_i64m4
+#define vleff_v_u64m4	vle64ff_v_u64m4
+#define vleff_v_i64m4_m	vle64ff_v_i64m4_m
+#define vleff_v_u64m4_m	vle64ff_v_u64m4_m
+#define vleff_v_i64m8	vle64ff_v_i64m8
+#define vleff_v_u64m8	vle64ff_v_u64m8
+#define vleff_v_i64m8_m	vle64ff_v_i64m8_m
+#define vleff_v_u64m8_m	vle64ff_v_u64m8_m
+#define vleff_v_f16m1	vle16ff_v_f16m1
+#define vleff_v_f16m1_m	vle16ff_v_f16m1_m
+#define vleff_v_f16m2	vle16ff_v_f16m2
+#define vleff_v_f16m2_m	vle16ff_v_f16m2_m
+#define vleff_v_f16m4	vle16ff_v_f16m4
+#define vleff_v_f16m4_m	vle16ff_v_f16m4_m
+#define vleff_v_f16m8	vle16ff_v_f16m8
+#define vleff_v_f16m8_m	vle16ff_v_f16m8_m
+#define vleff_v_f32m1	vle32ff_v_f32m1
+#define vleff_v_f32m1_m	vle32ff_v_f32m1_m
+#define vleff_v_f32m2	vle32ff_v_f32m2
+#define vleff_v_f32m2_m	vle32ff_v_f32m2_m
+#define vleff_v_f32m4	vle32ff_v_f32m4
+#define vleff_v_f32m4_m	vle32ff_v_f32m4_m
+#define vleff_v_f32m8	vle32ff_v_f32m8
+#define vleff_v_f32m8_m	vle32ff_v_f32m8_m
+#define vleff_v_f64m1	vle64ff_v_f64m1
+#define vleff_v_f64m1_m	vle64ff_v_f64m1_m
+#define vleff_v_f64m2	vle64ff_v_f64m2
+#define vleff_v_f64m2_m	vle64ff_v_f64m2_m
+#define vleff_v_f64m4	vle64ff_v_f64m4
+#define vleff_v_f64m4_m	vle64ff_v_f64m4_m
+#define vleff_v_f64m8	vle64ff_v_f64m8
+#define vleff_v_f64m8_m	vle64ff_v_f64m8_m
+
+/* Wrapper only.  */
+#define vlseg2e_v_i8m1x2	vlseg2e8_v_i8m1x2
+#define vlseg2e_v_u8m1x2	vlseg2e8_v_u8m1x2
+#define vlseg2e_v_i8m1x2_m	vlseg2e8_v_i8m1x2_m
+#define vlseg2e_v_u8m1x2_m	vlseg2e8_v_u8m1x2_m
+#define vlseg3e_v_i8m1x3	vlseg3e8_v_i8m1x3
+#define vlseg3e_v_u8m1x3	vlseg3e8_v_u8m1x3
+#define vlseg3e_v_i8m1x3_m	vlseg3e8_v_i8m1x3_m
+#define vlseg3e_v_u8m1x3_m	vlseg3e8_v_u8m1x3_m
+#define vlseg4e_v_i8m1x4	vlseg4e8_v_i8m1x4
+#define vlseg4e_v_u8m1x4	vlseg4e8_v_u8m1x4
+#define vlseg4e_v_i8m1x4_m	vlseg4e8_v_i8m1x4_m
+#define vlseg4e_v_u8m1x4_m	vlseg4e8_v_u8m1x4_m
+#define vlseg5e_v_i8m1x5	vlseg5e8_v_i8m1x5
+#define vlseg5e_v_u8m1x5	vlseg5e8_v_u8m1x5
+#define vlseg5e_v_i8m1x5_m	vlseg5e8_v_i8m1x5_m
+#define vlseg5e_v_u8m1x5_m	vlseg5e8_v_u8m1x5_m
+#define vlseg6e_v_i8m1x6	vlseg6e8_v_i8m1x6
+#define vlseg6e_v_u8m1x6	vlseg6e8_v_u8m1x6
+#define vlseg6e_v_i8m1x6_m	vlseg6e8_v_i8m1x6_m
+#define vlseg6e_v_u8m1x6_m	vlseg6e8_v_u8m1x6_m
+#define vlseg7e_v_i8m1x7	vlseg7e8_v_i8m1x7
+#define vlseg7e_v_u8m1x7	vlseg7e8_v_u8m1x7
+#define vlseg7e_v_i8m1x7_m	vlseg7e8_v_i8m1x7_m
+#define vlseg7e_v_u8m1x7_m	vlseg7e8_v_u8m1x7_m
+#define vlseg8e_v_i8m1x8	vlseg8e8_v_i8m1x8
+#define vlseg8e_v_u8m1x8	vlseg8e8_v_u8m1x8
+#define vlseg8e_v_i8m1x8_m	vlseg8e8_v_i8m1x8_m
+#define vlseg8e_v_u8m1x8_m	vlseg8e8_v_u8m1x8_m
+#define vlseg2e_v_i8m2x2	vlseg2e8_v_i8m2x2
+#define vlseg2e_v_u8m2x2	vlseg2e8_v_u8m2x2
+#define vlseg2e_v_i8m2x2_m	vlseg2e8_v_i8m2x2_m
+#define vlseg2e_v_u8m2x2_m	vlseg2e8_v_u8m2x2_m
+#define vlseg3e_v_i8m2x3	vlseg3e8_v_i8m2x3
+#define vlseg3e_v_u8m2x3	vlseg3e8_v_u8m2x3
+#define vlseg3e_v_i8m2x3_m	vlseg3e8_v_i8m2x3_m
+#define vlseg3e_v_u8m2x3_m	vlseg3e8_v_u8m2x3_m
+#define vlseg4e_v_i8m2x4	vlseg4e8_v_i8m2x4
+#define vlseg4e_v_u8m2x4	vlseg4e8_v_u8m2x4
+#define vlseg4e_v_i8m2x4_m	vlseg4e8_v_i8m2x4_m
+#define vlseg4e_v_u8m2x4_m	vlseg4e8_v_u8m2x4_m
+#define vlseg2e_v_i8m4x2	vlseg2e8_v_i8m4x2
+#define vlseg2e_v_u8m4x2	vlseg2e8_v_u8m4x2
+#define vlseg2e_v_i8m4x2_m	vlseg2e8_v_i8m4x2_m
+#define vlseg2e_v_u8m4x2_m	vlseg2e8_v_u8m4x2_m
+#define vlseg2e_v_i16m1x2	vlseg2e16_v_i16m1x2
+#define vlseg2e_v_u16m1x2	vlseg2e16_v_u16m1x2
+#define vlseg2e_v_i16m1x2_m	vlseg2e16_v_i16m1x2_m
+#define vlseg2e_v_u16m1x2_m	vlseg2e16_v_u16m1x2_m
+#define vlseg3e_v_i16m1x3	vlseg3e16_v_i16m1x3
+#define vlseg3e_v_u16m1x3	vlseg3e16_v_u16m1x3
+#define vlseg3e_v_i16m1x3_m	vlseg3e16_v_i16m1x3_m
+#define vlseg3e_v_u16m1x3_m	vlseg3e16_v_u16m1x3_m
+#define vlseg4e_v_i16m1x4	vlseg4e16_v_i16m1x4
+#define vlseg4e_v_u16m1x4	vlseg4e16_v_u16m1x4
+#define vlseg4e_v_i16m1x4_m	vlseg4e16_v_i16m1x4_m
+#define vlseg4e_v_u16m1x4_m	vlseg4e16_v_u16m1x4_m
+#define vlseg5e_v_i16m1x5	vlseg5e16_v_i16m1x5
+#define vlseg5e_v_u16m1x5	vlseg5e16_v_u16m1x5
+#define vlseg5e_v_i16m1x5_m	vlseg5e16_v_i16m1x5_m
+#define vlseg5e_v_u16m1x5_m	vlseg5e16_v_u16m1x5_m
+#define vlseg6e_v_i16m1x6	vlseg6e16_v_i16m1x6
+#define vlseg6e_v_u16m1x6	vlseg6e16_v_u16m1x6
+#define vlseg6e_v_i16m1x6_m	vlseg6e16_v_i16m1x6_m
+#define vlseg6e_v_u16m1x6_m	vlseg6e16_v_u16m1x6_m
+#define vlseg7e_v_i16m1x7	vlseg7e16_v_i16m1x7
+#define vlseg7e_v_u16m1x7	vlseg7e16_v_u16m1x7
+#define vlseg7e_v_i16m1x7_m	vlseg7e16_v_i16m1x7_m
+#define vlseg7e_v_u16m1x7_m	vlseg7e16_v_u16m1x7_m
+#define vlseg8e_v_i16m1x8	vlseg8e16_v_i16m1x8
+#define vlseg8e_v_u16m1x8	vlseg8e16_v_u16m1x8
+#define vlseg8e_v_i16m1x8_m	vlseg8e16_v_i16m1x8_m
+#define vlseg8e_v_u16m1x8_m	vlseg8e16_v_u16m1x8_m
+#define vlseg2e_v_i16m2x2	vlseg2e16_v_i16m2x2
+#define vlseg2e_v_u16m2x2	vlseg2e16_v_u16m2x2
+#define vlseg2e_v_i16m2x2_m	vlseg2e16_v_i16m2x2_m
+#define vlseg2e_v_u16m2x2_m	vlseg2e16_v_u16m2x2_m
+#define vlseg3e_v_i16m2x3	vlseg3e16_v_i16m2x3
+#define vlseg3e_v_u16m2x3	vlseg3e16_v_u16m2x3
+#define vlseg3e_v_i16m2x3_m	vlseg3e16_v_i16m2x3_m
+#define vlseg3e_v_u16m2x3_m	vlseg3e16_v_u16m2x3_m
+#define vlseg4e_v_i16m2x4	vlseg4e16_v_i16m2x4
+#define vlseg4e_v_u16m2x4	vlseg4e16_v_u16m2x4
+#define vlseg4e_v_i16m2x4_m	vlseg4e16_v_i16m2x4_m
+#define vlseg4e_v_u16m2x4_m	vlseg4e16_v_u16m2x4_m
+#define vlseg2e_v_i16m4x2	vlseg2e16_v_i16m4x2
+#define vlseg2e_v_u16m4x2	vlseg2e16_v_u16m4x2
+#define vlseg2e_v_i16m4x2_m	vlseg2e16_v_i16m4x2_m
+#define vlseg2e_v_u16m4x2_m	vlseg2e16_v_u16m4x2_m
+#define vlseg2e_v_i32m1x2	vlseg2e32_v_i32m1x2
+#define vlseg2e_v_u32m1x2	vlseg2e32_v_u32m1x2
+#define vlseg2e_v_i32m1x2_m	vlseg2e32_v_i32m1x2_m
+#define vlseg2e_v_u32m1x2_m	vlseg2e32_v_u32m1x2_m
+#define vlseg3e_v_i32m1x3	vlseg3e32_v_i32m1x3
+#define vlseg3e_v_u32m1x3	vlseg3e32_v_u32m1x3
+#define vlseg3e_v_i32m1x3_m	vlseg3e32_v_i32m1x3_m
+#define vlseg3e_v_u32m1x3_m	vlseg3e32_v_u32m1x3_m
+#define vlseg4e_v_i32m1x4	vlseg4e32_v_i32m1x4
+#define vlseg4e_v_u32m1x4	vlseg4e32_v_u32m1x4
+#define vlseg4e_v_i32m1x4_m	vlseg4e32_v_i32m1x4_m
+#define vlseg4e_v_u32m1x4_m	vlseg4e32_v_u32m1x4_m
+#define vlseg5e_v_i32m1x5	vlseg5e32_v_i32m1x5
+#define vlseg5e_v_u32m1x5	vlseg5e32_v_u32m1x5
+#define vlseg5e_v_i32m1x5_m	vlseg5e32_v_i32m1x5_m
+#define vlseg5e_v_u32m1x5_m	vlseg5e32_v_u32m1x5_m
+#define vlseg6e_v_i32m1x6	vlseg6e32_v_i32m1x6
+#define vlseg6e_v_u32m1x6	vlseg6e32_v_u32m1x6
+#define vlseg6e_v_i32m1x6_m	vlseg6e32_v_i32m1x6_m
+#define vlseg6e_v_u32m1x6_m	vlseg6e32_v_u32m1x6_m
+#define vlseg7e_v_i32m1x7	vlseg7e32_v_i32m1x7
+#define vlseg7e_v_u32m1x7	vlseg7e32_v_u32m1x7
+#define vlseg7e_v_i32m1x7_m	vlseg7e32_v_i32m1x7_m
+#define vlseg7e_v_u32m1x7_m	vlseg7e32_v_u32m1x7_m
+#define vlseg8e_v_i32m1x8	vlseg8e32_v_i32m1x8
+#define vlseg8e_v_u32m1x8	vlseg8e32_v_u32m1x8
+#define vlseg8e_v_i32m1x8_m	vlseg8e32_v_i32m1x8_m
+#define vlseg8e_v_u32m1x8_m	vlseg8e32_v_u32m1x8_m
+#define vlseg2e_v_i32m2x2	vlseg2e32_v_i32m2x2
+#define vlseg2e_v_u32m2x2	vlseg2e32_v_u32m2x2
+#define vlseg2e_v_i32m2x2_m	vlseg2e32_v_i32m2x2_m
+#define vlseg2e_v_u32m2x2_m	vlseg2e32_v_u32m2x2_m
+#define vlseg3e_v_i32m2x3	vlseg3e32_v_i32m2x3
+#define vlseg3e_v_u32m2x3	vlseg3e32_v_u32m2x3
+#define vlseg3e_v_i32m2x3_m	vlseg3e32_v_i32m2x3_m
+#define vlseg3e_v_u32m2x3_m	vlseg3e32_v_u32m2x3_m
+#define vlseg4e_v_i32m2x4	vlseg4e32_v_i32m2x4
+#define vlseg4e_v_u32m2x4	vlseg4e32_v_u32m2x4
+#define vlseg4e_v_i32m2x4_m	vlseg4e32_v_i32m2x4_m
+#define vlseg4e_v_u32m2x4_m	vlseg4e32_v_u32m2x4_m
+#define vlseg2e_v_i32m4x2	vlseg2e32_v_i32m4x2
+#define vlseg2e_v_u32m4x2	vlseg2e32_v_u32m4x2
+#define vlseg2e_v_i32m4x2_m	vlseg2e32_v_i32m4x2_m
+#define vlseg2e_v_u32m4x2_m	vlseg2e32_v_u32m4x2_m
+#define vlseg2e_v_i64m1x2	vlseg2e64_v_i64m1x2
+#define vlseg2e_v_u64m1x2	vlseg2e64_v_u64m1x2
+#define vlseg2e_v_i64m1x2_m	vlseg2e64_v_i64m1x2_m
+#define vlseg2e_v_u64m1x2_m	vlseg2e64_v_u64m1x2_m
+#define vlseg3e_v_i64m1x3	vlseg3e64_v_i64m1x3
+#define vlseg3e_v_u64m1x3	vlseg3e64_v_u64m1x3
+#define vlseg3e_v_i64m1x3_m	vlseg3e64_v_i64m1x3_m
+#define vlseg3e_v_u64m1x3_m	vlseg3e64_v_u64m1x3_m
+#define vlseg4e_v_i64m1x4	vlseg4e64_v_i64m1x4
+#define vlseg4e_v_u64m1x4	vlseg4e64_v_u64m1x4
+#define vlseg4e_v_i64m1x4_m	vlseg4e64_v_i64m1x4_m
+#define vlseg4e_v_u64m1x4_m	vlseg4e64_v_u64m1x4_m
+#define vlseg5e_v_i64m1x5	vlseg5e64_v_i64m1x5
+#define vlseg5e_v_u64m1x5	vlseg5e64_v_u64m1x5
+#define vlseg5e_v_i64m1x5_m	vlseg5e64_v_i64m1x5_m
+#define vlseg5e_v_u64m1x5_m	vlseg5e64_v_u64m1x5_m
+#define vlseg6e_v_i64m1x6	vlseg6e64_v_i64m1x6
+#define vlseg6e_v_u64m1x6	vlseg6e64_v_u64m1x6
+#define vlseg6e_v_i64m1x6_m	vlseg6e64_v_i64m1x6_m
+#define vlseg6e_v_u64m1x6_m	vlseg6e64_v_u64m1x6_m
+#define vlseg7e_v_i64m1x7	vlseg7e64_v_i64m1x7
+#define vlseg7e_v_u64m1x7	vlseg7e64_v_u64m1x7
+#define vlseg7e_v_i64m1x7_m	vlseg7e64_v_i64m1x7_m
+#define vlseg7e_v_u64m1x7_m	vlseg7e64_v_u64m1x7_m
+#define vlseg8e_v_i64m1x8	vlseg8e64_v_i64m1x8
+#define vlseg8e_v_u64m1x8	vlseg8e64_v_u64m1x8
+#define vlseg8e_v_i64m1x8_m	vlseg8e64_v_i64m1x8_m
+#define vlseg8e_v_u64m1x8_m	vlseg8e64_v_u64m1x8_m
+#define vlseg2e_v_i64m2x2	vlseg2e64_v_i64m2x2
+#define vlseg2e_v_u64m2x2	vlseg2e64_v_u64m2x2
+#define vlseg2e_v_i64m2x2_m	vlseg2e64_v_i64m2x2_m
+#define vlseg2e_v_u64m2x2_m	vlseg2e64_v_u64m2x2_m
+#define vlseg3e_v_i64m2x3	vlseg3e64_v_i64m2x3
+#define vlseg3e_v_u64m2x3	vlseg3e64_v_u64m2x3
+#define vlseg3e_v_i64m2x3_m	vlseg3e64_v_i64m2x3_m
+#define vlseg3e_v_u64m2x3_m	vlseg3e64_v_u64m2x3_m
+#define vlseg4e_v_i64m2x4	vlseg4e64_v_i64m2x4
+#define vlseg4e_v_u64m2x4	vlseg4e64_v_u64m2x4
+#define vlseg4e_v_i64m2x4_m	vlseg4e64_v_i64m2x4_m
+#define vlseg4e_v_u64m2x4_m	vlseg4e64_v_u64m2x4_m
+#define vlseg2e_v_i64m4x2	vlseg2e64_v_i64m4x2
+#define vlseg2e_v_u64m4x2	vlseg2e64_v_u64m4x2
+#define vlseg2e_v_i64m4x2_m	vlseg2e64_v_i64m4x2_m
+#define vlseg2e_v_u64m4x2_m	vlseg2e64_v_u64m4x2_m
+#define vlseg2e_v_f16m1x2	vlseg2e16_v_f16m1x2
+#define vlseg2e_v_f16m1x2_m	vlseg2e16_v_f16m1x2_m
+#define vlseg3e_v_f16m1x3	vlseg3e16_v_f16m1x3
+#define vlseg3e_v_f16m1x3_m	vlseg3e16_v_f16m1x3_m
+#define vlseg4e_v_f16m1x4	vlseg4e16_v_f16m1x4
+#define vlseg4e_v_f16m1x4_m	vlseg4e16_v_f16m1x4_m
+#define vlseg5e_v_f16m1x5	vlseg5e16_v_f16m1x5
+#define vlseg5e_v_f16m1x5_m	vlseg5e16_v_f16m1x5_m
+#define vlseg6e_v_f16m1x6	vlseg6e16_v_f16m1x6
+#define vlseg6e_v_f16m1x6_m	vlseg6e16_v_f16m1x6_m
+#define vlseg7e_v_f16m1x7	vlseg7e16_v_f16m1x7
+#define vlseg7e_v_f16m1x7_m	vlseg7e16_v_f16m1x7_m
+#define vlseg8e_v_f16m1x8	vlseg8e16_v_f16m1x8
+#define vlseg8e_v_f16m1x8_m	vlseg8e16_v_f16m1x8_m
+#define vlseg2e_v_f16m2x2	vlseg2e16_v_f16m2x2
+#define vlseg2e_v_f16m2x2_m	vlseg2e16_v_f16m2x2_m
+#define vlseg3e_v_f16m2x3	vlseg3e16_v_f16m2x3
+#define vlseg3e_v_f16m2x3_m	vlseg3e16_v_f16m2x3_m
+#define vlseg4e_v_f16m2x4	vlseg4e16_v_f16m2x4
+#define vlseg4e_v_f16m2x4_m	vlseg4e16_v_f16m2x4_m
+#define vlseg2e_v_f16m4x2	vlseg2e16_v_f16m4x2
+#define vlseg2e_v_f16m4x2_m	vlseg2e16_v_f16m4x2_m
+#define vlseg2e_v_f32m1x2	vlseg2e32_v_f32m1x2
+#define vlseg2e_v_f32m1x2_m	vlseg2e32_v_f32m1x2_m
+#define vlseg3e_v_f32m1x3	vlseg3e32_v_f32m1x3
+#define vlseg3e_v_f32m1x3_m	vlseg3e32_v_f32m1x3_m
+#define vlseg4e_v_f32m1x4	vlseg4e32_v_f32m1x4
+#define vlseg4e_v_f32m1x4_m	vlseg4e32_v_f32m1x4_m
+#define vlseg5e_v_f32m1x5	vlseg5e32_v_f32m1x5
+#define vlseg5e_v_f32m1x5_m	vlseg5e32_v_f32m1x5_m
+#define vlseg6e_v_f32m1x6	vlseg6e32_v_f32m1x6
+#define vlseg6e_v_f32m1x6_m	vlseg6e32_v_f32m1x6_m
+#define vlseg7e_v_f32m1x7	vlseg7e32_v_f32m1x7
+#define vlseg7e_v_f32m1x7_m	vlseg7e32_v_f32m1x7_m
+#define vlseg8e_v_f32m1x8	vlseg8e32_v_f32m1x8
+#define vlseg8e_v_f32m1x8_m	vlseg8e32_v_f32m1x8_m
+#define vlseg2e_v_f32m2x2	vlseg2e32_v_f32m2x2
+#define vlseg2e_v_f32m2x2_m	vlseg2e32_v_f32m2x2_m
+#define vlseg3e_v_f32m2x3	vlseg3e32_v_f32m2x3
+#define vlseg3e_v_f32m2x3_m	vlseg3e32_v_f32m2x3_m
+#define vlseg4e_v_f32m2x4	vlseg4e32_v_f32m2x4
+#define vlseg4e_v_f32m2x4_m	vlseg4e32_v_f32m2x4_m
+#define vlseg2e_v_f32m4x2	vlseg2e32_v_f32m4x2
+#define vlseg2e_v_f32m4x2_m	vlseg2e32_v_f32m4x2_m
+#define vlseg2e_v_f64m1x2	vlseg2e64_v_f64m1x2
+#define vlseg2e_v_f64m1x2_m	vlseg2e64_v_f64m1x2_m
+#define vlseg3e_v_f64m1x3	vlseg3e64_v_f64m1x3
+#define vlseg3e_v_f64m1x3_m	vlseg3e64_v_f64m1x3_m
+#define vlseg4e_v_f64m1x4	vlseg4e64_v_f64m1x4
+#define vlseg4e_v_f64m1x4_m	vlseg4e64_v_f64m1x4_m
+#define vlseg5e_v_f64m1x5	vlseg5e64_v_f64m1x5
+#define vlseg5e_v_f64m1x5_m	vlseg5e64_v_f64m1x5_m
+#define vlseg6e_v_f64m1x6	vlseg6e64_v_f64m1x6
+#define vlseg6e_v_f64m1x6_m	vlseg6e64_v_f64m1x6_m
+#define vlseg7e_v_f64m1x7	vlseg7e64_v_f64m1x7
+#define vlseg7e_v_f64m1x7_m	vlseg7e64_v_f64m1x7_m
+#define vlseg8e_v_f64m1x8	vlseg8e64_v_f64m1x8
+#define vlseg8e_v_f64m1x8_m	vlseg8e64_v_f64m1x8_m
+#define vlseg2e_v_f64m2x2	vlseg2e64_v_f64m2x2
+#define vlseg2e_v_f64m2x2_m	vlseg2e64_v_f64m2x2_m
+#define vlseg3e_v_f64m2x3	vlseg3e64_v_f64m2x3
+#define vlseg3e_v_f64m2x3_m	vlseg3e64_v_f64m2x3_m
+#define vlseg4e_v_f64m2x4	vlseg4e64_v_f64m2x4
+#define vlseg4e_v_f64m2x4_m	vlseg4e64_v_f64m2x4_m
+#define vlseg2e_v_f64m4x2	vlseg2e64_v_f64m4x2
+#define vlseg2e_v_f64m4x2_m	vlseg2e64_v_f64m4x2_m
+
+/* Wrapper only.  */
+#define vsseg2e_v_i8m1x2	vsseg2e8_v_i8m1x2
+#define vsseg2e_v_u8m1x2	vsseg2e8_v_u8m1x2
+#define vsseg2e_v_i8m1x2_m	vsseg2e8_v_i8m1x2_m
+#define vsseg2e_v_u8m1x2_m	vsseg2e8_v_u8m1x2_m
+#define vsseg3e_v_i8m1x3	vsseg3e8_v_i8m1x3
+#define vsseg3e_v_u8m1x3	vsseg3e8_v_u8m1x3
+#define vsseg3e_v_i8m1x3_m	vsseg3e8_v_i8m1x3_m
+#define vsseg3e_v_u8m1x3_m	vsseg3e8_v_u8m1x3_m
+#define vsseg4e_v_i8m1x4	vsseg4e8_v_i8m1x4
+#define vsseg4e_v_u8m1x4	vsseg4e8_v_u8m1x4
+#define vsseg4e_v_i8m1x4_m	vsseg4e8_v_i8m1x4_m
+#define vsseg4e_v_u8m1x4_m	vsseg4e8_v_u8m1x4_m
+#define vsseg5e_v_i8m1x5	vsseg5e8_v_i8m1x5
+#define vsseg5e_v_u8m1x5	vsseg5e8_v_u8m1x5
+#define vsseg5e_v_i8m1x5_m	vsseg5e8_v_i8m1x5_m
+#define vsseg5e_v_u8m1x5_m	vsseg5e8_v_u8m1x5_m
+#define vsseg6e_v_i8m1x6	vsseg6e8_v_i8m1x6
+#define vsseg6e_v_u8m1x6	vsseg6e8_v_u8m1x6
+#define vsseg6e_v_i8m1x6_m	vsseg6e8_v_i8m1x6_m
+#define vsseg6e_v_u8m1x6_m	vsseg6e8_v_u8m1x6_m
+#define vsseg7e_v_i8m1x7	vsseg7e8_v_i8m1x7
+#define vsseg7e_v_u8m1x7	vsseg7e8_v_u8m1x7
+#define vsseg7e_v_i8m1x7_m	vsseg7e8_v_i8m1x7_m
+#define vsseg7e_v_u8m1x7_m	vsseg7e8_v_u8m1x7_m
+#define vsseg8e_v_i8m1x8	vsseg8e8_v_i8m1x8
+#define vsseg8e_v_u8m1x8	vsseg8e8_v_u8m1x8
+#define vsseg8e_v_i8m1x8_m	vsseg8e8_v_i8m1x8_m
+#define vsseg8e_v_u8m1x8_m	vsseg8e8_v_u8m1x8_m
+#define vsseg2e_v_i8m2x2	vsseg2e8_v_i8m2x2
+#define vsseg2e_v_u8m2x2	vsseg2e8_v_u8m2x2
+#define vsseg2e_v_i8m2x2_m	vsseg2e8_v_i8m2x2_m
+#define vsseg2e_v_u8m2x2_m	vsseg2e8_v_u8m2x2_m
+#define vsseg3e_v_i8m2x3	vsseg3e8_v_i8m2x3
+#define vsseg3e_v_u8m2x3	vsseg3e8_v_u8m2x3
+#define vsseg3e_v_i8m2x3_m	vsseg3e8_v_i8m2x3_m
+#define vsseg3e_v_u8m2x3_m	vsseg3e8_v_u8m2x3_m
+#define vsseg4e_v_i8m2x4	vsseg4e8_v_i8m2x4
+#define vsseg4e_v_u8m2x4	vsseg4e8_v_u8m2x4
+#define vsseg4e_v_i8m2x4_m	vsseg4e8_v_i8m2x4_m
+#define vsseg4e_v_u8m2x4_m	vsseg4e8_v_u8m2x4_m
+#define vsseg2e_v_i8m4x2	vsseg2e8_v_i8m4x2
+#define vsseg2e_v_u8m4x2	vsseg2e8_v_u8m4x2
+#define vsseg2e_v_i8m4x2_m	vsseg2e8_v_i8m4x2_m
+#define vsseg2e_v_u8m4x2_m	vsseg2e8_v_u8m4x2_m
+#define vsseg2e_v_i16m1x2	vsseg2e16_v_i16m1x2
+#define vsseg2e_v_u16m1x2	vsseg2e16_v_u16m1x2
+#define vsseg2e_v_i16m1x2_m	vsseg2e16_v_i16m1x2_m
+#define vsseg2e_v_u16m1x2_m	vsseg2e16_v_u16m1x2_m
+#define vsseg3e_v_i16m1x3	vsseg3e16_v_i16m1x3
+#define vsseg3e_v_u16m1x3	vsseg3e16_v_u16m1x3
+#define vsseg3e_v_i16m1x3_m	vsseg3e16_v_i16m1x3_m
+#define vsseg3e_v_u16m1x3_m	vsseg3e16_v_u16m1x3_m
+#define vsseg4e_v_i16m1x4	vsseg4e16_v_i16m1x4
+#define vsseg4e_v_u16m1x4	vsseg4e16_v_u16m1x4
+#define vsseg4e_v_i16m1x4_m	vsseg4e16_v_i16m1x4_m
+#define vsseg4e_v_u16m1x4_m	vsseg4e16_v_u16m1x4_m
+#define vsseg5e_v_i16m1x5	vsseg5e16_v_i16m1x5
+#define vsseg5e_v_u16m1x5	vsseg5e16_v_u16m1x5
+#define vsseg5e_v_i16m1x5_m	vsseg5e16_v_i16m1x5_m
+#define vsseg5e_v_u16m1x5_m	vsseg5e16_v_u16m1x5_m
+#define vsseg6e_v_i16m1x6	vsseg6e16_v_i16m1x6
+#define vsseg6e_v_u16m1x6	vsseg6e16_v_u16m1x6
+#define vsseg6e_v_i16m1x6_m	vsseg6e16_v_i16m1x6_m
+#define vsseg6e_v_u16m1x6_m	vsseg6e16_v_u16m1x6_m
+#define vsseg7e_v_i16m1x7	vsseg7e16_v_i16m1x7
+#define vsseg7e_v_u16m1x7	vsseg7e16_v_u16m1x7
+#define vsseg7e_v_i16m1x7_m	vsseg7e16_v_i16m1x7_m
+#define vsseg7e_v_u16m1x7_m	vsseg7e16_v_u16m1x7_m
+#define vsseg8e_v_i16m1x8	vsseg8e16_v_i16m1x8
+#define vsseg8e_v_u16m1x8	vsseg8e16_v_u16m1x8
+#define vsseg8e_v_i16m1x8_m	vsseg8e16_v_i16m1x8_m
+#define vsseg8e_v_u16m1x8_m	vsseg8e16_v_u16m1x8_m
+#define vsseg2e_v_i16m2x2	vsseg2e16_v_i16m2x2
+#define vsseg2e_v_u16m2x2	vsseg2e16_v_u16m2x2
+#define vsseg2e_v_i16m2x2_m	vsseg2e16_v_i16m2x2_m
+#define vsseg2e_v_u16m2x2_m	vsseg2e16_v_u16m2x2_m
+#define vsseg3e_v_i16m2x3	vsseg3e16_v_i16m2x3
+#define vsseg3e_v_u16m2x3	vsseg3e16_v_u16m2x3
+#define vsseg3e_v_i16m2x3_m	vsseg3e16_v_i16m2x3_m
+#define vsseg3e_v_u16m2x3_m	vsseg3e16_v_u16m2x3_m
+#define vsseg4e_v_i16m2x4	vsseg4e16_v_i16m2x4
+#define vsseg4e_v_u16m2x4	vsseg4e16_v_u16m2x4
+#define vsseg4e_v_i16m2x4_m	vsseg4e16_v_i16m2x4_m
+#define vsseg4e_v_u16m2x4_m	vsseg4e16_v_u16m2x4_m
+#define vsseg2e_v_i16m4x2	vsseg2e16_v_i16m4x2
+#define vsseg2e_v_u16m4x2	vsseg2e16_v_u16m4x2
+#define vsseg2e_v_i16m4x2_m	vsseg2e16_v_i16m4x2_m
+#define vsseg2e_v_u16m4x2_m	vsseg2e16_v_u16m4x2_m
+#define vsseg2e_v_i32m1x2	vsseg2e32_v_i32m1x2
+#define vsseg2e_v_u32m1x2	vsseg2e32_v_u32m1x2
+#define vsseg2e_v_i32m1x2_m	vsseg2e32_v_i32m1x2_m
+#define vsseg2e_v_u32m1x2_m	vsseg2e32_v_u32m1x2_m
+#define vsseg3e_v_i32m1x3	vsseg3e32_v_i32m1x3
+#define vsseg3e_v_u32m1x3	vsseg3e32_v_u32m1x3
+#define vsseg3e_v_i32m1x3_m	vsseg3e32_v_i32m1x3_m
+#define vsseg3e_v_u32m1x3_m	vsseg3e32_v_u32m1x3_m
+#define vsseg4e_v_i32m1x4	vsseg4e32_v_i32m1x4
+#define vsseg4e_v_u32m1x4	vsseg4e32_v_u32m1x4
+#define vsseg4e_v_i32m1x4_m	vsseg4e32_v_i32m1x4_m
+#define vsseg4e_v_u32m1x4_m	vsseg4e32_v_u32m1x4_m
+#define vsseg5e_v_i32m1x5	vsseg5e32_v_i32m1x5
+#define vsseg5e_v_u32m1x5	vsseg5e32_v_u32m1x5
+#define vsseg5e_v_i32m1x5_m	vsseg5e32_v_i32m1x5_m
+#define vsseg5e_v_u32m1x5_m	vsseg5e32_v_u32m1x5_m
+#define vsseg6e_v_i32m1x6	vsseg6e32_v_i32m1x6
+#define vsseg6e_v_u32m1x6	vsseg6e32_v_u32m1x6
+#define vsseg6e_v_i32m1x6_m	vsseg6e32_v_i32m1x6_m
+#define vsseg6e_v_u32m1x6_m	vsseg6e32_v_u32m1x6_m
+#define vsseg7e_v_i32m1x7	vsseg7e32_v_i32m1x7
+#define vsseg7e_v_u32m1x7	vsseg7e32_v_u32m1x7
+#define vsseg7e_v_i32m1x7_m	vsseg7e32_v_i32m1x7_m
+#define vsseg7e_v_u32m1x7_m	vsseg7e32_v_u32m1x7_m
+#define vsseg8e_v_i32m1x8	vsseg8e32_v_i32m1x8
+#define vsseg8e_v_u32m1x8	vsseg8e32_v_u32m1x8
+#define vsseg8e_v_i32m1x8_m	vsseg8e32_v_i32m1x8_m
+#define vsseg8e_v_u32m1x8_m	vsseg8e32_v_u32m1x8_m
+#define vsseg2e_v_i32m2x2	vsseg2e32_v_i32m2x2
+#define vsseg2e_v_u32m2x2	vsseg2e32_v_u32m2x2
+#define vsseg2e_v_i32m2x2_m	vsseg2e32_v_i32m2x2_m
+#define vsseg2e_v_u32m2x2_m	vsseg2e32_v_u32m2x2_m
+#define vsseg3e_v_i32m2x3	vsseg3e32_v_i32m2x3
+#define vsseg3e_v_u32m2x3	vsseg3e32_v_u32m2x3
+#define vsseg3e_v_i32m2x3_m	vsseg3e32_v_i32m2x3_m
+#define vsseg3e_v_u32m2x3_m	vsseg3e32_v_u32m2x3_m
+#define vsseg4e_v_i32m2x4	vsseg4e32_v_i32m2x4
+#define vsseg4e_v_u32m2x4	vsseg4e32_v_u32m2x4
+#define vsseg4e_v_i32m2x4_m	vsseg4e32_v_i32m2x4_m
+#define vsseg4e_v_u32m2x4_m	vsseg4e32_v_u32m2x4_m
+#define vsseg2e_v_i32m4x2	vsseg2e32_v_i32m4x2
+#define vsseg2e_v_u32m4x2	vsseg2e32_v_u32m4x2
+#define vsseg2e_v_i32m4x2_m	vsseg2e32_v_i32m4x2_m
+#define vsseg2e_v_u32m4x2_m	vsseg2e32_v_u32m4x2_m
+#define vsseg2e_v_i64m1x2	vsseg2e64_v_i64m1x2
+#define vsseg2e_v_u64m1x2	vsseg2e64_v_u64m1x2
+#define vsseg2e_v_i64m1x2_m	vsseg2e64_v_i64m1x2_m
+#define vsseg2e_v_u64m1x2_m	vsseg2e64_v_u64m1x2_m
+#define vsseg3e_v_i64m1x3	vsseg3e64_v_i64m1x3
+#define vsseg3e_v_u64m1x3	vsseg3e64_v_u64m1x3
+#define vsseg3e_v_i64m1x3_m	vsseg3e64_v_i64m1x3_m
+#define vsseg3e_v_u64m1x3_m	vsseg3e64_v_u64m1x3_m
+#define vsseg4e_v_i64m1x4	vsseg4e64_v_i64m1x4
+#define vsseg4e_v_u64m1x4	vsseg4e64_v_u64m1x4
+#define vsseg4e_v_i64m1x4_m	vsseg4e64_v_i64m1x4_m
+#define vsseg4e_v_u64m1x4_m	vsseg4e64_v_u64m1x4_m
+#define vsseg5e_v_i64m1x5	vsseg5e64_v_i64m1x5
+#define vsseg5e_v_u64m1x5	vsseg5e64_v_u64m1x5
+#define vsseg5e_v_i64m1x5_m	vsseg5e64_v_i64m1x5_m
+#define vsseg5e_v_u64m1x5_m	vsseg5e64_v_u64m1x5_m
+#define vsseg6e_v_i64m1x6	vsseg6e64_v_i64m1x6
+#define vsseg6e_v_u64m1x6	vsseg6e64_v_u64m1x6
+#define vsseg6e_v_i64m1x6_m	vsseg6e64_v_i64m1x6_m
+#define vsseg6e_v_u64m1x6_m	vsseg6e64_v_u64m1x6_m
+#define vsseg7e_v_i64m1x7	vsseg7e64_v_i64m1x7
+#define vsseg7e_v_u64m1x7	vsseg7e64_v_u64m1x7
+#define vsseg7e_v_i64m1x7_m	vsseg7e64_v_i64m1x7_m
+#define vsseg7e_v_u64m1x7_m	vsseg7e64_v_u64m1x7_m
+#define vsseg8e_v_i64m1x8	vsseg8e64_v_i64m1x8
+#define vsseg8e_v_u64m1x8	vsseg8e64_v_u64m1x8
+#define vsseg8e_v_i64m1x8_m	vsseg8e64_v_i64m1x8_m
+#define vsseg8e_v_u64m1x8_m	vsseg8e64_v_u64m1x8_m
+#define vsseg2e_v_i64m2x2	vsseg2e64_v_i64m2x2
+#define vsseg2e_v_u64m2x2	vsseg2e64_v_u64m2x2
+#define vsseg2e_v_i64m2x2_m	vsseg2e64_v_i64m2x2_m
+#define vsseg2e_v_u64m2x2_m	vsseg2e64_v_u64m2x2_m
+#define vsseg3e_v_i64m2x3	vsseg3e64_v_i64m2x3
+#define vsseg3e_v_u64m2x3	vsseg3e64_v_u64m2x3
+#define vsseg3e_v_i64m2x3_m	vsseg3e64_v_i64m2x3_m
+#define vsseg3e_v_u64m2x3_m	vsseg3e64_v_u64m2x3_m
+#define vsseg4e_v_i64m2x4	vsseg4e64_v_i64m2x4
+#define vsseg4e_v_u64m2x4	vsseg4e64_v_u64m2x4
+#define vsseg4e_v_i64m2x4_m	vsseg4e64_v_i64m2x4_m
+#define vsseg4e_v_u64m2x4_m	vsseg4e64_v_u64m2x4_m
+#define vsseg2e_v_i64m4x2	vsseg2e64_v_i64m4x2
+#define vsseg2e_v_u64m4x2	vsseg2e64_v_u64m4x2
+#define vsseg2e_v_i64m4x2_m	vsseg2e64_v_i64m4x2_m
+#define vsseg2e_v_u64m4x2_m	vsseg2e64_v_u64m4x2_m
+#define vsseg2e_v_f16m1x2	vsseg2e16_v_f16m1x2
+#define vsseg2e_v_f16m1x2_m	vsseg2e16_v_f16m1x2_m
+#define vsseg3e_v_f16m1x3	vsseg3e16_v_f16m1x3
+#define vsseg3e_v_f16m1x3_m	vsseg3e16_v_f16m1x3_m
+#define vsseg4e_v_f16m1x4	vsseg4e16_v_f16m1x4
+#define vsseg4e_v_f16m1x4_m	vsseg4e16_v_f16m1x4_m
+#define vsseg5e_v_f16m1x5	vsseg5e16_v_f16m1x5
+#define vsseg5e_v_f16m1x5_m	vsseg5e16_v_f16m1x5_m
+#define vsseg6e_v_f16m1x6	vsseg6e16_v_f16m1x6
+#define vsseg6e_v_f16m1x6_m	vsseg6e16_v_f16m1x6_m
+#define vsseg7e_v_f16m1x7	vsseg7e16_v_f16m1x7
+#define vsseg7e_v_f16m1x7_m	vsseg7e16_v_f16m1x7_m
+#define vsseg8e_v_f16m1x8	vsseg8e16_v_f16m1x8
+#define vsseg8e_v_f16m1x8_m	vsseg8e16_v_f16m1x8_m
+#define vsseg2e_v_f16m2x2	vsseg2e16_v_f16m2x2
+#define vsseg2e_v_f16m2x2_m	vsseg2e16_v_f16m2x2_m
+#define vsseg3e_v_f16m2x3	vsseg3e16_v_f16m2x3
+#define vsseg3e_v_f16m2x3_m	vsseg3e16_v_f16m2x3_m
+#define vsseg4e_v_f16m2x4	vsseg4e16_v_f16m2x4
+#define vsseg4e_v_f16m2x4_m	vsseg4e16_v_f16m2x4_m
+#define vsseg2e_v_f16m4x2	vsseg2e16_v_f16m4x2
+#define vsseg2e_v_f16m4x2_m	vsseg2e16_v_f16m4x2_m
+#define vsseg2e_v_f32m1x2	vsseg2e32_v_f32m1x2
+#define vsseg2e_v_f32m1x2_m	vsseg2e32_v_f32m1x2_m
+#define vsseg3e_v_f32m1x3	vsseg3e32_v_f32m1x3
+#define vsseg3e_v_f32m1x3_m	vsseg3e32_v_f32m1x3_m
+#define vsseg4e_v_f32m1x4	vsseg4e32_v_f32m1x4
+#define vsseg4e_v_f32m1x4_m	vsseg4e32_v_f32m1x4_m
+#define vsseg5e_v_f32m1x5	vsseg5e32_v_f32m1x5
+#define vsseg5e_v_f32m1x5_m	vsseg5e32_v_f32m1x5_m
+#define vsseg6e_v_f32m1x6	vsseg6e32_v_f32m1x6
+#define vsseg6e_v_f32m1x6_m	vsseg6e32_v_f32m1x6_m
+#define vsseg7e_v_f32m1x7	vsseg7e32_v_f32m1x7
+#define vsseg7e_v_f32m1x7_m	vsseg7e32_v_f32m1x7_m
+#define vsseg8e_v_f32m1x8	vsseg8e32_v_f32m1x8
+#define vsseg8e_v_f32m1x8_m	vsseg8e32_v_f32m1x8_m
+#define vsseg2e_v_f32m2x2	vsseg2e32_v_f32m2x2
+#define vsseg2e_v_f32m2x2_m	vsseg2e32_v_f32m2x2_m
+#define vsseg3e_v_f32m2x3	vsseg3e32_v_f32m2x3
+#define vsseg3e_v_f32m2x3_m	vsseg3e32_v_f32m2x3_m
+#define vsseg4e_v_f32m2x4	vsseg4e32_v_f32m2x4
+#define vsseg4e_v_f32m2x4_m	vsseg4e32_v_f32m2x4_m
+#define vsseg2e_v_f32m4x2	vsseg2e32_v_f32m4x2
+#define vsseg2e_v_f32m4x2_m	vsseg2e32_v_f32m4x2_m
+#define vsseg2e_v_f64m1x2	vsseg2e64_v_f64m1x2
+#define vsseg2e_v_f64m1x2_m	vsseg2e64_v_f64m1x2_m
+#define vsseg3e_v_f64m1x3	vsseg3e64_v_f64m1x3
+#define vsseg3e_v_f64m1x3_m	vsseg3e64_v_f64m1x3_m
+#define vsseg4e_v_f64m1x4	vsseg4e64_v_f64m1x4
+#define vsseg4e_v_f64m1x4_m	vsseg4e64_v_f64m1x4_m
+#define vsseg5e_v_f64m1x5	vsseg5e64_v_f64m1x5
+#define vsseg5e_v_f64m1x5_m	vsseg5e64_v_f64m1x5_m
+#define vsseg6e_v_f64m1x6	vsseg6e64_v_f64m1x6
+#define vsseg6e_v_f64m1x6_m	vsseg6e64_v_f64m1x6_m
+#define vsseg7e_v_f64m1x7	vsseg7e64_v_f64m1x7
+#define vsseg7e_v_f64m1x7_m	vsseg7e64_v_f64m1x7_m
+#define vsseg8e_v_f64m1x8	vsseg8e64_v_f64m1x8
+#define vsseg8e_v_f64m1x8_m	vsseg8e64_v_f64m1x8_m
+#define vsseg2e_v_f64m2x2	vsseg2e64_v_f64m2x2
+#define vsseg2e_v_f64m2x2_m	vsseg2e64_v_f64m2x2_m
+#define vsseg3e_v_f64m2x3	vsseg3e64_v_f64m2x3
+#define vsseg3e_v_f64m2x3_m	vsseg3e64_v_f64m2x3_m
+#define vsseg4e_v_f64m2x4	vsseg4e64_v_f64m2x4
+#define vsseg4e_v_f64m2x4_m	vsseg4e64_v_f64m2x4_m
+#define vsseg2e_v_f64m4x2	vsseg2e64_v_f64m4x2
+#define vsseg2e_v_f64m4x2_m	vsseg2e64_v_f64m4x2_m
+
+/* Wrapper only.  */
+#define vlsseg2e_v_i8m1x2	vlsseg2e8_v_i8m1x2
+#define vlsseg2e_v_u8m1x2	vlsseg2e8_v_u8m1x2
+#define vlsseg2e_v_i8m1x2_m	vlsseg2e8_v_i8m1x2_m
+#define vlsseg2e_v_u8m1x2_m	vlsseg2e8_v_u8m1x2_m
+#define vlsseg3e_v_i8m1x3	vlsseg3e8_v_i8m1x3
+#define vlsseg3e_v_u8m1x3	vlsseg3e8_v_u8m1x3
+#define vlsseg3e_v_i8m1x3_m	vlsseg3e8_v_i8m1x3_m
+#define vlsseg3e_v_u8m1x3_m	vlsseg3e8_v_u8m1x3_m
+#define vlsseg4e_v_i8m1x4	vlsseg4e8_v_i8m1x4
+#define vlsseg4e_v_u8m1x4	vlsseg4e8_v_u8m1x4
+#define vlsseg4e_v_i8m1x4_m	vlsseg4e8_v_i8m1x4_m
+#define vlsseg4e_v_u8m1x4_m	vlsseg4e8_v_u8m1x4_m
+#define vlsseg5e_v_i8m1x5	vlsseg5e8_v_i8m1x5
+#define vlsseg5e_v_u8m1x5	vlsseg5e8_v_u8m1x5
+#define vlsseg5e_v_i8m1x5_m	vlsseg5e8_v_i8m1x5_m
+#define vlsseg5e_v_u8m1x5_m	vlsseg5e8_v_u8m1x5_m
+#define vlsseg6e_v_i8m1x6	vlsseg6e8_v_i8m1x6
+#define vlsseg6e_v_u8m1x6	vlsseg6e8_v_u8m1x6
+#define vlsseg6e_v_i8m1x6_m	vlsseg6e8_v_i8m1x6_m
+#define vlsseg6e_v_u8m1x6_m	vlsseg6e8_v_u8m1x6_m
+#define vlsseg7e_v_i8m1x7	vlsseg7e8_v_i8m1x7
+#define vlsseg7e_v_u8m1x7	vlsseg7e8_v_u8m1x7
+#define vlsseg7e_v_i8m1x7_m	vlsseg7e8_v_i8m1x7_m
+#define vlsseg7e_v_u8m1x7_m	vlsseg7e8_v_u8m1x7_m
+#define vlsseg8e_v_i8m1x8	vlsseg8e8_v_i8m1x8
+#define vlsseg8e_v_u8m1x8	vlsseg8e8_v_u8m1x8
+#define vlsseg8e_v_i8m1x8_m	vlsseg8e8_v_i8m1x8_m
+#define vlsseg8e_v_u8m1x8_m	vlsseg8e8_v_u8m1x8_m
+#define vlsseg2e_v_i8m2x2	vlsseg2e8_v_i8m2x2
+#define vlsseg2e_v_u8m2x2	vlsseg2e8_v_u8m2x2
+#define vlsseg2e_v_i8m2x2_m	vlsseg2e8_v_i8m2x2_m
+#define vlsseg2e_v_u8m2x2_m	vlsseg2e8_v_u8m2x2_m
+#define vlsseg3e_v_i8m2x3	vlsseg3e8_v_i8m2x3
+#define vlsseg3e_v_u8m2x3	vlsseg3e8_v_u8m2x3
+#define vlsseg3e_v_i8m2x3_m	vlsseg3e8_v_i8m2x3_m
+#define vlsseg3e_v_u8m2x3_m	vlsseg3e8_v_u8m2x3_m
+#define vlsseg4e_v_i8m2x4	vlsseg4e8_v_i8m2x4
+#define vlsseg4e_v_u8m2x4	vlsseg4e8_v_u8m2x4
+#define vlsseg4e_v_i8m2x4_m	vlsseg4e8_v_i8m2x4_m
+#define vlsseg4e_v_u8m2x4_m	vlsseg4e8_v_u8m2x4_m
+#define vlsseg2e_v_i8m4x2	vlsseg2e8_v_i8m4x2
+#define vlsseg2e_v_u8m4x2	vlsseg2e8_v_u8m4x2
+#define vlsseg2e_v_i8m4x2_m	vlsseg2e8_v_i8m4x2_m
+#define vlsseg2e_v_u8m4x2_m	vlsseg2e8_v_u8m4x2_m
+#define vlsseg2e_v_i16m1x2	vlsseg2e16_v_i16m1x2
+#define vlsseg2e_v_u16m1x2	vlsseg2e16_v_u16m1x2
+#define vlsseg2e_v_i16m1x2_m	vlsseg2e16_v_i16m1x2_m
+#define vlsseg2e_v_u16m1x2_m	vlsseg2e16_v_u16m1x2_m
+#define vlsseg3e_v_i16m1x3	vlsseg3e16_v_i16m1x3
+#define vlsseg3e_v_u16m1x3	vlsseg3e16_v_u16m1x3
+#define vlsseg3e_v_i16m1x3_m	vlsseg3e16_v_i16m1x3_m
+#define vlsseg3e_v_u16m1x3_m	vlsseg3e16_v_u16m1x3_m
+#define vlsseg4e_v_i16m1x4	vlsseg4e16_v_i16m1x4
+#define vlsseg4e_v_u16m1x4	vlsseg4e16_v_u16m1x4
+#define vlsseg4e_v_i16m1x4_m	vlsseg4e16_v_i16m1x4_m
+#define vlsseg4e_v_u16m1x4_m	vlsseg4e16_v_u16m1x4_m
+#define vlsseg5e_v_i16m1x5	vlsseg5e16_v_i16m1x5
+#define vlsseg5e_v_u16m1x5	vlsseg5e16_v_u16m1x5
+#define vlsseg5e_v_i16m1x5_m	vlsseg5e16_v_i16m1x5_m
+#define vlsseg5e_v_u16m1x5_m	vlsseg5e16_v_u16m1x5_m
+#define vlsseg6e_v_i16m1x6	vlsseg6e16_v_i16m1x6
+#define vlsseg6e_v_u16m1x6	vlsseg6e16_v_u16m1x6
+#define vlsseg6e_v_i16m1x6_m	vlsseg6e16_v_i16m1x6_m
+#define vlsseg6e_v_u16m1x6_m	vlsseg6e16_v_u16m1x6_m
+#define vlsseg7e_v_i16m1x7	vlsseg7e16_v_i16m1x7
+#define vlsseg7e_v_u16m1x7	vlsseg7e16_v_u16m1x7
+#define vlsseg7e_v_i16m1x7_m	vlsseg7e16_v_i16m1x7_m
+#define vlsseg7e_v_u16m1x7_m	vlsseg7e16_v_u16m1x7_m
+#define vlsseg8e_v_i16m1x8	vlsseg8e16_v_i16m1x8
+#define vlsseg8e_v_u16m1x8	vlsseg8e16_v_u16m1x8
+#define vlsseg8e_v_i16m1x8_m	vlsseg8e16_v_i16m1x8_m
+#define vlsseg8e_v_u16m1x8_m	vlsseg8e16_v_u16m1x8_m
+#define vlsseg2e_v_i16m2x2	vlsseg2e16_v_i16m2x2
+#define vlsseg2e_v_u16m2x2	vlsseg2e16_v_u16m2x2
+#define vlsseg2e_v_i16m2x2_m	vlsseg2e16_v_i16m2x2_m
+#define vlsseg2e_v_u16m2x2_m	vlsseg2e16_v_u16m2x2_m
+#define vlsseg3e_v_i16m2x3	vlsseg3e16_v_i16m2x3
+#define vlsseg3e_v_u16m2x3	vlsseg3e16_v_u16m2x3
+#define vlsseg3e_v_i16m2x3_m	vlsseg3e16_v_i16m2x3_m
+#define vlsseg3e_v_u16m2x3_m	vlsseg3e16_v_u16m2x3_m
+#define vlsseg4e_v_i16m2x4	vlsseg4e16_v_i16m2x4
+#define vlsseg4e_v_u16m2x4	vlsseg4e16_v_u16m2x4
+#define vlsseg4e_v_i16m2x4_m	vlsseg4e16_v_i16m2x4_m
+#define vlsseg4e_v_u16m2x4_m	vlsseg4e16_v_u16m2x4_m
+#define vlsseg2e_v_i16m4x2	vlsseg2e16_v_i16m4x2
+#define vlsseg2e_v_u16m4x2	vlsseg2e16_v_u16m4x2
+#define vlsseg2e_v_i16m4x2_m	vlsseg2e16_v_i16m4x2_m
+#define vlsseg2e_v_u16m4x2_m	vlsseg2e16_v_u16m4x2_m
+#define vlsseg2e_v_i32m1x2	vlsseg2e32_v_i32m1x2
+#define vlsseg2e_v_u32m1x2	vlsseg2e32_v_u32m1x2
+#define vlsseg2e_v_i32m1x2_m	vlsseg2e32_v_i32m1x2_m
+#define vlsseg2e_v_u32m1x2_m	vlsseg2e32_v_u32m1x2_m
+#define vlsseg3e_v_i32m1x3	vlsseg3e32_v_i32m1x3
+#define vlsseg3e_v_u32m1x3	vlsseg3e32_v_u32m1x3
+#define vlsseg3e_v_i32m1x3_m	vlsseg3e32_v_i32m1x3_m
+#define vlsseg3e_v_u32m1x3_m	vlsseg3e32_v_u32m1x3_m
+#define vlsseg4e_v_i32m1x4	vlsseg4e32_v_i32m1x4
+#define vlsseg4e_v_u32m1x4	vlsseg4e32_v_u32m1x4
+#define vlsseg4e_v_i32m1x4_m	vlsseg4e32_v_i32m1x4_m
+#define vlsseg4e_v_u32m1x4_m	vlsseg4e32_v_u32m1x4_m
+#define vlsseg5e_v_i32m1x5	vlsseg5e32_v_i32m1x5
+#define vlsseg5e_v_u32m1x5	vlsseg5e32_v_u32m1x5
+#define vlsseg5e_v_i32m1x5_m	vlsseg5e32_v_i32m1x5_m
+#define vlsseg5e_v_u32m1x5_m	vlsseg5e32_v_u32m1x5_m
+#define vlsseg6e_v_i32m1x6	vlsseg6e32_v_i32m1x6
+#define vlsseg6e_v_u32m1x6	vlsseg6e32_v_u32m1x6
+#define vlsseg6e_v_i32m1x6_m	vlsseg6e32_v_i32m1x6_m
+#define vlsseg6e_v_u32m1x6_m	vlsseg6e32_v_u32m1x6_m
+#define vlsseg7e_v_i32m1x7	vlsseg7e32_v_i32m1x7
+#define vlsseg7e_v_u32m1x7	vlsseg7e32_v_u32m1x7
+#define vlsseg7e_v_i32m1x7_m	vlsseg7e32_v_i32m1x7_m
+#define vlsseg7e_v_u32m1x7_m	vlsseg7e32_v_u32m1x7_m
+#define vlsseg8e_v_i32m1x8	vlsseg8e32_v_i32m1x8
+#define vlsseg8e_v_u32m1x8	vlsseg8e32_v_u32m1x8
+#define vlsseg8e_v_i32m1x8_m	vlsseg8e32_v_i32m1x8_m
+#define vlsseg8e_v_u32m1x8_m	vlsseg8e32_v_u32m1x8_m
+#define vlsseg2e_v_i32m2x2	vlsseg2e32_v_i32m2x2
+#define vlsseg2e_v_u32m2x2	vlsseg2e32_v_u32m2x2
+#define vlsseg2e_v_i32m2x2_m	vlsseg2e32_v_i32m2x2_m
+#define vlsseg2e_v_u32m2x2_m	vlsseg2e32_v_u32m2x2_m
+#define vlsseg3e_v_i32m2x3	vlsseg3e32_v_i32m2x3
+#define vlsseg3e_v_u32m2x3	vlsseg3e32_v_u32m2x3
+#define vlsseg3e_v_i32m2x3_m	vlsseg3e32_v_i32m2x3_m
+#define vlsseg3e_v_u32m2x3_m	vlsseg3e32_v_u32m2x3_m
+#define vlsseg4e_v_i32m2x4	vlsseg4e32_v_i32m2x4
+#define vlsseg4e_v_u32m2x4	vlsseg4e32_v_u32m2x4
+#define vlsseg4e_v_i32m2x4_m	vlsseg4e32_v_i32m2x4_m
+#define vlsseg4e_v_u32m2x4_m	vlsseg4e32_v_u32m2x4_m
+#define vlsseg2e_v_i32m4x2	vlsseg2e32_v_i32m4x2
+#define vlsseg2e_v_u32m4x2	vlsseg2e32_v_u32m4x2
+#define vlsseg2e_v_i32m4x2_m	vlsseg2e32_v_i32m4x2_m
+#define vlsseg2e_v_u32m4x2_m	vlsseg2e32_v_u32m4x2_m
+#define vlsseg2e_v_i64m1x2	vlsseg2e64_v_i64m1x2
+#define vlsseg2e_v_u64m1x2	vlsseg2e64_v_u64m1x2
+#define vlsseg2e_v_i64m1x2_m	vlsseg2e64_v_i64m1x2_m
+#define vlsseg2e_v_u64m1x2_m	vlsseg2e64_v_u64m1x2_m
+#define vlsseg3e_v_i64m1x3	vlsseg3e64_v_i64m1x3
+#define vlsseg3e_v_u64m1x3	vlsseg3e64_v_u64m1x3
+#define vlsseg3e_v_i64m1x3_m	vlsseg3e64_v_i64m1x3_m
+#define vlsseg3e_v_u64m1x3_m	vlsseg3e64_v_u64m1x3_m
+#define vlsseg4e_v_i64m1x4	vlsseg4e64_v_i64m1x4
+#define vlsseg4e_v_u64m1x4	vlsseg4e64_v_u64m1x4
+#define vlsseg4e_v_i64m1x4_m	vlsseg4e64_v_i64m1x4_m
+#define vlsseg4e_v_u64m1x4_m	vlsseg4e64_v_u64m1x4_m
+#define vlsseg5e_v_i64m1x5	vlsseg5e64_v_i64m1x5
+#define vlsseg5e_v_u64m1x5	vlsseg5e64_v_u64m1x5
+#define vlsseg5e_v_i64m1x5_m	vlsseg5e64_v_i64m1x5_m
+#define vlsseg5e_v_u64m1x5_m	vlsseg5e64_v_u64m1x5_m
+#define vlsseg6e_v_i64m1x6	vlsseg6e64_v_i64m1x6
+#define vlsseg6e_v_u64m1x6	vlsseg6e64_v_u64m1x6
+#define vlsseg6e_v_i64m1x6_m	vlsseg6e64_v_i64m1x6_m
+#define vlsseg6e_v_u64m1x6_m	vlsseg6e64_v_u64m1x6_m
+#define vlsseg7e_v_i64m1x7	vlsseg7e64_v_i64m1x7
+#define vlsseg7e_v_u64m1x7	vlsseg7e64_v_u64m1x7
+#define vlsseg7e_v_i64m1x7_m	vlsseg7e64_v_i64m1x7_m
+#define vlsseg7e_v_u64m1x7_m	vlsseg7e64_v_u64m1x7_m
+#define vlsseg8e_v_i64m1x8	vlsseg8e64_v_i64m1x8
+#define vlsseg8e_v_u64m1x8	vlsseg8e64_v_u64m1x8
+#define vlsseg8e_v_i64m1x8_m	vlsseg8e64_v_i64m1x8_m
+#define vlsseg8e_v_u64m1x8_m	vlsseg8e64_v_u64m1x8_m
+#define vlsseg2e_v_i64m2x2	vlsseg2e64_v_i64m2x2
+#define vlsseg2e_v_u64m2x2	vlsseg2e64_v_u64m2x2
+#define vlsseg2e_v_i64m2x2_m	vlsseg2e64_v_i64m2x2_m
+#define vlsseg2e_v_u64m2x2_m	vlsseg2e64_v_u64m2x2_m
+#define vlsseg3e_v_i64m2x3	vlsseg3e64_v_i64m2x3
+#define vlsseg3e_v_u64m2x3	vlsseg3e64_v_u64m2x3
+#define vlsseg3e_v_i64m2x3_m	vlsseg3e64_v_i64m2x3_m
+#define vlsseg3e_v_u64m2x3_m	vlsseg3e64_v_u64m2x3_m
+#define vlsseg4e_v_i64m2x4	vlsseg4e64_v_i64m2x4
+#define vlsseg4e_v_u64m2x4	vlsseg4e64_v_u64m2x4
+#define vlsseg4e_v_i64m2x4_m	vlsseg4e64_v_i64m2x4_m
+#define vlsseg4e_v_u64m2x4_m	vlsseg4e64_v_u64m2x4_m
+#define vlsseg2e_v_i64m4x2	vlsseg2e64_v_i64m4x2
+#define vlsseg2e_v_u64m4x2	vlsseg2e64_v_u64m4x2
+#define vlsseg2e_v_i64m4x2_m	vlsseg2e64_v_i64m4x2_m
+#define vlsseg2e_v_u64m4x2_m	vlsseg2e64_v_u64m4x2_m
+#define vlsseg2e_v_f16m1x2	vlsseg2e16_v_f16m1x2
+#define vlsseg2e_v_f16m1x2_m	vlsseg2e16_v_f16m1x2_m
+#define vlsseg3e_v_f16m1x3	vlsseg3e16_v_f16m1x3
+#define vlsseg3e_v_f16m1x3_m	vlsseg3e16_v_f16m1x3_m
+#define vlsseg4e_v_f16m1x4	vlsseg4e16_v_f16m1x4
+#define vlsseg4e_v_f16m1x4_m	vlsseg4e16_v_f16m1x4_m
+#define vlsseg5e_v_f16m1x5	vlsseg5e16_v_f16m1x5
+#define vlsseg5e_v_f16m1x5_m	vlsseg5e16_v_f16m1x5_m
+#define vlsseg6e_v_f16m1x6	vlsseg6e16_v_f16m1x6
+#define vlsseg6e_v_f16m1x6_m	vlsseg6e16_v_f16m1x6_m
+#define vlsseg7e_v_f16m1x7	vlsseg7e16_v_f16m1x7
+#define vlsseg7e_v_f16m1x7_m	vlsseg7e16_v_f16m1x7_m
+#define vlsseg8e_v_f16m1x8	vlsseg8e16_v_f16m1x8
+#define vlsseg8e_v_f16m1x8_m	vlsseg8e16_v_f16m1x8_m
+#define vlsseg2e_v_f16m2x2	vlsseg2e16_v_f16m2x2
+#define vlsseg2e_v_f16m2x2_m	vlsseg2e16_v_f16m2x2_m
+#define vlsseg3e_v_f16m2x3	vlsseg3e16_v_f16m2x3
+#define vlsseg3e_v_f16m2x3_m	vlsseg3e16_v_f16m2x3_m
+#define vlsseg4e_v_f16m2x4	vlsseg4e16_v_f16m2x4
+#define vlsseg4e_v_f16m2x4_m	vlsseg4e16_v_f16m2x4_m
+#define vlsseg2e_v_f16m4x2	vlsseg2e16_v_f16m4x2
+#define vlsseg2e_v_f16m4x2_m	vlsseg2e16_v_f16m4x2_m
+#define vlsseg2e_v_f32m1x2	vlsseg2e32_v_f32m1x2
+#define vlsseg2e_v_f32m1x2_m	vlsseg2e32_v_f32m1x2_m
+#define vlsseg3e_v_f32m1x3	vlsseg3e32_v_f32m1x3
+#define vlsseg3e_v_f32m1x3_m	vlsseg3e32_v_f32m1x3_m
+#define vlsseg4e_v_f32m1x4	vlsseg4e32_v_f32m1x4
+#define vlsseg4e_v_f32m1x4_m	vlsseg4e32_v_f32m1x4_m
+#define vlsseg5e_v_f32m1x5	vlsseg5e32_v_f32m1x5
+#define vlsseg5e_v_f32m1x5_m	vlsseg5e32_v_f32m1x5_m
+#define vlsseg6e_v_f32m1x6	vlsseg6e32_v_f32m1x6
+#define vlsseg6e_v_f32m1x6_m	vlsseg6e32_v_f32m1x6_m
+#define vlsseg7e_v_f32m1x7	vlsseg7e32_v_f32m1x7
+#define vlsseg7e_v_f32m1x7_m	vlsseg7e32_v_f32m1x7_m
+#define vlsseg8e_v_f32m1x8	vlsseg8e32_v_f32m1x8
+#define vlsseg8e_v_f32m1x8_m	vlsseg8e32_v_f32m1x8_m
+#define vlsseg2e_v_f32m2x2	vlsseg2e32_v_f32m2x2
+#define vlsseg2e_v_f32m2x2_m	vlsseg2e32_v_f32m2x2_m
+#define vlsseg3e_v_f32m2x3	vlsseg3e32_v_f32m2x3
+#define vlsseg3e_v_f32m2x3_m	vlsseg3e32_v_f32m2x3_m
+#define vlsseg4e_v_f32m2x4	vlsseg4e32_v_f32m2x4
+#define vlsseg4e_v_f32m2x4_m	vlsseg4e32_v_f32m2x4_m
+#define vlsseg2e_v_f32m4x2	vlsseg2e32_v_f32m4x2
+#define vlsseg2e_v_f32m4x2_m	vlsseg2e32_v_f32m4x2_m
+#define vlsseg2e_v_f64m1x2	vlsseg2e64_v_f64m1x2
+#define vlsseg2e_v_f64m1x2_m	vlsseg2e64_v_f64m1x2_m
+#define vlsseg3e_v_f64m1x3	vlsseg3e64_v_f64m1x3
+#define vlsseg3e_v_f64m1x3_m	vlsseg3e64_v_f64m1x3_m
+#define vlsseg4e_v_f64m1x4	vlsseg4e64_v_f64m1x4
+#define vlsseg4e_v_f64m1x4_m	vlsseg4e64_v_f64m1x4_m
+#define vlsseg5e_v_f64m1x5	vlsseg5e64_v_f64m1x5
+#define vlsseg5e_v_f64m1x5_m	vlsseg5e64_v_f64m1x5_m
+#define vlsseg6e_v_f64m1x6	vlsseg6e64_v_f64m1x6
+#define vlsseg6e_v_f64m1x6_m	vlsseg6e64_v_f64m1x6_m
+#define vlsseg7e_v_f64m1x7	vlsseg7e64_v_f64m1x7
+#define vlsseg7e_v_f64m1x7_m	vlsseg7e64_v_f64m1x7_m
+#define vlsseg8e_v_f64m1x8	vlsseg8e64_v_f64m1x8
+#define vlsseg8e_v_f64m1x8_m	vlsseg8e64_v_f64m1x8_m
+#define vlsseg2e_v_f64m2x2	vlsseg2e64_v_f64m2x2
+#define vlsseg2e_v_f64m2x2_m	vlsseg2e64_v_f64m2x2_m
+#define vlsseg3e_v_f64m2x3	vlsseg3e64_v_f64m2x3
+#define vlsseg3e_v_f64m2x3_m	vlsseg3e64_v_f64m2x3_m
+#define vlsseg4e_v_f64m2x4	vlsseg4e64_v_f64m2x4
+#define vlsseg4e_v_f64m2x4_m	vlsseg4e64_v_f64m2x4_m
+#define vlsseg2e_v_f64m4x2	vlsseg2e64_v_f64m4x2
+#define vlsseg2e_v_f64m4x2_m	vlsseg2e64_v_f64m4x2_m
+
+/* Wrapper only.  */
+#define vssseg2e_v_i8m1x2	vssseg2e8_v_i8m1x2
+#define vssseg2e_v_u8m1x2	vssseg2e8_v_u8m1x2
+#define vssseg2e_v_i8m1x2_m	vssseg2e8_v_i8m1x2_m
+#define vssseg2e_v_u8m1x2_m	vssseg2e8_v_u8m1x2_m
+#define vssseg3e_v_i8m1x3	vssseg3e8_v_i8m1x3
+#define vssseg3e_v_u8m1x3	vssseg3e8_v_u8m1x3
+#define vssseg3e_v_i8m1x3_m	vssseg3e8_v_i8m1x3_m
+#define vssseg3e_v_u8m1x3_m	vssseg3e8_v_u8m1x3_m
+#define vssseg4e_v_i8m1x4	vssseg4e8_v_i8m1x4
+#define vssseg4e_v_u8m1x4	vssseg4e8_v_u8m1x4
+#define vssseg4e_v_i8m1x4_m	vssseg4e8_v_i8m1x4_m
+#define vssseg4e_v_u8m1x4_m	vssseg4e8_v_u8m1x4_m
+#define vssseg5e_v_i8m1x5	vssseg5e8_v_i8m1x5
+#define vssseg5e_v_u8m1x5	vssseg5e8_v_u8m1x5
+#define vssseg5e_v_i8m1x5_m	vssseg5e8_v_i8m1x5_m
+#define vssseg5e_v_u8m1x5_m	vssseg5e8_v_u8m1x5_m
+#define vssseg6e_v_i8m1x6	vssseg6e8_v_i8m1x6
+#define vssseg6e_v_u8m1x6	vssseg6e8_v_u8m1x6
+#define vssseg6e_v_i8m1x6_m	vssseg6e8_v_i8m1x6_m
+#define vssseg6e_v_u8m1x6_m	vssseg6e8_v_u8m1x6_m
+#define vssseg7e_v_i8m1x7	vssseg7e8_v_i8m1x7
+#define vssseg7e_v_u8m1x7	vssseg7e8_v_u8m1x7
+#define vssseg7e_v_i8m1x7_m	vssseg7e8_v_i8m1x7_m
+#define vssseg7e_v_u8m1x7_m	vssseg7e8_v_u8m1x7_m
+#define vssseg8e_v_i8m1x8	vssseg8e8_v_i8m1x8
+#define vssseg8e_v_u8m1x8	vssseg8e8_v_u8m1x8
+#define vssseg8e_v_i8m1x8_m	vssseg8e8_v_i8m1x8_m
+#define vssseg8e_v_u8m1x8_m	vssseg8e8_v_u8m1x8_m
+#define vssseg2e_v_i8m2x2	vssseg2e8_v_i8m2x2
+#define vssseg2e_v_u8m2x2	vssseg2e8_v_u8m2x2
+#define vssseg2e_v_i8m2x2_m	vssseg2e8_v_i8m2x2_m
+#define vssseg2e_v_u8m2x2_m	vssseg2e8_v_u8m2x2_m
+#define vssseg3e_v_i8m2x3	vssseg3e8_v_i8m2x3
+#define vssseg3e_v_u8m2x3	vssseg3e8_v_u8m2x3
+#define vssseg3e_v_i8m2x3_m	vssseg3e8_v_i8m2x3_m
+#define vssseg3e_v_u8m2x3_m	vssseg3e8_v_u8m2x3_m
+#define vssseg4e_v_i8m2x4	vssseg4e8_v_i8m2x4
+#define vssseg4e_v_u8m2x4	vssseg4e8_v_u8m2x4
+#define vssseg4e_v_i8m2x4_m	vssseg4e8_v_i8m2x4_m
+#define vssseg4e_v_u8m2x4_m	vssseg4e8_v_u8m2x4_m
+#define vssseg2e_v_i8m4x2	vssseg2e8_v_i8m4x2
+#define vssseg2e_v_u8m4x2	vssseg2e8_v_u8m4x2
+#define vssseg2e_v_i8m4x2_m	vssseg2e8_v_i8m4x2_m
+#define vssseg2e_v_u8m4x2_m	vssseg2e8_v_u8m4x2_m
+#define vssseg2e_v_i16m1x2	vssseg2e16_v_i16m1x2
+#define vssseg2e_v_u16m1x2	vssseg2e16_v_u16m1x2
+#define vssseg2e_v_i16m1x2_m	vssseg2e16_v_i16m1x2_m
+#define vssseg2e_v_u16m1x2_m	vssseg2e16_v_u16m1x2_m
+#define vssseg3e_v_i16m1x3	vssseg3e16_v_i16m1x3
+#define vssseg3e_v_u16m1x3	vssseg3e16_v_u16m1x3
+#define vssseg3e_v_i16m1x3_m	vssseg3e16_v_i16m1x3_m
+#define vssseg3e_v_u16m1x3_m	vssseg3e16_v_u16m1x3_m
+#define vssseg4e_v_i16m1x4	vssseg4e16_v_i16m1x4
+#define vssseg4e_v_u16m1x4	vssseg4e16_v_u16m1x4
+#define vssseg4e_v_i16m1x4_m	vssseg4e16_v_i16m1x4_m
+#define vssseg4e_v_u16m1x4_m	vssseg4e16_v_u16m1x4_m
+#define vssseg5e_v_i16m1x5	vssseg5e16_v_i16m1x5
+#define vssseg5e_v_u16m1x5	vssseg5e16_v_u16m1x5
+#define vssseg5e_v_i16m1x5_m	vssseg5e16_v_i16m1x5_m
+#define vssseg5e_v_u16m1x5_m	vssseg5e16_v_u16m1x5_m
+#define vssseg6e_v_i16m1x6	vssseg6e16_v_i16m1x6
+#define vssseg6e_v_u16m1x6	vssseg6e16_v_u16m1x6
+#define vssseg6e_v_i16m1x6_m	vssseg6e16_v_i16m1x6_m
+#define vssseg6e_v_u16m1x6_m	vssseg6e16_v_u16m1x6_m
+#define vssseg7e_v_i16m1x7	vssseg7e16_v_i16m1x7
+#define vssseg7e_v_u16m1x7	vssseg7e16_v_u16m1x7
+#define vssseg7e_v_i16m1x7_m	vssseg7e16_v_i16m1x7_m
+#define vssseg7e_v_u16m1x7_m	vssseg7e16_v_u16m1x7_m
+#define vssseg8e_v_i16m1x8	vssseg8e16_v_i16m1x8
+#define vssseg8e_v_u16m1x8	vssseg8e16_v_u16m1x8
+#define vssseg8e_v_i16m1x8_m	vssseg8e16_v_i16m1x8_m
+#define vssseg8e_v_u16m1x8_m	vssseg8e16_v_u16m1x8_m
+#define vssseg2e_v_i16m2x2	vssseg2e16_v_i16m2x2
+#define vssseg2e_v_u16m2x2	vssseg2e16_v_u16m2x2
+#define vssseg2e_v_i16m2x2_m	vssseg2e16_v_i16m2x2_m
+#define vssseg2e_v_u16m2x2_m	vssseg2e16_v_u16m2x2_m
+#define vssseg3e_v_i16m2x3	vssseg3e16_v_i16m2x3
+#define vssseg3e_v_u16m2x3	vssseg3e16_v_u16m2x3
+#define vssseg3e_v_i16m2x3_m	vssseg3e16_v_i16m2x3_m
+#define vssseg3e_v_u16m2x3_m	vssseg3e16_v_u16m2x3_m
+#define vssseg4e_v_i16m2x4	vssseg4e16_v_i16m2x4
+#define vssseg4e_v_u16m2x4	vssseg4e16_v_u16m2x4
+#define vssseg4e_v_i16m2x4_m	vssseg4e16_v_i16m2x4_m
+#define vssseg4e_v_u16m2x4_m	vssseg4e16_v_u16m2x4_m
+#define vssseg2e_v_i16m4x2	vssseg2e16_v_i16m4x2
+#define vssseg2e_v_u16m4x2	vssseg2e16_v_u16m4x2
+#define vssseg2e_v_i16m4x2_m	vssseg2e16_v_i16m4x2_m
+#define vssseg2e_v_u16m4x2_m	vssseg2e16_v_u16m4x2_m
+#define vssseg2e_v_i32m1x2	vssseg2e32_v_i32m1x2
+#define vssseg2e_v_u32m1x2	vssseg2e32_v_u32m1x2
+#define vssseg2e_v_i32m1x2_m	vssseg2e32_v_i32m1x2_m
+#define vssseg2e_v_u32m1x2_m	vssseg2e32_v_u32m1x2_m
+#define vssseg3e_v_i32m1x3	vssseg3e32_v_i32m1x3
+#define vssseg3e_v_u32m1x3	vssseg3e32_v_u32m1x3
+#define vssseg3e_v_i32m1x3_m	vssseg3e32_v_i32m1x3_m
+#define vssseg3e_v_u32m1x3_m	vssseg3e32_v_u32m1x3_m
+#define vssseg4e_v_i32m1x4	vssseg4e32_v_i32m1x4
+#define vssseg4e_v_u32m1x4	vssseg4e32_v_u32m1x4
+#define vssseg4e_v_i32m1x4_m	vssseg4e32_v_i32m1x4_m
+#define vssseg4e_v_u32m1x4_m	vssseg4e32_v_u32m1x4_m
+#define vssseg5e_v_i32m1x5	vssseg5e32_v_i32m1x5
+#define vssseg5e_v_u32m1x5	vssseg5e32_v_u32m1x5
+#define vssseg5e_v_i32m1x5_m	vssseg5e32_v_i32m1x5_m
+#define vssseg5e_v_u32m1x5_m	vssseg5e32_v_u32m1x5_m
+#define vssseg6e_v_i32m1x6	vssseg6e32_v_i32m1x6
+#define vssseg6e_v_u32m1x6	vssseg6e32_v_u32m1x6
+#define vssseg6e_v_i32m1x6_m	vssseg6e32_v_i32m1x6_m
+#define vssseg6e_v_u32m1x6_m	vssseg6e32_v_u32m1x6_m
+#define vssseg7e_v_i32m1x7	vssseg7e32_v_i32m1x7
+#define vssseg7e_v_u32m1x7	vssseg7e32_v_u32m1x7
+#define vssseg7e_v_i32m1x7_m	vssseg7e32_v_i32m1x7_m
+#define vssseg7e_v_u32m1x7_m	vssseg7e32_v_u32m1x7_m
+#define vssseg8e_v_i32m1x8	vssseg8e32_v_i32m1x8
+#define vssseg8e_v_u32m1x8	vssseg8e32_v_u32m1x8
+#define vssseg8e_v_i32m1x8_m	vssseg8e32_v_i32m1x8_m
+#define vssseg8e_v_u32m1x8_m	vssseg8e32_v_u32m1x8_m
+#define vssseg2e_v_i32m2x2	vssseg2e32_v_i32m2x2
+#define vssseg2e_v_u32m2x2	vssseg2e32_v_u32m2x2
+#define vssseg2e_v_i32m2x2_m	vssseg2e32_v_i32m2x2_m
+#define vssseg2e_v_u32m2x2_m	vssseg2e32_v_u32m2x2_m
+#define vssseg3e_v_i32m2x3	vssseg3e32_v_i32m2x3
+#define vssseg3e_v_u32m2x3	vssseg3e32_v_u32m2x3
+#define vssseg3e_v_i32m2x3_m	vssseg3e32_v_i32m2x3_m
+#define vssseg3e_v_u32m2x3_m	vssseg3e32_v_u32m2x3_m
+#define vssseg4e_v_i32m2x4	vssseg4e32_v_i32m2x4
+#define vssseg4e_v_u32m2x4	vssseg4e32_v_u32m2x4
+#define vssseg4e_v_i32m2x4_m	vssseg4e32_v_i32m2x4_m
+#define vssseg4e_v_u32m2x4_m	vssseg4e32_v_u32m2x4_m
+#define vssseg2e_v_i32m4x2	vssseg2e32_v_i32m4x2
+#define vssseg2e_v_u32m4x2	vssseg2e32_v_u32m4x2
+#define vssseg2e_v_i32m4x2_m	vssseg2e32_v_i32m4x2_m
+#define vssseg2e_v_u32m4x2_m	vssseg2e32_v_u32m4x2_m
+#define vssseg2e_v_i64m1x2	vssseg2e64_v_i64m1x2
+#define vssseg2e_v_u64m1x2	vssseg2e64_v_u64m1x2
+#define vssseg2e_v_i64m1x2_m	vssseg2e64_v_i64m1x2_m
+#define vssseg2e_v_u64m1x2_m	vssseg2e64_v_u64m1x2_m
+#define vssseg3e_v_i64m1x3	vssseg3e64_v_i64m1x3
+#define vssseg3e_v_u64m1x3	vssseg3e64_v_u64m1x3
+#define vssseg3e_v_i64m1x3_m	vssseg3e64_v_i64m1x3_m
+#define vssseg3e_v_u64m1x3_m	vssseg3e64_v_u64m1x3_m
+#define vssseg4e_v_i64m1x4	vssseg4e64_v_i64m1x4
+#define vssseg4e_v_u64m1x4	vssseg4e64_v_u64m1x4
+#define vssseg4e_v_i64m1x4_m	vssseg4e64_v_i64m1x4_m
+#define vssseg4e_v_u64m1x4_m	vssseg4e64_v_u64m1x4_m
+#define vssseg5e_v_i64m1x5	vssseg5e64_v_i64m1x5
+#define vssseg5e_v_u64m1x5	vssseg5e64_v_u64m1x5
+#define vssseg5e_v_i64m1x5_m	vssseg5e64_v_i64m1x5_m
+#define vssseg5e_v_u64m1x5_m	vssseg5e64_v_u64m1x5_m
+#define vssseg6e_v_i64m1x6	vssseg6e64_v_i64m1x6
+#define vssseg6e_v_u64m1x6	vssseg6e64_v_u64m1x6
+#define vssseg6e_v_i64m1x6_m	vssseg6e64_v_i64m1x6_m
+#define vssseg6e_v_u64m1x6_m	vssseg6e64_v_u64m1x6_m
+#define vssseg7e_v_i64m1x7	vssseg7e64_v_i64m1x7
+#define vssseg7e_v_u64m1x7	vssseg7e64_v_u64m1x7
+#define vssseg7e_v_i64m1x7_m	vssseg7e64_v_i64m1x7_m
+#define vssseg7e_v_u64m1x7_m	vssseg7e64_v_u64m1x7_m
+#define vssseg8e_v_i64m1x8	vssseg8e64_v_i64m1x8
+#define vssseg8e_v_u64m1x8	vssseg8e64_v_u64m1x8
+#define vssseg8e_v_i64m1x8_m	vssseg8e64_v_i64m1x8_m
+#define vssseg8e_v_u64m1x8_m	vssseg8e64_v_u64m1x8_m
+#define vssseg2e_v_i64m2x2	vssseg2e64_v_i64m2x2
+#define vssseg2e_v_u64m2x2	vssseg2e64_v_u64m2x2
+#define vssseg2e_v_i64m2x2_m	vssseg2e64_v_i64m2x2_m
+#define vssseg2e_v_u64m2x2_m	vssseg2e64_v_u64m2x2_m
+#define vssseg3e_v_i64m2x3	vssseg3e64_v_i64m2x3
+#define vssseg3e_v_u64m2x3	vssseg3e64_v_u64m2x3
+#define vssseg3e_v_i64m2x3_m	vssseg3e64_v_i64m2x3_m
+#define vssseg3e_v_u64m2x3_m	vssseg3e64_v_u64m2x3_m
+#define vssseg4e_v_i64m2x4	vssseg4e64_v_i64m2x4
+#define vssseg4e_v_u64m2x4	vssseg4e64_v_u64m2x4
+#define vssseg4e_v_i64m2x4_m	vssseg4e64_v_i64m2x4_m
+#define vssseg4e_v_u64m2x4_m	vssseg4e64_v_u64m2x4_m
+#define vssseg2e_v_i64m4x2	vssseg2e64_v_i64m4x2
+#define vssseg2e_v_u64m4x2	vssseg2e64_v_u64m4x2
+#define vssseg2e_v_i64m4x2_m	vssseg2e64_v_i64m4x2_m
+#define vssseg2e_v_u64m4x2_m	vssseg2e64_v_u64m4x2_m
+#define vssseg2e_v_f16m1x2	vssseg2e16_v_f16m1x2
+#define vssseg2e_v_f16m1x2_m	vssseg2e16_v_f16m1x2_m
+#define vssseg3e_v_f16m1x3	vssseg3e16_v_f16m1x3
+#define vssseg3e_v_f16m1x3_m	vssseg3e16_v_f16m1x3_m
+#define vssseg4e_v_f16m1x4	vssseg4e16_v_f16m1x4
+#define vssseg4e_v_f16m1x4_m	vssseg4e16_v_f16m1x4_m
+#define vssseg5e_v_f16m1x5	vssseg5e16_v_f16m1x5
+#define vssseg5e_v_f16m1x5_m	vssseg5e16_v_f16m1x5_m
+#define vssseg6e_v_f16m1x6	vssseg6e16_v_f16m1x6
+#define vssseg6e_v_f16m1x6_m	vssseg6e16_v_f16m1x6_m
+#define vssseg7e_v_f16m1x7	vssseg7e16_v_f16m1x7
+#define vssseg7e_v_f16m1x7_m	vssseg7e16_v_f16m1x7_m
+#define vssseg8e_v_f16m1x8	vssseg8e16_v_f16m1x8
+#define vssseg8e_v_f16m1x8_m	vssseg8e16_v_f16m1x8_m
+#define vssseg2e_v_f16m2x2	vssseg2e16_v_f16m2x2
+#define vssseg2e_v_f16m2x2_m	vssseg2e16_v_f16m2x2_m
+#define vssseg3e_v_f16m2x3	vssseg3e16_v_f16m2x3
+#define vssseg3e_v_f16m2x3_m	vssseg3e16_v_f16m2x3_m
+#define vssseg4e_v_f16m2x4	vssseg4e16_v_f16m2x4
+#define vssseg4e_v_f16m2x4_m	vssseg4e16_v_f16m2x4_m
+#define vssseg2e_v_f16m4x2	vssseg2e16_v_f16m4x2
+#define vssseg2e_v_f16m4x2_m	vssseg2e16_v_f16m4x2_m
+#define vssseg2e_v_f32m1x2	vssseg2e32_v_f32m1x2
+#define vssseg2e_v_f32m1x2_m	vssseg2e32_v_f32m1x2_m
+#define vssseg3e_v_f32m1x3	vssseg3e32_v_f32m1x3
+#define vssseg3e_v_f32m1x3_m	vssseg3e32_v_f32m1x3_m
+#define vssseg4e_v_f32m1x4	vssseg4e32_v_f32m1x4
+#define vssseg4e_v_f32m1x4_m	vssseg4e32_v_f32m1x4_m
+#define vssseg5e_v_f32m1x5	vssseg5e32_v_f32m1x5
+#define vssseg5e_v_f32m1x5_m	vssseg5e32_v_f32m1x5_m
+#define vssseg6e_v_f32m1x6	vssseg6e32_v_f32m1x6
+#define vssseg6e_v_f32m1x6_m	vssseg6e32_v_f32m1x6_m
+#define vssseg7e_v_f32m1x7	vssseg7e32_v_f32m1x7
+#define vssseg7e_v_f32m1x7_m	vssseg7e32_v_f32m1x7_m
+#define vssseg8e_v_f32m1x8	vssseg8e32_v_f32m1x8
+#define vssseg8e_v_f32m1x8_m	vssseg8e32_v_f32m1x8_m
+#define vssseg2e_v_f32m2x2	vssseg2e32_v_f32m2x2
+#define vssseg2e_v_f32m2x2_m	vssseg2e32_v_f32m2x2_m
+#define vssseg3e_v_f32m2x3	vssseg3e32_v_f32m2x3
+#define vssseg3e_v_f32m2x3_m	vssseg3e32_v_f32m2x3_m
+#define vssseg4e_v_f32m2x4	vssseg4e32_v_f32m2x4
+#define vssseg4e_v_f32m2x4_m	vssseg4e32_v_f32m2x4_m
+#define vssseg2e_v_f32m4x2	vssseg2e32_v_f32m4x2
+#define vssseg2e_v_f32m4x2_m	vssseg2e32_v_f32m4x2_m
+#define vssseg2e_v_f64m1x2	vssseg2e64_v_f64m1x2
+#define vssseg2e_v_f64m1x2_m	vssseg2e64_v_f64m1x2_m
+#define vssseg3e_v_f64m1x3	vssseg3e64_v_f64m1x3
+#define vssseg3e_v_f64m1x3_m	vssseg3e64_v_f64m1x3_m
+#define vssseg4e_v_f64m1x4	vssseg4e64_v_f64m1x4
+#define vssseg4e_v_f64m1x4_m	vssseg4e64_v_f64m1x4_m
+#define vssseg5e_v_f64m1x5	vssseg5e64_v_f64m1x5
+#define vssseg5e_v_f64m1x5_m	vssseg5e64_v_f64m1x5_m
+#define vssseg6e_v_f64m1x6	vssseg6e64_v_f64m1x6
+#define vssseg6e_v_f64m1x6_m	vssseg6e64_v_f64m1x6_m
+#define vssseg7e_v_f64m1x7	vssseg7e64_v_f64m1x7
+#define vssseg7e_v_f64m1x7_m	vssseg7e64_v_f64m1x7_m
+#define vssseg8e_v_f64m1x8	vssseg8e64_v_f64m1x8
+#define vssseg8e_v_f64m1x8_m	vssseg8e64_v_f64m1x8_m
+#define vssseg2e_v_f64m2x2	vssseg2e64_v_f64m2x2
+#define vssseg2e_v_f64m2x2_m	vssseg2e64_v_f64m2x2_m
+#define vssseg3e_v_f64m2x3	vssseg3e64_v_f64m2x3
+#define vssseg3e_v_f64m2x3_m	vssseg3e64_v_f64m2x3_m
+#define vssseg4e_v_f64m2x4	vssseg4e64_v_f64m2x4
+#define vssseg4e_v_f64m2x4_m	vssseg4e64_v_f64m2x4_m
+#define vssseg2e_v_f64m4x2	vssseg2e64_v_f64m4x2
+#define vssseg2e_v_f64m4x2_m	vssseg2e64_v_f64m4x2_m
+
+/* Wrapper only.  */
+#define vlxseg2e_v_i8m1x2	vloxseg2ei8_v_i8m1x2
+#define vlxseg2e_v_u8m1x2	vloxseg2ei8_v_u8m1x2
+#define vlxseg2e_v_i8m1x2_m	vloxseg2ei8_v_i8m1x2_m
+#define vlxseg2e_v_u8m1x2_m	vloxseg2ei8_v_u8m1x2_m
+#define vlxseg3e_v_i8m1x3	vloxseg3ei8_v_i8m1x3
+#define vlxseg3e_v_u8m1x3	vloxseg3ei8_v_u8m1x3
+#define vlxseg3e_v_i8m1x3_m	vloxseg3ei8_v_i8m1x3_m
+#define vlxseg3e_v_u8m1x3_m	vloxseg3ei8_v_u8m1x3_m
+#define vlxseg4e_v_i8m1x4	vloxseg4ei8_v_i8m1x4
+#define vlxseg4e_v_u8m1x4	vloxseg4ei8_v_u8m1x4
+#define vlxseg4e_v_i8m1x4_m	vloxseg4ei8_v_i8m1x4_m
+#define vlxseg4e_v_u8m1x4_m	vloxseg4ei8_v_u8m1x4_m
+#define vlxseg5e_v_i8m1x5	vloxseg5ei8_v_i8m1x5
+#define vlxseg5e_v_u8m1x5	vloxseg5ei8_v_u8m1x5
+#define vlxseg5e_v_i8m1x5_m	vloxseg5ei8_v_i8m1x5_m
+#define vlxseg5e_v_u8m1x5_m	vloxseg5ei8_v_u8m1x5_m
+#define vlxseg6e_v_i8m1x6	vloxseg6ei8_v_i8m1x6
+#define vlxseg6e_v_u8m1x6	vloxseg6ei8_v_u8m1x6
+#define vlxseg6e_v_i8m1x6_m	vloxseg6ei8_v_i8m1x6_m
+#define vlxseg6e_v_u8m1x6_m	vloxseg6ei8_v_u8m1x6_m
+#define vlxseg7e_v_i8m1x7	vloxseg7ei8_v_i8m1x7
+#define vlxseg7e_v_u8m1x7	vloxseg7ei8_v_u8m1x7
+#define vlxseg7e_v_i8m1x7_m	vloxseg7ei8_v_i8m1x7_m
+#define vlxseg7e_v_u8m1x7_m	vloxseg7ei8_v_u8m1x7_m
+#define vlxseg8e_v_i8m1x8	vloxseg8ei8_v_i8m1x8
+#define vlxseg8e_v_u8m1x8	vloxseg8ei8_v_u8m1x8
+#define vlxseg8e_v_i8m1x8_m	vloxseg8ei8_v_i8m1x8_m
+#define vlxseg8e_v_u8m1x8_m	vloxseg8ei8_v_u8m1x8_m
+#define vlxseg2e_v_i8m2x2	vloxseg2ei8_v_i8m2x2
+#define vlxseg2e_v_u8m2x2	vloxseg2ei8_v_u8m2x2
+#define vlxseg2e_v_i8m2x2_m	vloxseg2ei8_v_i8m2x2_m
+#define vlxseg2e_v_u8m2x2_m	vloxseg2ei8_v_u8m2x2_m
+#define vlxseg3e_v_i8m2x3	vloxseg3ei8_v_i8m2x3
+#define vlxseg3e_v_u8m2x3	vloxseg3ei8_v_u8m2x3
+#define vlxseg3e_v_i8m2x3_m	vloxseg3ei8_v_i8m2x3_m
+#define vlxseg3e_v_u8m2x3_m	vloxseg3ei8_v_u8m2x3_m
+#define vlxseg4e_v_i8m2x4	vloxseg4ei8_v_i8m2x4
+#define vlxseg4e_v_u8m2x4	vloxseg4ei8_v_u8m2x4
+#define vlxseg4e_v_i8m2x4_m	vloxseg4ei8_v_i8m2x4_m
+#define vlxseg4e_v_u8m2x4_m	vloxseg4ei8_v_u8m2x4_m
+#define vlxseg2e_v_i8m4x2	vloxseg2ei8_v_i8m4x2
+#define vlxseg2e_v_u8m4x2	vloxseg2ei8_v_u8m4x2
+#define vlxseg2e_v_i8m4x2_m	vloxseg2ei8_v_i8m4x2_m
+#define vlxseg2e_v_u8m4x2_m	vloxseg2ei8_v_u8m4x2_m
+#define vlxseg2e_v_i16m1x2	vloxseg2ei16_v_i16m1x2
+#define vlxseg2e_v_u16m1x2	vloxseg2ei16_v_u16m1x2
+#define vlxseg2e_v_i16m1x2_m	vloxseg2ei16_v_i16m1x2_m
+#define vlxseg2e_v_u16m1x2_m	vloxseg2ei16_v_u16m1x2_m
+#define vlxseg3e_v_i16m1x3	vloxseg3ei16_v_i16m1x3
+#define vlxseg3e_v_u16m1x3	vloxseg3ei16_v_u16m1x3
+#define vlxseg3e_v_i16m1x3_m	vloxseg3ei16_v_i16m1x3_m
+#define vlxseg3e_v_u16m1x3_m	vloxseg3ei16_v_u16m1x3_m
+#define vlxseg4e_v_i16m1x4	vloxseg4ei16_v_i16m1x4
+#define vlxseg4e_v_u16m1x4	vloxseg4ei16_v_u16m1x4
+#define vlxseg4e_v_i16m1x4_m	vloxseg4ei16_v_i16m1x4_m
+#define vlxseg4e_v_u16m1x4_m	vloxseg4ei16_v_u16m1x4_m
+#define vlxseg5e_v_i16m1x5	vloxseg5ei16_v_i16m1x5
+#define vlxseg5e_v_u16m1x5	vloxseg5ei16_v_u16m1x5
+#define vlxseg5e_v_i16m1x5_m	vloxseg5ei16_v_i16m1x5_m
+#define vlxseg5e_v_u16m1x5_m	vloxseg5ei16_v_u16m1x5_m
+#define vlxseg6e_v_i16m1x6	vloxseg6ei16_v_i16m1x6
+#define vlxseg6e_v_u16m1x6	vloxseg6ei16_v_u16m1x6
+#define vlxseg6e_v_i16m1x6_m	vloxseg6ei16_v_i16m1x6_m
+#define vlxseg6e_v_u16m1x6_m	vloxseg6ei16_v_u16m1x6_m
+#define vlxseg7e_v_i16m1x7	vloxseg7ei16_v_i16m1x7
+#define vlxseg7e_v_u16m1x7	vloxseg7ei16_v_u16m1x7
+#define vlxseg7e_v_i16m1x7_m	vloxseg7ei16_v_i16m1x7_m
+#define vlxseg7e_v_u16m1x7_m	vloxseg7ei16_v_u16m1x7_m
+#define vlxseg8e_v_i16m1x8	vloxseg8ei16_v_i16m1x8
+#define vlxseg8e_v_u16m1x8	vloxseg8ei16_v_u16m1x8
+#define vlxseg8e_v_i16m1x8_m	vloxseg8ei16_v_i16m1x8_m
+#define vlxseg8e_v_u16m1x8_m	vloxseg8ei16_v_u16m1x8_m
+#define vlxseg2e_v_i16m2x2	vloxseg2ei16_v_i16m2x2
+#define vlxseg2e_v_u16m2x2	vloxseg2ei16_v_u16m2x2
+#define vlxseg2e_v_i16m2x2_m	vloxseg2ei16_v_i16m2x2_m
+#define vlxseg2e_v_u16m2x2_m	vloxseg2ei16_v_u16m2x2_m
+#define vlxseg3e_v_i16m2x3	vloxseg3ei16_v_i16m2x3
+#define vlxseg3e_v_u16m2x3	vloxseg3ei16_v_u16m2x3
+#define vlxseg3e_v_i16m2x3_m	vloxseg3ei16_v_i16m2x3_m
+#define vlxseg3e_v_u16m2x3_m	vloxseg3ei16_v_u16m2x3_m
+#define vlxseg4e_v_i16m2x4	vloxseg4ei16_v_i16m2x4
+#define vlxseg4e_v_u16m2x4	vloxseg4ei16_v_u16m2x4
+#define vlxseg4e_v_i16m2x4_m	vloxseg4ei16_v_i16m2x4_m
+#define vlxseg4e_v_u16m2x4_m	vloxseg4ei16_v_u16m2x4_m
+#define vlxseg2e_v_i16m4x2	vloxseg2ei16_v_i16m4x2
+#define vlxseg2e_v_u16m4x2	vloxseg2ei16_v_u16m4x2
+#define vlxseg2e_v_i16m4x2_m	vloxseg2ei16_v_i16m4x2_m
+#define vlxseg2e_v_u16m4x2_m	vloxseg2ei16_v_u16m4x2_m
+#define vlxseg2e_v_i32m1x2	vloxseg2ei32_v_i32m1x2
+#define vlxseg2e_v_u32m1x2	vloxseg2ei32_v_u32m1x2
+#define vlxseg2e_v_i32m1x2_m	vloxseg2ei32_v_i32m1x2_m
+#define vlxseg2e_v_u32m1x2_m	vloxseg2ei32_v_u32m1x2_m
+#define vlxseg3e_v_i32m1x3	vloxseg3ei32_v_i32m1x3
+#define vlxseg3e_v_u32m1x3	vloxseg3ei32_v_u32m1x3
+#define vlxseg3e_v_i32m1x3_m	vloxseg3ei32_v_i32m1x3_m
+#define vlxseg3e_v_u32m1x3_m	vloxseg3ei32_v_u32m1x3_m
+#define vlxseg4e_v_i32m1x4	vloxseg4ei32_v_i32m1x4
+#define vlxseg4e_v_u32m1x4	vloxseg4ei32_v_u32m1x4
+#define vlxseg4e_v_i32m1x4_m	vloxseg4ei32_v_i32m1x4_m
+#define vlxseg4e_v_u32m1x4_m	vloxseg4ei32_v_u32m1x4_m
+#define vlxseg5e_v_i32m1x5	vloxseg5ei32_v_i32m1x5
+#define vlxseg5e_v_u32m1x5	vloxseg5ei32_v_u32m1x5
+#define vlxseg5e_v_i32m1x5_m	vloxseg5ei32_v_i32m1x5_m
+#define vlxseg5e_v_u32m1x5_m	vloxseg5ei32_v_u32m1x5_m
+#define vlxseg6e_v_i32m1x6	vloxseg6ei32_v_i32m1x6
+#define vlxseg6e_v_u32m1x6	vloxseg6ei32_v_u32m1x6
+#define vlxseg6e_v_i32m1x6_m	vloxseg6ei32_v_i32m1x6_m
+#define vlxseg6e_v_u32m1x6_m	vloxseg6ei32_v_u32m1x6_m
+#define vlxseg7e_v_i32m1x7	vloxseg7ei32_v_i32m1x7
+#define vlxseg7e_v_u32m1x7	vloxseg7ei32_v_u32m1x7
+#define vlxseg7e_v_i32m1x7_m	vloxseg7ei32_v_i32m1x7_m
+#define vlxseg7e_v_u32m1x7_m	vloxseg7ei32_v_u32m1x7_m
+#define vlxseg8e_v_i32m1x8	vloxseg8ei32_v_i32m1x8
+#define vlxseg8e_v_u32m1x8	vloxseg8ei32_v_u32m1x8
+#define vlxseg8e_v_i32m1x8_m	vloxseg8ei32_v_i32m1x8_m
+#define vlxseg8e_v_u32m1x8_m	vloxseg8ei32_v_u32m1x8_m
+#define vlxseg2e_v_i32m2x2	vloxseg2ei32_v_i32m2x2
+#define vlxseg2e_v_u32m2x2	vloxseg2ei32_v_u32m2x2
+#define vlxseg2e_v_i32m2x2_m	vloxseg2ei32_v_i32m2x2_m
+#define vlxseg2e_v_u32m2x2_m	vloxseg2ei32_v_u32m2x2_m
+#define vlxseg3e_v_i32m2x3	vloxseg3ei32_v_i32m2x3
+#define vlxseg3e_v_u32m2x3	vloxseg3ei32_v_u32m2x3
+#define vlxseg3e_v_i32m2x3_m	vloxseg3ei32_v_i32m2x3_m
+#define vlxseg3e_v_u32m2x3_m	vloxseg3ei32_v_u32m2x3_m
+#define vlxseg4e_v_i32m2x4	vloxseg4ei32_v_i32m2x4
+#define vlxseg4e_v_u32m2x4	vloxseg4ei32_v_u32m2x4
+#define vlxseg4e_v_i32m2x4_m	vloxseg4ei32_v_i32m2x4_m
+#define vlxseg4e_v_u32m2x4_m	vloxseg4ei32_v_u32m2x4_m
+#define vlxseg2e_v_i32m4x2	vloxseg2ei32_v_i32m4x2
+#define vlxseg2e_v_u32m4x2	vloxseg2ei32_v_u32m4x2
+#define vlxseg2e_v_i32m4x2_m	vloxseg2ei32_v_i32m4x2_m
+#define vlxseg2e_v_u32m4x2_m	vloxseg2ei32_v_u32m4x2_m
+#define vlxseg2e_v_i64m1x2	vloxseg2ei64_v_i64m1x2
+#define vlxseg2e_v_u64m1x2	vloxseg2ei64_v_u64m1x2
+#define vlxseg2e_v_i64m1x2_m	vloxseg2ei64_v_i64m1x2_m
+#define vlxseg2e_v_u64m1x2_m	vloxseg2ei64_v_u64m1x2_m
+#define vlxseg3e_v_i64m1x3	vloxseg3ei64_v_i64m1x3
+#define vlxseg3e_v_u64m1x3	vloxseg3ei64_v_u64m1x3
+#define vlxseg3e_v_i64m1x3_m	vloxseg3ei64_v_i64m1x3_m
+#define vlxseg3e_v_u64m1x3_m	vloxseg3ei64_v_u64m1x3_m
+#define vlxseg4e_v_i64m1x4	vloxseg4ei64_v_i64m1x4
+#define vlxseg4e_v_u64m1x4	vloxseg4ei64_v_u64m1x4
+#define vlxseg4e_v_i64m1x4_m	vloxseg4ei64_v_i64m1x4_m
+#define vlxseg4e_v_u64m1x4_m	vloxseg4ei64_v_u64m1x4_m
+#define vlxseg5e_v_i64m1x5	vloxseg5ei64_v_i64m1x5
+#define vlxseg5e_v_u64m1x5	vloxseg5ei64_v_u64m1x5
+#define vlxseg5e_v_i64m1x5_m	vloxseg5ei64_v_i64m1x5_m
+#define vlxseg5e_v_u64m1x5_m	vloxseg5ei64_v_u64m1x5_m
+#define vlxseg6e_v_i64m1x6	vloxseg6ei64_v_i64m1x6
+#define vlxseg6e_v_u64m1x6	vloxseg6ei64_v_u64m1x6
+#define vlxseg6e_v_i64m1x6_m	vloxseg6ei64_v_i64m1x6_m
+#define vlxseg6e_v_u64m1x6_m	vloxseg6ei64_v_u64m1x6_m
+#define vlxseg7e_v_i64m1x7	vloxseg7ei64_v_i64m1x7
+#define vlxseg7e_v_u64m1x7	vloxseg7ei64_v_u64m1x7
+#define vlxseg7e_v_i64m1x7_m	vloxseg7ei64_v_i64m1x7_m
+#define vlxseg7e_v_u64m1x7_m	vloxseg7ei64_v_u64m1x7_m
+#define vlxseg8e_v_i64m1x8	vloxseg8ei64_v_i64m1x8
+#define vlxseg8e_v_u64m1x8	vloxseg8ei64_v_u64m1x8
+#define vlxseg8e_v_i64m1x8_m	vloxseg8ei64_v_i64m1x8_m
+#define vlxseg8e_v_u64m1x8_m	vloxseg8ei64_v_u64m1x8_m
+#define vlxseg2e_v_i64m2x2	vloxseg2ei64_v_i64m2x2
+#define vlxseg2e_v_u64m2x2	vloxseg2ei64_v_u64m2x2
+#define vlxseg2e_v_i64m2x2_m	vloxseg2ei64_v_i64m2x2_m
+#define vlxseg2e_v_u64m2x2_m	vloxseg2ei64_v_u64m2x2_m
+#define vlxseg3e_v_i64m2x3	vloxseg3ei64_v_i64m2x3
+#define vlxseg3e_v_u64m2x3	vloxseg3ei64_v_u64m2x3
+#define vlxseg3e_v_i64m2x3_m	vloxseg3ei64_v_i64m2x3_m
+#define vlxseg3e_v_u64m2x3_m	vloxseg3ei64_v_u64m2x3_m
+#define vlxseg4e_v_i64m2x4	vloxseg4ei64_v_i64m2x4
+#define vlxseg4e_v_u64m2x4	vloxseg4ei64_v_u64m2x4
+#define vlxseg4e_v_i64m2x4_m	vloxseg4ei64_v_i64m2x4_m
+#define vlxseg4e_v_u64m2x4_m	vloxseg4ei64_v_u64m2x4_m
+#define vlxseg2e_v_i64m4x2	vloxseg2ei64_v_i64m4x2
+#define vlxseg2e_v_u64m4x2	vloxseg2ei64_v_u64m4x2
+#define vlxseg2e_v_i64m4x2_m	vloxseg2ei64_v_i64m4x2_m
+#define vlxseg2e_v_u64m4x2_m	vloxseg2ei64_v_u64m4x2_m
+#define vlxseg2e_v_f16m1x2	vloxseg2ei16_v_f16m1x2
+#define vlxseg2e_v_f16m1x2_m	vloxseg2ei16_v_f16m1x2_m
+#define vlxseg3e_v_f16m1x3	vloxseg3ei16_v_f16m1x3
+#define vlxseg3e_v_f16m1x3_m	vloxseg3ei16_v_f16m1x3_m
+#define vlxseg4e_v_f16m1x4	vloxseg4ei16_v_f16m1x4
+#define vlxseg4e_v_f16m1x4_m	vloxseg4ei16_v_f16m1x4_m
+#define vlxseg5e_v_f16m1x5	vloxseg5ei16_v_f16m1x5
+#define vlxseg5e_v_f16m1x5_m	vloxseg5ei16_v_f16m1x5_m
+#define vlxseg6e_v_f16m1x6	vloxseg6ei16_v_f16m1x6
+#define vlxseg6e_v_f16m1x6_m	vloxseg6ei16_v_f16m1x6_m
+#define vlxseg7e_v_f16m1x7	vloxseg7ei16_v_f16m1x7
+#define vlxseg7e_v_f16m1x7_m	vloxseg7ei16_v_f16m1x7_m
+#define vlxseg8e_v_f16m1x8	vloxseg8ei16_v_f16m1x8
+#define vlxseg8e_v_f16m1x8_m	vloxseg8ei16_v_f16m1x8_m
+#define vlxseg2e_v_f16m2x2	vloxseg2ei16_v_f16m2x2
+#define vlxseg2e_v_f16m2x2_m	vloxseg2ei16_v_f16m2x2_m
+#define vlxseg3e_v_f16m2x3	vloxseg3ei16_v_f16m2x3
+#define vlxseg3e_v_f16m2x3_m	vloxseg3ei16_v_f16m2x3_m
+#define vlxseg4e_v_f16m2x4	vloxseg4ei16_v_f16m2x4
+#define vlxseg4e_v_f16m2x4_m	vloxseg4ei16_v_f16m2x4_m
+#define vlxseg2e_v_f16m4x2	vloxseg2ei16_v_f16m4x2
+#define vlxseg2e_v_f16m4x2_m	vloxseg2ei16_v_f16m4x2_m
+#define vlxseg2e_v_f32m1x2	vloxseg2ei32_v_f32m1x2
+#define vlxseg2e_v_f32m1x2_m	vloxseg2ei32_v_f32m1x2_m
+#define vlxseg3e_v_f32m1x3	vloxseg3ei32_v_f32m1x3
+#define vlxseg3e_v_f32m1x3_m	vloxseg3ei32_v_f32m1x3_m
+#define vlxseg4e_v_f32m1x4	vloxseg4ei32_v_f32m1x4
+#define vlxseg4e_v_f32m1x4_m	vloxseg4ei32_v_f32m1x4_m
+#define vlxseg5e_v_f32m1x5	vloxseg5ei32_v_f32m1x5
+#define vlxseg5e_v_f32m1x5_m	vloxseg5ei32_v_f32m1x5_m
+#define vlxseg6e_v_f32m1x6	vloxseg6ei32_v_f32m1x6
+#define vlxseg6e_v_f32m1x6_m	vloxseg6ei32_v_f32m1x6_m
+#define vlxseg7e_v_f32m1x7	vloxseg7ei32_v_f32m1x7
+#define vlxseg7e_v_f32m1x7_m	vloxseg7ei32_v_f32m1x7_m
+#define vlxseg8e_v_f32m1x8	vloxseg8ei32_v_f32m1x8
+#define vlxseg8e_v_f32m1x8_m	vloxseg8ei32_v_f32m1x8_m
+#define vlxseg2e_v_f32m2x2	vloxseg2ei32_v_f32m2x2
+#define vlxseg2e_v_f32m2x2_m	vloxseg2ei32_v_f32m2x2_m
+#define vlxseg3e_v_f32m2x3	vloxseg3ei32_v_f32m2x3
+#define vlxseg3e_v_f32m2x3_m	vloxseg3ei32_v_f32m2x3_m
+#define vlxseg4e_v_f32m2x4	vloxseg4ei32_v_f32m2x4
+#define vlxseg4e_v_f32m2x4_m	vloxseg4ei32_v_f32m2x4_m
+#define vlxseg2e_v_f32m4x2	vloxseg2ei32_v_f32m4x2
+#define vlxseg2e_v_f32m4x2_m	vloxseg2ei32_v_f32m4x2_m
+#define vlxseg2e_v_f64m1x2	vloxseg2ei64_v_f64m1x2
+#define vlxseg2e_v_f64m1x2_m	vloxseg2ei64_v_f64m1x2_m
+#define vlxseg3e_v_f64m1x3	vloxseg3ei64_v_f64m1x3
+#define vlxseg3e_v_f64m1x3_m	vloxseg3ei64_v_f64m1x3_m
+#define vlxseg4e_v_f64m1x4	vloxseg4ei64_v_f64m1x4
+#define vlxseg4e_v_f64m1x4_m	vloxseg4ei64_v_f64m1x4_m
+#define vlxseg5e_v_f64m1x5	vloxseg5ei64_v_f64m1x5
+#define vlxseg5e_v_f64m1x5_m	vloxseg5ei64_v_f64m1x5_m
+#define vlxseg6e_v_f64m1x6	vloxseg6ei64_v_f64m1x6
+#define vlxseg6e_v_f64m1x6_m	vloxseg6ei64_v_f64m1x6_m
+#define vlxseg7e_v_f64m1x7	vloxseg7ei64_v_f64m1x7
+#define vlxseg7e_v_f64m1x7_m	vloxseg7ei64_v_f64m1x7_m
+#define vlxseg8e_v_f64m1x8	vloxseg8ei64_v_f64m1x8
+#define vlxseg8e_v_f64m1x8_m	vloxseg8ei64_v_f64m1x8_m
+#define vlxseg2e_v_f64m2x2	vloxseg2ei64_v_f64m2x2
+#define vlxseg2e_v_f64m2x2_m	vloxseg2ei64_v_f64m2x2_m
+#define vlxseg3e_v_f64m2x3	vloxseg3ei64_v_f64m2x3
+#define vlxseg3e_v_f64m2x3_m	vloxseg3ei64_v_f64m2x3_m
+#define vlxseg4e_v_f64m2x4	vloxseg4ei64_v_f64m2x4
+#define vlxseg4e_v_f64m2x4_m	vloxseg4ei64_v_f64m2x4_m
+#define vlxseg2e_v_f64m4x2	vloxseg2ei64_v_f64m4x2
+#define vlxseg2e_v_f64m4x2_m	vloxseg2ei64_v_f64m4x2_m
+
+/* Wrapper only.  */
+#define vsxseg2e_v_i8m1x2	vsoxseg2ei8_v_i8m1x2
+#define vsxseg2e_v_u8m1x2	vsoxseg2ei8_v_u8m1x2
+#define vsxseg2e_v_i8m1x2_m	vsoxseg2ei8_v_i8m1x2_m
+#define vsxseg2e_v_u8m1x2_m	vsoxseg2ei8_v_u8m1x2_m
+#define vsxseg3e_v_i8m1x3	vsoxseg3ei8_v_i8m1x3
+#define vsxseg3e_v_u8m1x3	vsoxseg3ei8_v_u8m1x3
+#define vsxseg3e_v_i8m1x3_m	vsoxseg3ei8_v_i8m1x3_m
+#define vsxseg3e_v_u8m1x3_m	vsoxseg3ei8_v_u8m1x3_m
+#define vsxseg4e_v_i8m1x4	vsoxseg4ei8_v_i8m1x4
+#define vsxseg4e_v_u8m1x4	vsoxseg4ei8_v_u8m1x4
+#define vsxseg4e_v_i8m1x4_m	vsoxseg4ei8_v_i8m1x4_m
+#define vsxseg4e_v_u8m1x4_m	vsoxseg4ei8_v_u8m1x4_m
+#define vsxseg5e_v_i8m1x5	vsoxseg5ei8_v_i8m1x5
+#define vsxseg5e_v_u8m1x5	vsoxseg5ei8_v_u8m1x5
+#define vsxseg5e_v_i8m1x5_m	vsoxseg5ei8_v_i8m1x5_m
+#define vsxseg5e_v_u8m1x5_m	vsoxseg5ei8_v_u8m1x5_m
+#define vsxseg6e_v_i8m1x6	vsoxseg6ei8_v_i8m1x6
+#define vsxseg6e_v_u8m1x6	vsoxseg6ei8_v_u8m1x6
+#define vsxseg6e_v_i8m1x6_m	vsoxseg6ei8_v_i8m1x6_m
+#define vsxseg6e_v_u8m1x6_m	vsoxseg6ei8_v_u8m1x6_m
+#define vsxseg7e_v_i8m1x7	vsoxseg7ei8_v_i8m1x7
+#define vsxseg7e_v_u8m1x7	vsoxseg7ei8_v_u8m1x7
+#define vsxseg7e_v_i8m1x7_m	vsoxseg7ei8_v_i8m1x7_m
+#define vsxseg7e_v_u8m1x7_m	vsoxseg7ei8_v_u8m1x7_m
+#define vsxseg8e_v_i8m1x8	vsoxseg8ei8_v_i8m1x8
+#define vsxseg8e_v_u8m1x8	vsoxseg8ei8_v_u8m1x8
+#define vsxseg8e_v_i8m1x8_m	vsoxseg8ei8_v_i8m1x8_m
+#define vsxseg8e_v_u8m1x8_m	vsoxseg8ei8_v_u8m1x8_m
+#define vsxseg2e_v_i8m2x2	vsoxseg2ei8_v_i8m2x2
+#define vsxseg2e_v_u8m2x2	vsoxseg2ei8_v_u8m2x2
+#define vsxseg2e_v_i8m2x2_m	vsoxseg2ei8_v_i8m2x2_m
+#define vsxseg2e_v_u8m2x2_m	vsoxseg2ei8_v_u8m2x2_m
+#define vsxseg3e_v_i8m2x3	vsoxseg3ei8_v_i8m2x3
+#define vsxseg3e_v_u8m2x3	vsoxseg3ei8_v_u8m2x3
+#define vsxseg3e_v_i8m2x3_m	vsoxseg3ei8_v_i8m2x3_m
+#define vsxseg3e_v_u8m2x3_m	vsoxseg3ei8_v_u8m2x3_m
+#define vsxseg4e_v_i8m2x4	vsoxseg4ei8_v_i8m2x4
+#define vsxseg4e_v_u8m2x4	vsoxseg4ei8_v_u8m2x4
+#define vsxseg4e_v_i8m2x4_m	vsoxseg4ei8_v_i8m2x4_m
+#define vsxseg4e_v_u8m2x4_m	vsoxseg4ei8_v_u8m2x4_m
+#define vsxseg2e_v_i8m4x2	vsoxseg2ei8_v_i8m4x2
+#define vsxseg2e_v_u8m4x2	vsoxseg2ei8_v_u8m4x2
+#define vsxseg2e_v_i8m4x2_m	vsoxseg2ei8_v_i8m4x2_m
+#define vsxseg2e_v_u8m4x2_m	vsoxseg2ei8_v_u8m4x2_m
+#define vsxseg2e_v_i16m1x2	vsoxseg2ei16_v_i16m1x2
+#define vsxseg2e_v_u16m1x2	vsoxseg2ei16_v_u16m1x2
+#define vsxseg2e_v_i16m1x2_m	vsoxseg2ei16_v_i16m1x2_m
+#define vsxseg2e_v_u16m1x2_m	vsoxseg2ei16_v_u16m1x2_m
+#define vsxseg3e_v_i16m1x3	vsoxseg3ei16_v_i16m1x3
+#define vsxseg3e_v_u16m1x3	vsoxseg3ei16_v_u16m1x3
+#define vsxseg3e_v_i16m1x3_m	vsoxseg3ei16_v_i16m1x3_m
+#define vsxseg3e_v_u16m1x3_m	vsoxseg3ei16_v_u16m1x3_m
+#define vsxseg4e_v_i16m1x4	vsoxseg4ei16_v_i16m1x4
+#define vsxseg4e_v_u16m1x4	vsoxseg4ei16_v_u16m1x4
+#define vsxseg4e_v_i16m1x4_m	vsoxseg4ei16_v_i16m1x4_m
+#define vsxseg4e_v_u16m1x4_m	vsoxseg4ei16_v_u16m1x4_m
+#define vsxseg5e_v_i16m1x5	vsoxseg5ei16_v_i16m1x5
+#define vsxseg5e_v_u16m1x5	vsoxseg5ei16_v_u16m1x5
+#define vsxseg5e_v_i16m1x5_m	vsoxseg5ei16_v_i16m1x5_m
+#define vsxseg5e_v_u16m1x5_m	vsoxseg5ei16_v_u16m1x5_m
+#define vsxseg6e_v_i16m1x6	vsoxseg6ei16_v_i16m1x6
+#define vsxseg6e_v_u16m1x6	vsoxseg6ei16_v_u16m1x6
+#define vsxseg6e_v_i16m1x6_m	vsoxseg6ei16_v_i16m1x6_m
+#define vsxseg6e_v_u16m1x6_m	vsoxseg6ei16_v_u16m1x6_m
+#define vsxseg7e_v_i16m1x7	vsoxseg7ei16_v_i16m1x7
+#define vsxseg7e_v_u16m1x7	vsoxseg7ei16_v_u16m1x7
+#define vsxseg7e_v_i16m1x7_m	vsoxseg7ei16_v_i16m1x7_m
+#define vsxseg7e_v_u16m1x7_m	vsoxseg7ei16_v_u16m1x7_m
+#define vsxseg8e_v_i16m1x8	vsoxseg8ei16_v_i16m1x8
+#define vsxseg8e_v_u16m1x8	vsoxseg8ei16_v_u16m1x8
+#define vsxseg8e_v_i16m1x8_m	vsoxseg8ei16_v_i16m1x8_m
+#define vsxseg8e_v_u16m1x8_m	vsoxseg8ei16_v_u16m1x8_m
+#define vsxseg2e_v_i16m2x2	vsoxseg2ei16_v_i16m2x2
+#define vsxseg2e_v_u16m2x2	vsoxseg2ei16_v_u16m2x2
+#define vsxseg2e_v_i16m2x2_m	vsoxseg2ei16_v_i16m2x2_m
+#define vsxseg2e_v_u16m2x2_m	vsoxseg2ei16_v_u16m2x2_m
+#define vsxseg3e_v_i16m2x3	vsoxseg3ei16_v_i16m2x3
+#define vsxseg3e_v_u16m2x3	vsoxseg3ei16_v_u16m2x3
+#define vsxseg3e_v_i16m2x3_m	vsoxseg3ei16_v_i16m2x3_m
+#define vsxseg3e_v_u16m2x3_m	vsoxseg3ei16_v_u16m2x3_m
+#define vsxseg4e_v_i16m2x4	vsoxseg4ei16_v_i16m2x4
+#define vsxseg4e_v_u16m2x4	vsoxseg4ei16_v_u16m2x4
+#define vsxseg4e_v_i16m2x4_m	vsoxseg4ei16_v_i16m2x4_m
+#define vsxseg4e_v_u16m2x4_m	vsoxseg4ei16_v_u16m2x4_m
+#define vsxseg2e_v_i16m4x2	vsoxseg2ei16_v_i16m4x2
+#define vsxseg2e_v_u16m4x2	vsoxseg2ei16_v_u16m4x2
+#define vsxseg2e_v_i16m4x2_m	vsoxseg2ei16_v_i16m4x2_m
+#define vsxseg2e_v_u16m4x2_m	vsoxseg2ei16_v_u16m4x2_m
+#define vsxseg2e_v_i32m1x2	vsoxseg2ei32_v_i32m1x2
+#define vsxseg2e_v_u32m1x2	vsoxseg2ei32_v_u32m1x2
+#define vsxseg2e_v_i32m1x2_m	vsoxseg2ei32_v_i32m1x2_m
+#define vsxseg2e_v_u32m1x2_m	vsoxseg2ei32_v_u32m1x2_m
+#define vsxseg3e_v_i32m1x3	vsoxseg3ei32_v_i32m1x3
+#define vsxseg3e_v_u32m1x3	vsoxseg3ei32_v_u32m1x3
+#define vsxseg3e_v_i32m1x3_m	vsoxseg3ei32_v_i32m1x3_m
+#define vsxseg3e_v_u32m1x3_m	vsoxseg3ei32_v_u32m1x3_m
+#define vsxseg4e_v_i32m1x4	vsoxseg4ei32_v_i32m1x4
+#define vsxseg4e_v_u32m1x4	vsoxseg4ei32_v_u32m1x4
+#define vsxseg4e_v_i32m1x4_m	vsoxseg4ei32_v_i32m1x4_m
+#define vsxseg4e_v_u32m1x4_m	vsoxseg4ei32_v_u32m1x4_m
+#define vsxseg5e_v_i32m1x5	vsoxseg5ei32_v_i32m1x5
+#define vsxseg5e_v_u32m1x5	vsoxseg5ei32_v_u32m1x5
+#define vsxseg5e_v_i32m1x5_m	vsoxseg5ei32_v_i32m1x5_m
+#define vsxseg5e_v_u32m1x5_m	vsoxseg5ei32_v_u32m1x5_m
+#define vsxseg6e_v_i32m1x6	vsoxseg6ei32_v_i32m1x6
+#define vsxseg6e_v_u32m1x6	vsoxseg6ei32_v_u32m1x6
+#define vsxseg6e_v_i32m1x6_m	vsoxseg6ei32_v_i32m1x6_m
+#define vsxseg6e_v_u32m1x6_m	vsoxseg6ei32_v_u32m1x6_m
+#define vsxseg7e_v_i32m1x7	vsoxseg7ei32_v_i32m1x7
+#define vsxseg7e_v_u32m1x7	vsoxseg7ei32_v_u32m1x7
+#define vsxseg7e_v_i32m1x7_m	vsoxseg7ei32_v_i32m1x7_m
+#define vsxseg7e_v_u32m1x7_m	vsoxseg7ei32_v_u32m1x7_m
+#define vsxseg8e_v_i32m1x8	vsoxseg8ei32_v_i32m1x8
+#define vsxseg8e_v_u32m1x8	vsoxseg8ei32_v_u32m1x8
+#define vsxseg8e_v_i32m1x8_m	vsoxseg8ei32_v_i32m1x8_m
+#define vsxseg8e_v_u32m1x8_m	vsoxseg8ei32_v_u32m1x8_m
+#define vsxseg2e_v_i32m2x2	vsoxseg2ei32_v_i32m2x2
+#define vsxseg2e_v_u32m2x2	vsoxseg2ei32_v_u32m2x2
+#define vsxseg2e_v_i32m2x2_m	vsoxseg2ei32_v_i32m2x2_m
+#define vsxseg2e_v_u32m2x2_m	vsoxseg2ei32_v_u32m2x2_m
+#define vsxseg3e_v_i32m2x3	vsoxseg3ei32_v_i32m2x3
+#define vsxseg3e_v_u32m2x3	vsoxseg3ei32_v_u32m2x3
+#define vsxseg3e_v_i32m2x3_m	vsoxseg3ei32_v_i32m2x3_m
+#define vsxseg3e_v_u32m2x3_m	vsoxseg3ei32_v_u32m2x3_m
+#define vsxseg4e_v_i32m2x4	vsoxseg4ei32_v_i32m2x4
+#define vsxseg4e_v_u32m2x4	vsoxseg4ei32_v_u32m2x4
+#define vsxseg4e_v_i32m2x4_m	vsoxseg4ei32_v_i32m2x4_m
+#define vsxseg4e_v_u32m2x4_m	vsoxseg4ei32_v_u32m2x4_m
+#define vsxseg2e_v_i32m4x2	vsoxseg2ei32_v_i32m4x2
+#define vsxseg2e_v_u32m4x2	vsoxseg2ei32_v_u32m4x2
+#define vsxseg2e_v_i32m4x2_m	vsoxseg2ei32_v_i32m4x2_m
+#define vsxseg2e_v_u32m4x2_m	vsoxseg2ei32_v_u32m4x2_m
+#define vsxseg2e_v_i64m1x2	vsoxseg2ei64_v_i64m1x2
+#define vsxseg2e_v_u64m1x2	vsoxseg2ei64_v_u64m1x2
+#define vsxseg2e_v_i64m1x2_m	vsoxseg2ei64_v_i64m1x2_m
+#define vsxseg2e_v_u64m1x2_m	vsoxseg2ei64_v_u64m1x2_m
+#define vsxseg3e_v_i64m1x3	vsoxseg3ei64_v_i64m1x3
+#define vsxseg3e_v_u64m1x3	vsoxseg3ei64_v_u64m1x3
+#define vsxseg3e_v_i64m1x3_m	vsoxseg3ei64_v_i64m1x3_m
+#define vsxseg3e_v_u64m1x3_m	vsoxseg3ei64_v_u64m1x3_m
+#define vsxseg4e_v_i64m1x4	vsoxseg4ei64_v_i64m1x4
+#define vsxseg4e_v_u64m1x4	vsoxseg4ei64_v_u64m1x4
+#define vsxseg4e_v_i64m1x4_m	vsoxseg4ei64_v_i64m1x4_m
+#define vsxseg4e_v_u64m1x4_m	vsoxseg4ei64_v_u64m1x4_m
+#define vsxseg5e_v_i64m1x5	vsoxseg5ei64_v_i64m1x5
+#define vsxseg5e_v_u64m1x5	vsoxseg5ei64_v_u64m1x5
+#define vsxseg5e_v_i64m1x5_m	vsoxseg5ei64_v_i64m1x5_m
+#define vsxseg5e_v_u64m1x5_m	vsoxseg5ei64_v_u64m1x5_m
+#define vsxseg6e_v_i64m1x6	vsoxseg6ei64_v_i64m1x6
+#define vsxseg6e_v_u64m1x6	vsoxseg6ei64_v_u64m1x6
+#define vsxseg6e_v_i64m1x6_m	vsoxseg6ei64_v_i64m1x6_m
+#define vsxseg6e_v_u64m1x6_m	vsoxseg6ei64_v_u64m1x6_m
+#define vsxseg7e_v_i64m1x7	vsoxseg7ei64_v_i64m1x7
+#define vsxseg7e_v_u64m1x7	vsoxseg7ei64_v_u64m1x7
+#define vsxseg7e_v_i64m1x7_m	vsoxseg7ei64_v_i64m1x7_m
+#define vsxseg7e_v_u64m1x7_m	vsoxseg7ei64_v_u64m1x7_m
+#define vsxseg8e_v_i64m1x8	vsoxseg8ei64_v_i64m1x8
+#define vsxseg8e_v_u64m1x8	vsoxseg8ei64_v_u64m1x8
+#define vsxseg8e_v_i64m1x8_m	vsoxseg8ei64_v_i64m1x8_m
+#define vsxseg8e_v_u64m1x8_m	vsoxseg8ei64_v_u64m1x8_m
+#define vsxseg2e_v_i64m2x2	vsoxseg2ei64_v_i64m2x2
+#define vsxseg2e_v_u64m2x2	vsoxseg2ei64_v_u64m2x2
+#define vsxseg2e_v_i64m2x2_m	vsoxseg2ei64_v_i64m2x2_m
+#define vsxseg2e_v_u64m2x2_m	vsoxseg2ei64_v_u64m2x2_m
+#define vsxseg3e_v_i64m2x3	vsoxseg3ei64_v_i64m2x3
+#define vsxseg3e_v_u64m2x3	vsoxseg3ei64_v_u64m2x3
+#define vsxseg3e_v_i64m2x3_m	vsoxseg3ei64_v_i64m2x3_m
+#define vsxseg3e_v_u64m2x3_m	vsoxseg3ei64_v_u64m2x3_m
+#define vsxseg4e_v_i64m2x4	vsoxseg4ei64_v_i64m2x4
+#define vsxseg4e_v_u64m2x4	vsoxseg4ei64_v_u64m2x4
+#define vsxseg4e_v_i64m2x4_m	vsoxseg4ei64_v_i64m2x4_m
+#define vsxseg4e_v_u64m2x4_m	vsoxseg4ei64_v_u64m2x4_m
+#define vsxseg2e_v_i64m4x2	vsoxseg2ei64_v_i64m4x2
+#define vsxseg2e_v_u64m4x2	vsoxseg2ei64_v_u64m4x2
+#define vsxseg2e_v_i64m4x2_m	vsoxseg2ei64_v_i64m4x2_m
+#define vsxseg2e_v_u64m4x2_m	vsoxseg2ei64_v_u64m4x2_m
+#define vsxseg2e_v_f16m1x2	vsoxseg2ei16_v_f16m1x2
+#define vsxseg2e_v_f16m1x2_m	vsoxseg2ei16_v_f16m1x2_m
+#define vsxseg3e_v_f16m1x3	vsoxseg3ei16_v_f16m1x3
+#define vsxseg3e_v_f16m1x3_m	vsoxseg3ei16_v_f16m1x3_m
+#define vsxseg4e_v_f16m1x4	vsoxseg4ei16_v_f16m1x4
+#define vsxseg4e_v_f16m1x4_m	vsoxseg4ei16_v_f16m1x4_m
+#define vsxseg5e_v_f16m1x5	vsoxseg5ei16_v_f16m1x5
+#define vsxseg5e_v_f16m1x5_m	vsoxseg5ei16_v_f16m1x5_m
+#define vsxseg6e_v_f16m1x6	vsoxseg6ei16_v_f16m1x6
+#define vsxseg6e_v_f16m1x6_m	vsoxseg6ei16_v_f16m1x6_m
+#define vsxseg7e_v_f16m1x7	vsoxseg7ei16_v_f16m1x7
+#define vsxseg7e_v_f16m1x7_m	vsoxseg7ei16_v_f16m1x7_m
+#define vsxseg8e_v_f16m1x8	vsoxseg8ei16_v_f16m1x8
+#define vsxseg8e_v_f16m1x8_m	vsoxseg8ei16_v_f16m1x8_m
+#define vsxseg2e_v_f16m2x2	vsoxseg2ei16_v_f16m2x2
+#define vsxseg2e_v_f16m2x2_m	vsoxseg2ei16_v_f16m2x2_m
+#define vsxseg3e_v_f16m2x3	vsoxseg3ei16_v_f16m2x3
+#define vsxseg3e_v_f16m2x3_m	vsoxseg3ei16_v_f16m2x3_m
+#define vsxseg4e_v_f16m2x4	vsoxseg4ei16_v_f16m2x4
+#define vsxseg4e_v_f16m2x4_m	vsoxseg4ei16_v_f16m2x4_m
+#define vsxseg2e_v_f16m4x2	vsoxseg2ei16_v_f16m4x2
+#define vsxseg2e_v_f16m4x2_m	vsoxseg2ei16_v_f16m4x2_m
+#define vsxseg2e_v_f32m1x2	vsoxseg2ei32_v_f32m1x2
+#define vsxseg2e_v_f32m1x2_m	vsoxseg2ei32_v_f32m1x2_m
+#define vsxseg3e_v_f32m1x3	vsoxseg3ei32_v_f32m1x3
+#define vsxseg3e_v_f32m1x3_m	vsoxseg3ei32_v_f32m1x3_m
+#define vsxseg4e_v_f32m1x4	vsoxseg4ei32_v_f32m1x4
+#define vsxseg4e_v_f32m1x4_m	vsoxseg4ei32_v_f32m1x4_m
+#define vsxseg5e_v_f32m1x5	vsoxseg5ei32_v_f32m1x5
+#define vsxseg5e_v_f32m1x5_m	vsoxseg5ei32_v_f32m1x5_m
+#define vsxseg6e_v_f32m1x6	vsoxseg6ei32_v_f32m1x6
+#define vsxseg6e_v_f32m1x6_m	vsoxseg6ei32_v_f32m1x6_m
+#define vsxseg7e_v_f32m1x7	vsoxseg7ei32_v_f32m1x7
+#define vsxseg7e_v_f32m1x7_m	vsoxseg7ei32_v_f32m1x7_m
+#define vsxseg8e_v_f32m1x8	vsoxseg8ei32_v_f32m1x8
+#define vsxseg8e_v_f32m1x8_m	vsoxseg8ei32_v_f32m1x8_m
+#define vsxseg2e_v_f32m2x2	vsoxseg2ei32_v_f32m2x2
+#define vsxseg2e_v_f32m2x2_m	vsoxseg2ei32_v_f32m2x2_m
+#define vsxseg3e_v_f32m2x3	vsoxseg3ei32_v_f32m2x3
+#define vsxseg3e_v_f32m2x3_m	vsoxseg3ei32_v_f32m2x3_m
+#define vsxseg4e_v_f32m2x4	vsoxseg4ei32_v_f32m2x4
+#define vsxseg4e_v_f32m2x4_m	vsoxseg4ei32_v_f32m2x4_m
+#define vsxseg2e_v_f32m4x2	vsoxseg2ei32_v_f32m4x2
+#define vsxseg2e_v_f32m4x2_m	vsoxseg2ei32_v_f32m4x2_m
+#define vsxseg2e_v_f64m1x2	vsoxseg2ei64_v_f64m1x2
+#define vsxseg2e_v_f64m1x2_m	vsoxseg2ei64_v_f64m1x2_m
+#define vsxseg3e_v_f64m1x3	vsoxseg3ei64_v_f64m1x3
+#define vsxseg3e_v_f64m1x3_m	vsoxseg3ei64_v_f64m1x3_m
+#define vsxseg4e_v_f64m1x4	vsoxseg4ei64_v_f64m1x4
+#define vsxseg4e_v_f64m1x4_m	vsoxseg4ei64_v_f64m1x4_m
+#define vsxseg5e_v_f64m1x5	vsoxseg5ei64_v_f64m1x5
+#define vsxseg5e_v_f64m1x5_m	vsoxseg5ei64_v_f64m1x5_m
+#define vsxseg6e_v_f64m1x6	vsoxseg6ei64_v_f64m1x6
+#define vsxseg6e_v_f64m1x6_m	vsoxseg6ei64_v_f64m1x6_m
+#define vsxseg7e_v_f64m1x7	vsoxseg7ei64_v_f64m1x7
+#define vsxseg7e_v_f64m1x7_m	vsoxseg7ei64_v_f64m1x7_m
+#define vsxseg8e_v_f64m1x8	vsoxseg8ei64_v_f64m1x8
+#define vsxseg8e_v_f64m1x8_m	vsoxseg8ei64_v_f64m1x8_m
+#define vsxseg2e_v_f64m2x2	vsoxseg2ei64_v_f64m2x2
+#define vsxseg2e_v_f64m2x2_m	vsoxseg2ei64_v_f64m2x2_m
+#define vsxseg3e_v_f64m2x3	vsoxseg3ei64_v_f64m2x3
+#define vsxseg3e_v_f64m2x3_m	vsoxseg3ei64_v_f64m2x3_m
+#define vsxseg4e_v_f64m2x4	vsoxseg4ei64_v_f64m2x4
+#define vsxseg4e_v_f64m2x4_m	vsoxseg4ei64_v_f64m2x4_m
+#define vsxseg2e_v_f64m4x2	vsoxseg2ei64_v_f64m4x2
+#define vsxseg2e_v_f64m4x2_m	vsoxseg2ei64_v_f64m4x2_m
+
+/* Wrapper only.  */
+#define vlseg2eff_v_i8m1x2	vlseg2e8ff_v_i8m1x2
+#define vlseg2eff_v_u8m1x2	vlseg2e8ff_v_u8m1x2
+#define vlseg2eff_v_i8m1x2_m	vlseg2e8ff_v_i8m1x2_m
+#define vlseg2eff_v_u8m1x2_m	vlseg2e8ff_v_u8m1x2_m
+#define vlseg3eff_v_i8m1x3	vlseg3e8ff_v_i8m1x3
+#define vlseg3eff_v_u8m1x3	vlseg3e8ff_v_u8m1x3
+#define vlseg3eff_v_i8m1x3_m	vlseg3e8ff_v_i8m1x3_m
+#define vlseg3eff_v_u8m1x3_m	vlseg3e8ff_v_u8m1x3_m
+#define vlseg4eff_v_i8m1x4	vlseg4e8ff_v_i8m1x4
+#define vlseg4eff_v_u8m1x4	vlseg4e8ff_v_u8m1x4
+#define vlseg4eff_v_i8m1x4_m	vlseg4e8ff_v_i8m1x4_m
+#define vlseg4eff_v_u8m1x4_m	vlseg4e8ff_v_u8m1x4_m
+#define vlseg5eff_v_i8m1x5	vlseg5e8ff_v_i8m1x5
+#define vlseg5eff_v_u8m1x5	vlseg5e8ff_v_u8m1x5
+#define vlseg5eff_v_i8m1x5_m	vlseg5e8ff_v_i8m1x5_m
+#define vlseg5eff_v_u8m1x5_m	vlseg5e8ff_v_u8m1x5_m
+#define vlseg6eff_v_i8m1x6	vlseg6e8ff_v_i8m1x6
+#define vlseg6eff_v_u8m1x6	vlseg6e8ff_v_u8m1x6
+#define vlseg6eff_v_i8m1x6_m	vlseg6e8ff_v_i8m1x6_m
+#define vlseg6eff_v_u8m1x6_m	vlseg6e8ff_v_u8m1x6_m
+#define vlseg7eff_v_i8m1x7	vlseg7e8ff_v_i8m1x7
+#define vlseg7eff_v_u8m1x7	vlseg7e8ff_v_u8m1x7
+#define vlseg7eff_v_i8m1x7_m	vlseg7e8ff_v_i8m1x7_m
+#define vlseg7eff_v_u8m1x7_m	vlseg7e8ff_v_u8m1x7_m
+#define vlseg8eff_v_i8m1x8	vlseg8e8ff_v_i8m1x8
+#define vlseg8eff_v_u8m1x8	vlseg8e8ff_v_u8m1x8
+#define vlseg8eff_v_i8m1x8_m	vlseg8e8ff_v_i8m1x8_m
+#define vlseg8eff_v_u8m1x8_m	vlseg8e8ff_v_u8m1x8_m
+#define vlseg2eff_v_i8m2x2	vlseg2e8ff_v_i8m2x2
+#define vlseg2eff_v_u8m2x2	vlseg2e8ff_v_u8m2x2
+#define vlseg2eff_v_i8m2x2_m	vlseg2e8ff_v_i8m2x2_m
+#define vlseg2eff_v_u8m2x2_m	vlseg2e8ff_v_u8m2x2_m
+#define vlseg3eff_v_i8m2x3	vlseg3e8ff_v_i8m2x3
+#define vlseg3eff_v_u8m2x3	vlseg3e8ff_v_u8m2x3
+#define vlseg3eff_v_i8m2x3_m	vlseg3e8ff_v_i8m2x3_m
+#define vlseg3eff_v_u8m2x3_m	vlseg3e8ff_v_u8m2x3_m
+#define vlseg4eff_v_i8m2x4	vlseg4e8ff_v_i8m2x4
+#define vlseg4eff_v_u8m2x4	vlseg4e8ff_v_u8m2x4
+#define vlseg4eff_v_i8m2x4_m	vlseg4e8ff_v_i8m2x4_m
+#define vlseg4eff_v_u8m2x4_m	vlseg4e8ff_v_u8m2x4_m
+#define vlseg2eff_v_i8m4x2	vlseg2e8ff_v_i8m4x2
+#define vlseg2eff_v_u8m4x2	vlseg2e8ff_v_u8m4x2
+#define vlseg2eff_v_i8m4x2_m	vlseg2e8ff_v_i8m4x2_m
+#define vlseg2eff_v_u8m4x2_m	vlseg2e8ff_v_u8m4x2_m
+#define vlseg2eff_v_i16m1x2	vlseg2e16ff_v_i16m1x2
+#define vlseg2eff_v_u16m1x2	vlseg2e16ff_v_u16m1x2
+#define vlseg2eff_v_i16m1x2_m	vlseg2e16ff_v_i16m1x2_m
+#define vlseg2eff_v_u16m1x2_m	vlseg2e16ff_v_u16m1x2_m
+#define vlseg3eff_v_i16m1x3	vlseg3e16ff_v_i16m1x3
+#define vlseg3eff_v_u16m1x3	vlseg3e16ff_v_u16m1x3
+#define vlseg3eff_v_i16m1x3_m	vlseg3e16ff_v_i16m1x3_m
+#define vlseg3eff_v_u16m1x3_m	vlseg3e16ff_v_u16m1x3_m
+#define vlseg4eff_v_i16m1x4	vlseg4e16ff_v_i16m1x4
+#define vlseg4eff_v_u16m1x4	vlseg4e16ff_v_u16m1x4
+#define vlseg4eff_v_i16m1x4_m	vlseg4e16ff_v_i16m1x4_m
+#define vlseg4eff_v_u16m1x4_m	vlseg4e16ff_v_u16m1x4_m
+#define vlseg5eff_v_i16m1x5	vlseg5e16ff_v_i16m1x5
+#define vlseg5eff_v_u16m1x5	vlseg5e16ff_v_u16m1x5
+#define vlseg5eff_v_i16m1x5_m	vlseg5e16ff_v_i16m1x5_m
+#define vlseg5eff_v_u16m1x5_m	vlseg5e16ff_v_u16m1x5_m
+#define vlseg6eff_v_i16m1x6	vlseg6e16ff_v_i16m1x6
+#define vlseg6eff_v_u16m1x6	vlseg6e16ff_v_u16m1x6
+#define vlseg6eff_v_i16m1x6_m	vlseg6e16ff_v_i16m1x6_m
+#define vlseg6eff_v_u16m1x6_m	vlseg6e16ff_v_u16m1x6_m
+#define vlseg7eff_v_i16m1x7	vlseg7e16ff_v_i16m1x7
+#define vlseg7eff_v_u16m1x7	vlseg7e16ff_v_u16m1x7
+#define vlseg7eff_v_i16m1x7_m	vlseg7e16ff_v_i16m1x7_m
+#define vlseg7eff_v_u16m1x7_m	vlseg7e16ff_v_u16m1x7_m
+#define vlseg8eff_v_i16m1x8	vlseg8e16ff_v_i16m1x8
+#define vlseg8eff_v_u16m1x8	vlseg8e16ff_v_u16m1x8
+#define vlseg8eff_v_i16m1x8_m	vlseg8e16ff_v_i16m1x8_m
+#define vlseg8eff_v_u16m1x8_m	vlseg8e16ff_v_u16m1x8_m
+#define vlseg2eff_v_i16m2x2	vlseg2e16ff_v_i16m2x2
+#define vlseg2eff_v_u16m2x2	vlseg2e16ff_v_u16m2x2
+#define vlseg2eff_v_i16m2x2_m	vlseg2e16ff_v_i16m2x2_m
+#define vlseg2eff_v_u16m2x2_m	vlseg2e16ff_v_u16m2x2_m
+#define vlseg3eff_v_i16m2x3	vlseg3e16ff_v_i16m2x3
+#define vlseg3eff_v_u16m2x3	vlseg3e16ff_v_u16m2x3
+#define vlseg3eff_v_i16m2x3_m	vlseg3e16ff_v_i16m2x3_m
+#define vlseg3eff_v_u16m2x3_m	vlseg3e16ff_v_u16m2x3_m
+#define vlseg4eff_v_i16m2x4	vlseg4e16ff_v_i16m2x4
+#define vlseg4eff_v_u16m2x4	vlseg4e16ff_v_u16m2x4
+#define vlseg4eff_v_i16m2x4_m	vlseg4e16ff_v_i16m2x4_m
+#define vlseg4eff_v_u16m2x4_m	vlseg4e16ff_v_u16m2x4_m
+#define vlseg2eff_v_i16m4x2	vlseg2e16ff_v_i16m4x2
+#define vlseg2eff_v_u16m4x2	vlseg2e16ff_v_u16m4x2
+#define vlseg2eff_v_i16m4x2_m	vlseg2e16ff_v_i16m4x2_m
+#define vlseg2eff_v_u16m4x2_m	vlseg2e16ff_v_u16m4x2_m
+#define vlseg2eff_v_i32m1x2	vlseg2e32ff_v_i32m1x2
+#define vlseg2eff_v_u32m1x2	vlseg2e32ff_v_u32m1x2
+#define vlseg2eff_v_i32m1x2_m	vlseg2e32ff_v_i32m1x2_m
+#define vlseg2eff_v_u32m1x2_m	vlseg2e32ff_v_u32m1x2_m
+#define vlseg3eff_v_i32m1x3	vlseg3e32ff_v_i32m1x3
+#define vlseg3eff_v_u32m1x3	vlseg3e32ff_v_u32m1x3
+#define vlseg3eff_v_i32m1x3_m	vlseg3e32ff_v_i32m1x3_m
+#define vlseg3eff_v_u32m1x3_m	vlseg3e32ff_v_u32m1x3_m
+#define vlseg4eff_v_i32m1x4	vlseg4e32ff_v_i32m1x4
+#define vlseg4eff_v_u32m1x4	vlseg4e32ff_v_u32m1x4
+#define vlseg4eff_v_i32m1x4_m	vlseg4e32ff_v_i32m1x4_m
+#define vlseg4eff_v_u32m1x4_m	vlseg4e32ff_v_u32m1x4_m
+#define vlseg5eff_v_i32m1x5	vlseg5e32ff_v_i32m1x5
+#define vlseg5eff_v_u32m1x5	vlseg5e32ff_v_u32m1x5
+#define vlseg5eff_v_i32m1x5_m	vlseg5e32ff_v_i32m1x5_m
+#define vlseg5eff_v_u32m1x5_m	vlseg5e32ff_v_u32m1x5_m
+#define vlseg6eff_v_i32m1x6	vlseg6e32ff_v_i32m1x6
+#define vlseg6eff_v_u32m1x6	vlseg6e32ff_v_u32m1x6
+#define vlseg6eff_v_i32m1x6_m	vlseg6e32ff_v_i32m1x6_m
+#define vlseg6eff_v_u32m1x6_m	vlseg6e32ff_v_u32m1x6_m
+#define vlseg7eff_v_i32m1x7	vlseg7e32ff_v_i32m1x7
+#define vlseg7eff_v_u32m1x7	vlseg7e32ff_v_u32m1x7
+#define vlseg7eff_v_i32m1x7_m	vlseg7e32ff_v_i32m1x7_m
+#define vlseg7eff_v_u32m1x7_m	vlseg7e32ff_v_u32m1x7_m
+#define vlseg8eff_v_i32m1x8	vlseg8e32ff_v_i32m1x8
+#define vlseg8eff_v_u32m1x8	vlseg8e32ff_v_u32m1x8
+#define vlseg8eff_v_i32m1x8_m	vlseg8e32ff_v_i32m1x8_m
+#define vlseg8eff_v_u32m1x8_m	vlseg8e32ff_v_u32m1x8_m
+#define vlseg2eff_v_i32m2x2	vlseg2e32ff_v_i32m2x2
+#define vlseg2eff_v_u32m2x2	vlseg2e32ff_v_u32m2x2
+#define vlseg2eff_v_i32m2x2_m	vlseg2e32ff_v_i32m2x2_m
+#define vlseg2eff_v_u32m2x2_m	vlseg2e32ff_v_u32m2x2_m
+#define vlseg3eff_v_i32m2x3	vlseg3e32ff_v_i32m2x3
+#define vlseg3eff_v_u32m2x3	vlseg3e32ff_v_u32m2x3
+#define vlseg3eff_v_i32m2x3_m	vlseg3e32ff_v_i32m2x3_m
+#define vlseg3eff_v_u32m2x3_m	vlseg3e32ff_v_u32m2x3_m
+#define vlseg4eff_v_i32m2x4	vlseg4e32ff_v_i32m2x4
+#define vlseg4eff_v_u32m2x4	vlseg4e32ff_v_u32m2x4
+#define vlseg4eff_v_i32m2x4_m	vlseg4e32ff_v_i32m2x4_m
+#define vlseg4eff_v_u32m2x4_m	vlseg4e32ff_v_u32m2x4_m
+#define vlseg2eff_v_i32m4x2	vlseg2e32ff_v_i32m4x2
+#define vlseg2eff_v_u32m4x2	vlseg2e32ff_v_u32m4x2
+#define vlseg2eff_v_i32m4x2_m	vlseg2e32ff_v_i32m4x2_m
+#define vlseg2eff_v_u32m4x2_m	vlseg2e32ff_v_u32m4x2_m
+#define vlseg2eff_v_i64m1x2	vlseg2e64ff_v_i64m1x2
+#define vlseg2eff_v_u64m1x2	vlseg2e64ff_v_u64m1x2
+#define vlseg2eff_v_i64m1x2_m	vlseg2e64ff_v_i64m1x2_m
+#define vlseg2eff_v_u64m1x2_m	vlseg2e64ff_v_u64m1x2_m
+#define vlseg3eff_v_i64m1x3	vlseg3e64ff_v_i64m1x3
+#define vlseg3eff_v_u64m1x3	vlseg3e64ff_v_u64m1x3
+#define vlseg3eff_v_i64m1x3_m	vlseg3e64ff_v_i64m1x3_m
+#define vlseg3eff_v_u64m1x3_m	vlseg3e64ff_v_u64m1x3_m
+#define vlseg4eff_v_i64m1x4	vlseg4e64ff_v_i64m1x4
+#define vlseg4eff_v_u64m1x4	vlseg4e64ff_v_u64m1x4
+#define vlseg4eff_v_i64m1x4_m	vlseg4e64ff_v_i64m1x4_m
+#define vlseg4eff_v_u64m1x4_m	vlseg4e64ff_v_u64m1x4_m
+#define vlseg5eff_v_i64m1x5	vlseg5e64ff_v_i64m1x5
+#define vlseg5eff_v_u64m1x5	vlseg5e64ff_v_u64m1x5
+#define vlseg5eff_v_i64m1x5_m	vlseg5e64ff_v_i64m1x5_m
+#define vlseg5eff_v_u64m1x5_m	vlseg5e64ff_v_u64m1x5_m
+#define vlseg6eff_v_i64m1x6	vlseg6e64ff_v_i64m1x6
+#define vlseg6eff_v_u64m1x6	vlseg6e64ff_v_u64m1x6
+#define vlseg6eff_v_i64m1x6_m	vlseg6e64ff_v_i64m1x6_m
+#define vlseg6eff_v_u64m1x6_m	vlseg6e64ff_v_u64m1x6_m
+#define vlseg7eff_v_i64m1x7	vlseg7e64ff_v_i64m1x7
+#define vlseg7eff_v_u64m1x7	vlseg7e64ff_v_u64m1x7
+#define vlseg7eff_v_i64m1x7_m	vlseg7e64ff_v_i64m1x7_m
+#define vlseg7eff_v_u64m1x7_m	vlseg7e64ff_v_u64m1x7_m
+#define vlseg8eff_v_i64m1x8	vlseg8e64ff_v_i64m1x8
+#define vlseg8eff_v_u64m1x8	vlseg8e64ff_v_u64m1x8
+#define vlseg8eff_v_i64m1x8_m	vlseg8e64ff_v_i64m1x8_m
+#define vlseg8eff_v_u64m1x8_m	vlseg8e64ff_v_u64m1x8_m
+#define vlseg2eff_v_i64m2x2	vlseg2e64ff_v_i64m2x2
+#define vlseg2eff_v_u64m2x2	vlseg2e64ff_v_u64m2x2
+#define vlseg2eff_v_i64m2x2_m	vlseg2e64ff_v_i64m2x2_m
+#define vlseg2eff_v_u64m2x2_m	vlseg2e64ff_v_u64m2x2_m
+#define vlseg3eff_v_i64m2x3	vlseg3e64ff_v_i64m2x3
+#define vlseg3eff_v_u64m2x3	vlseg3e64ff_v_u64m2x3
+#define vlseg3eff_v_i64m2x3_m	vlseg3e64ff_v_i64m2x3_m
+#define vlseg3eff_v_u64m2x3_m	vlseg3e64ff_v_u64m2x3_m
+#define vlseg4eff_v_i64m2x4	vlseg4e64ff_v_i64m2x4
+#define vlseg4eff_v_u64m2x4	vlseg4e64ff_v_u64m2x4
+#define vlseg4eff_v_i64m2x4_m	vlseg4e64ff_v_i64m2x4_m
+#define vlseg4eff_v_u64m2x4_m	vlseg4e64ff_v_u64m2x4_m
+#define vlseg2eff_v_i64m4x2	vlseg2e64ff_v_i64m4x2
+#define vlseg2eff_v_u64m4x2	vlseg2e64ff_v_u64m4x2
+#define vlseg2eff_v_i64m4x2_m	vlseg2e64ff_v_i64m4x2_m
+#define vlseg2eff_v_u64m4x2_m	vlseg2e64ff_v_u64m4x2_m
+#define vlseg2eff_v_f16m1x2	vlseg2e16ff_v_f16m1x2
+#define vlseg2eff_v_f16m1x2_m	vlseg2e16ff_v_f16m1x2_m
+#define vlseg3eff_v_f16m1x3	vlseg3e16ff_v_f16m1x3
+#define vlseg3eff_v_f16m1x3_m	vlseg3e16ff_v_f16m1x3_m
+#define vlseg4eff_v_f16m1x4	vlseg4e16ff_v_f16m1x4
+#define vlseg4eff_v_f16m1x4_m	vlseg4e16ff_v_f16m1x4_m
+#define vlseg5eff_v_f16m1x5	vlseg5e16ff_v_f16m1x5
+#define vlseg5eff_v_f16m1x5_m	vlseg5e16ff_v_f16m1x5_m
+#define vlseg6eff_v_f16m1x6	vlseg6e16ff_v_f16m1x6
+#define vlseg6eff_v_f16m1x6_m	vlseg6e16ff_v_f16m1x6_m
+#define vlseg7eff_v_f16m1x7	vlseg7e16ff_v_f16m1x7
+#define vlseg7eff_v_f16m1x7_m	vlseg7e16ff_v_f16m1x7_m
+#define vlseg8eff_v_f16m1x8	vlseg8e16ff_v_f16m1x8
+#define vlseg8eff_v_f16m1x8_m	vlseg8e16ff_v_f16m1x8_m
+#define vlseg2eff_v_f16m2x2	vlseg2e16ff_v_f16m2x2
+#define vlseg2eff_v_f16m2x2_m	vlseg2e16ff_v_f16m2x2_m
+#define vlseg3eff_v_f16m2x3	vlseg3e16ff_v_f16m2x3
+#define vlseg3eff_v_f16m2x3_m	vlseg3e16ff_v_f16m2x3_m
+#define vlseg4eff_v_f16m2x4	vlseg4e16ff_v_f16m2x4
+#define vlseg4eff_v_f16m2x4_m	vlseg4e16ff_v_f16m2x4_m
+#define vlseg2eff_v_f16m4x2	vlseg2e16ff_v_f16m4x2
+#define vlseg2eff_v_f16m4x2_m	vlseg2e16ff_v_f16m4x2_m
+#define vlseg2eff_v_f32m1x2	vlseg2e32ff_v_f32m1x2
+#define vlseg2eff_v_f32m1x2_m	vlseg2e32ff_v_f32m1x2_m
+#define vlseg3eff_v_f32m1x3	vlseg3e32ff_v_f32m1x3
+#define vlseg3eff_v_f32m1x3_m	vlseg3e32ff_v_f32m1x3_m
+#define vlseg4eff_v_f32m1x4	vlseg4e32ff_v_f32m1x4
+#define vlseg4eff_v_f32m1x4_m	vlseg4e32ff_v_f32m1x4_m
+#define vlseg5eff_v_f32m1x5	vlseg5e32ff_v_f32m1x5
+#define vlseg5eff_v_f32m1x5_m	vlseg5e32ff_v_f32m1x5_m
+#define vlseg6eff_v_f32m1x6	vlseg6e32ff_v_f32m1x6
+#define vlseg6eff_v_f32m1x6_m	vlseg6e32ff_v_f32m1x6_m
+#define vlseg7eff_v_f32m1x7	vlseg7e32ff_v_f32m1x7
+#define vlseg7eff_v_f32m1x7_m	vlseg7e32ff_v_f32m1x7_m
+#define vlseg8eff_v_f32m1x8	vlseg8e32ff_v_f32m1x8
+#define vlseg8eff_v_f32m1x8_m	vlseg8e32ff_v_f32m1x8_m
+#define vlseg2eff_v_f32m2x2	vlseg2e32ff_v_f32m2x2
+#define vlseg2eff_v_f32m2x2_m	vlseg2e32ff_v_f32m2x2_m
+#define vlseg3eff_v_f32m2x3	vlseg3e32ff_v_f32m2x3
+#define vlseg3eff_v_f32m2x3_m	vlseg3e32ff_v_f32m2x3_m
+#define vlseg4eff_v_f32m2x4	vlseg4e32ff_v_f32m2x4
+#define vlseg4eff_v_f32m2x4_m	vlseg4e32ff_v_f32m2x4_m
+#define vlseg2eff_v_f32m4x2	vlseg2e32ff_v_f32m4x2
+#define vlseg2eff_v_f32m4x2_m	vlseg2e32ff_v_f32m4x2_m
+#define vlseg2eff_v_f64m1x2	vlseg2e64ff_v_f64m1x2
+#define vlseg2eff_v_f64m1x2_m	vlseg2e64ff_v_f64m1x2_m
+#define vlseg3eff_v_f64m1x3	vlseg3e64ff_v_f64m1x3
+#define vlseg3eff_v_f64m1x3_m	vlseg3e64ff_v_f64m1x3_m
+#define vlseg4eff_v_f64m1x4	vlseg4e64ff_v_f64m1x4
+#define vlseg4eff_v_f64m1x4_m	vlseg4e64ff_v_f64m1x4_m
+#define vlseg5eff_v_f64m1x5	vlseg5e64ff_v_f64m1x5
+#define vlseg5eff_v_f64m1x5_m	vlseg5e64ff_v_f64m1x5_m
+#define vlseg6eff_v_f64m1x6	vlseg6e64ff_v_f64m1x6
+#define vlseg6eff_v_f64m1x6_m	vlseg6e64ff_v_f64m1x6_m
+#define vlseg7eff_v_f64m1x7	vlseg7e64ff_v_f64m1x7
+#define vlseg7eff_v_f64m1x7_m	vlseg7e64ff_v_f64m1x7_m
+#define vlseg8eff_v_f64m1x8	vlseg8e64ff_v_f64m1x8
+#define vlseg8eff_v_f64m1x8_m	vlseg8e64ff_v_f64m1x8_m
+#define vlseg2eff_v_f64m2x2	vlseg2e64ff_v_f64m2x2
+#define vlseg2eff_v_f64m2x2_m	vlseg2e64ff_v_f64m2x2_m
+#define vlseg3eff_v_f64m2x3	vlseg3e64ff_v_f64m2x3
+#define vlseg3eff_v_f64m2x3_m	vlseg3e64ff_v_f64m2x3_m
+#define vlseg4eff_v_f64m2x4	vlseg4e64ff_v_f64m2x4
+#define vlseg4eff_v_f64m2x4_m	vlseg4e64ff_v_f64m2x4_m
+#define vlseg2eff_v_f64m4x2	vlseg2e64ff_v_f64m4x2
+#define vlseg2eff_v_f64m4x2_m	vlseg2e64ff_v_f64m4x2_m
+
+/* Wrapper only.  */
+#define vfncvt_x_f_v_i8m1	vfncvt_x_f_w_i8m1
+#define vfncvt_x_f_v_i8m1_m	vfncvt_x_f_w_i8m1_m
+#define vfncvt_x_f_v_i8m2	vfncvt_x_f_w_i8m2
+#define vfncvt_x_f_v_i8m2_m	vfncvt_x_f_w_i8m2_m
+#define vfncvt_x_f_v_i8m4	vfncvt_x_f_w_i8m4
+#define vfncvt_x_f_v_i8m4_m	vfncvt_x_f_w_i8m4_m
+#define vfncvt_x_f_v_i16m1	vfncvt_x_f_w_i16m1
+#define vfncvt_x_f_v_i16m1_m	vfncvt_x_f_w_i16m1_m
+#define vfncvt_x_f_v_i16m2	vfncvt_x_f_w_i16m2
+#define vfncvt_x_f_v_i16m2_m	vfncvt_x_f_w_i16m2_m
+#define vfncvt_x_f_v_i16m4	vfncvt_x_f_w_i16m4
+#define vfncvt_x_f_v_i16m4_m	vfncvt_x_f_w_i16m4_m
+#define vfncvt_x_f_v_i32m1	vfncvt_x_f_w_i32m1
+#define vfncvt_x_f_v_i32m1_m	vfncvt_x_f_w_i32m1_m
+#define vfncvt_x_f_v_i32m2	vfncvt_x_f_w_i32m2
+#define vfncvt_x_f_v_i32m2_m	vfncvt_x_f_w_i32m2_m
+#define vfncvt_x_f_v_i32m4	vfncvt_x_f_w_i32m4
+#define vfncvt_x_f_v_i32m4_m	vfncvt_x_f_w_i32m4_m
+
+/* Wrapper only.  */
+#define vfncvt_xu_f_v_u8m1	vfncvt_xu_f_w_u8m1
+#define vfncvt_xu_f_v_u8m1_m	vfncvt_xu_f_w_u8m1_m
+#define vfncvt_xu_f_v_u8m2	vfncvt_xu_f_w_u8m2
+#define vfncvt_xu_f_v_u8m2_m	vfncvt_xu_f_w_u8m2_m
+#define vfncvt_xu_f_v_u8m4	vfncvt_xu_f_w_u8m4
+#define vfncvt_xu_f_v_u8m4_m	vfncvt_xu_f_w_u8m4_m
+#define vfncvt_xu_f_v_u16m1	vfncvt_xu_f_w_u16m1
+#define vfncvt_xu_f_v_u16m1_m	vfncvt_xu_f_w_u16m1_m
+#define vfncvt_xu_f_v_u16m2	vfncvt_xu_f_w_u16m2
+#define vfncvt_xu_f_v_u16m2_m	vfncvt_xu_f_w_u16m2_m
+#define vfncvt_xu_f_v_u16m4	vfncvt_xu_f_w_u16m4
+#define vfncvt_xu_f_v_u16m4_m	vfncvt_xu_f_w_u16m4_m
+#define vfncvt_xu_f_v_u32m1	vfncvt_xu_f_w_u32m1
+#define vfncvt_xu_f_v_u32m1_m	vfncvt_xu_f_w_u32m1_m
+#define vfncvt_xu_f_v_u32m2	vfncvt_xu_f_w_u32m2
+#define vfncvt_xu_f_v_u32m2_m	vfncvt_xu_f_w_u32m2_m
+#define vfncvt_xu_f_v_u32m4	vfncvt_xu_f_w_u32m4
+#define vfncvt_xu_f_v_u32m4_m	vfncvt_xu_f_w_u32m4_m
+
+/* Wrapper only.  */
+#define vnsrl_vv_u8m1	vnsrl_wv_u8m1
+#define vnsrl_vv_u8m1_m	vnsrl_wv_u8m1_m
+#define vnsrl_vv_u8m2	vnsrl_wv_u8m2
+#define vnsrl_vv_u8m2_m	vnsrl_wv_u8m2_m
+#define vnsrl_vv_u8m4	vnsrl_wv_u8m4
+#define vnsrl_vv_u8m4_m	vnsrl_wv_u8m4_m
+#define vnsrl_vv_u16m1	vnsrl_wv_u16m1
+#define vnsrl_vv_u16m1_m	vnsrl_wv_u16m1_m
+#define vnsrl_vv_u16m2	vnsrl_wv_u16m2
+#define vnsrl_vv_u16m2_m	vnsrl_wv_u16m2_m
+#define vnsrl_vv_u16m4	vnsrl_wv_u16m4
+#define vnsrl_vv_u16m4_m	vnsrl_wv_u16m4_m
+#define vnsrl_vv_u32m1	vnsrl_wv_u32m1
+#define vnsrl_vv_u32m1_m	vnsrl_wv_u32m1_m
+#define vnsrl_vv_u32m2	vnsrl_wv_u32m2
+#define vnsrl_vv_u32m2_m	vnsrl_wv_u32m2_m
+#define vnsrl_vv_u32m4	vnsrl_wv_u32m4
+#define vnsrl_vv_u32m4_m	vnsrl_wv_u32m4_m
+
+/* Wrapper only.  */
+#define vnsrl_vx_u8m1	vnsrl_wx_u8m1
+#define vnsrl_vx_u8m1_m	vnsrl_wx_u8m1_m
+#define vnsrl_vx_u8m2	vnsrl_wx_u8m2
+#define vnsrl_vx_u8m2_m	vnsrl_wx_u8m2_m
+#define vnsrl_vx_u8m4	vnsrl_wx_u8m4
+#define vnsrl_vx_u8m4_m	vnsrl_wx_u8m4_m
+#define vnsrl_vx_u16m1	vnsrl_wx_u16m1
+#define vnsrl_vx_u16m1_m	vnsrl_wx_u16m1_m
+#define vnsrl_vx_u16m2	vnsrl_wx_u16m2
+#define vnsrl_vx_u16m2_m	vnsrl_wx_u16m2_m
+#define vnsrl_vx_u16m4	vnsrl_wx_u16m4
+#define vnsrl_vx_u16m4_m	vnsrl_wx_u16m4_m
+#define vnsrl_vx_u32m1	vnsrl_wx_u32m1
+#define vnsrl_vx_u32m1_m	vnsrl_wx_u32m1_m
+#define vnsrl_vx_u32m2	vnsrl_wx_u32m2
+#define vnsrl_vx_u32m2_m	vnsrl_wx_u32m2_m
+#define vnsrl_vx_u32m4	vnsrl_wx_u32m4
+#define vnsrl_vx_u32m4_m	vnsrl_wx_u32m4_m
+
+/* Wrapper only.  */
+#define vnsra_vv_i8m1	vnsra_wv_i8m1
+#define vnsra_vv_i8m1_m	vnsra_wv_i8m1_m
+#define vnsra_vv_i8m2	vnsra_wv_i8m2
+#define vnsra_vv_i8m2_m	vnsra_wv_i8m2_m
+#define vnsra_vv_i8m4	vnsra_wv_i8m4
+#define vnsra_vv_i8m4_m	vnsra_wv_i8m4_m
+#define vnsra_vv_i16m1	vnsra_wv_i16m1
+#define vnsra_vv_i16m1_m	vnsra_wv_i16m1_m
+#define vnsra_vv_i16m2	vnsra_wv_i16m2
+#define vnsra_vv_i16m2_m	vnsra_wv_i16m2_m
+#define vnsra_vv_i16m4	vnsra_wv_i16m4
+#define vnsra_vv_i16m4_m	vnsra_wv_i16m4_m
+#define vnsra_vv_i32m1	vnsra_wv_i32m1
+#define vnsra_vv_i32m1_m	vnsra_wv_i32m1_m
+#define vnsra_vv_i32m2	vnsra_wv_i32m2
+#define vnsra_vv_i32m2_m	vnsra_wv_i32m2_m
+#define vnsra_vv_i32m4	vnsra_wv_i32m4
+#define vnsra_vv_i32m4_m	vnsra_wv_i32m4_m
+
+/* Wrapper only.  */
+#define vnsra_vx_i8m1	vnsra_wx_i8m1
+#define vnsra_vx_i8m1_m	vnsra_wx_i8m1_m
+#define vnsra_vx_i8m2	vnsra_wx_i8m2
+#define vnsra_vx_i8m2_m	vnsra_wx_i8m2_m
+#define vnsra_vx_i8m4	vnsra_wx_i8m4
+#define vnsra_vx_i8m4_m	vnsra_wx_i8m4_m
+#define vnsra_vx_i16m1	vnsra_wx_i16m1
+#define vnsra_vx_i16m1_m	vnsra_wx_i16m1_m
+#define vnsra_vx_i16m2	vnsra_wx_i16m2
+#define vnsra_vx_i16m2_m	vnsra_wx_i16m2_m
+#define vnsra_vx_i16m4	vnsra_wx_i16m4
+#define vnsra_vx_i16m4_m	vnsra_wx_i16m4_m
+#define vnsra_vx_i32m1	vnsra_wx_i32m1
+#define vnsra_vx_i32m1_m	vnsra_wx_i32m1_m
+#define vnsra_vx_i32m2	vnsra_wx_i32m2
+#define vnsra_vx_i32m2_m	vnsra_wx_i32m2_m
+#define vnsra_vx_i32m4	vnsra_wx_i32m4
+#define vnsra_vx_i32m4_m	vnsra_wx_i32m4_m
+
+/* Wrapper only.  */
+#define vnclipu_vv_u8m1	vnclipu_wv_u8m1
+#define vnclipu_vv_u8m1_m	vnclipu_wv_u8m1_m
+#define vnclipu_vv_u8m2	vnclipu_wv_u8m2
+#define vnclipu_vv_u8m2_m	vnclipu_wv_u8m2_m
+#define vnclipu_vv_u8m4	vnclipu_wv_u8m4
+#define vnclipu_vv_u8m4_m	vnclipu_wv_u8m4_m
+#define vnclipu_vv_u16m1	vnclipu_wv_u16m1
+#define vnclipu_vv_u16m1_m	vnclipu_wv_u16m1_m
+#define vnclipu_vv_u16m2	vnclipu_wv_u16m2
+#define vnclipu_vv_u16m2_m	vnclipu_wv_u16m2_m
+#define vnclipu_vv_u16m4	vnclipu_wv_u16m4
+#define vnclipu_vv_u16m4_m	vnclipu_wv_u16m4_m
+#define vnclipu_vv_u32m1	vnclipu_wv_u32m1
+#define vnclipu_vv_u32m1_m	vnclipu_wv_u32m1_m
+#define vnclipu_vv_u32m2	vnclipu_wv_u32m2
+#define vnclipu_vv_u32m2_m	vnclipu_wv_u32m2_m
+#define vnclipu_vv_u32m4	vnclipu_wv_u32m4
+#define vnclipu_vv_u32m4_m	vnclipu_wv_u32m4_m
+
+/* Wrapper only.  */
+#define vnclipu_vx_u8m1	vnclipu_wx_u8m1
+#define vnclipu_vx_u8m1_m	vnclipu_wx_u8m1_m
+#define vnclipu_vx_u8m2	vnclipu_wx_u8m2
+#define vnclipu_vx_u8m2_m	vnclipu_wx_u8m2_m
+#define vnclipu_vx_u8m4	vnclipu_wx_u8m4
+#define vnclipu_vx_u8m4_m	vnclipu_wx_u8m4_m
+#define vnclipu_vx_u16m1	vnclipu_wx_u16m1
+#define vnclipu_vx_u16m1_m	vnclipu_wx_u16m1_m
+#define vnclipu_vx_u16m2	vnclipu_wx_u16m2
+#define vnclipu_vx_u16m2_m	vnclipu_wx_u16m2_m
+#define vnclipu_vx_u16m4	vnclipu_wx_u16m4
+#define vnclipu_vx_u16m4_m	vnclipu_wx_u16m4_m
+#define vnclipu_vx_u32m1	vnclipu_wx_u32m1
+#define vnclipu_vx_u32m1_m	vnclipu_wx_u32m1_m
+#define vnclipu_vx_u32m2	vnclipu_wx_u32m2
+#define vnclipu_vx_u32m2_m	vnclipu_wx_u32m2_m
+#define vnclipu_vx_u32m4	vnclipu_wx_u32m4
+#define vnclipu_vx_u32m4_m	vnclipu_wx_u32m4_m
+
+/* Wrapper only.  */
+#define vnclip_vv_i8m1	vnclip_wv_i8m1
+#define vnclip_vv_i8m1_m	vnclip_wv_i8m1_m
+#define vnclip_vv_i8m2	vnclip_wv_i8m2
+#define vnclip_vv_i8m2_m	vnclip_wv_i8m2_m
+#define vnclip_vv_i8m4	vnclip_wv_i8m4
+#define vnclip_vv_i8m4_m	vnclip_wv_i8m4_m
+#define vnclip_vv_i16m1	vnclip_wv_i16m1
+#define vnclip_vv_i16m1_m	vnclip_wv_i16m1_m
+#define vnclip_vv_i16m2	vnclip_wv_i16m2
+#define vnclip_vv_i16m2_m	vnclip_wv_i16m2_m
+#define vnclip_vv_i16m4	vnclip_wv_i16m4
+#define vnclip_vv_i16m4_m	vnclip_wv_i16m4_m
+#define vnclip_vv_i32m1	vnclip_wv_i32m1
+#define vnclip_vv_i32m1_m	vnclip_wv_i32m1_m
+#define vnclip_vv_i32m2	vnclip_wv_i32m2
+#define vnclip_vv_i32m2_m	vnclip_wv_i32m2_m
+#define vnclip_vv_i32m4	vnclip_wv_i32m4
+#define vnclip_vv_i32m4_m	vnclip_wv_i32m4_m
+
+/* Wrapper only.  */
+#define vnclip_vx_i8m1	vnclip_wx_i8m1
+#define vnclip_vx_i8m1_m	vnclip_wx_i8m1_m
+#define vnclip_vx_i8m2	vnclip_wx_i8m2
+#define vnclip_vx_i8m2_m	vnclip_wx_i8m2_m
+#define vnclip_vx_i8m4	vnclip_wx_i8m4
+#define vnclip_vx_i8m4_m	vnclip_wx_i8m4_m
+#define vnclip_vx_i16m1	vnclip_wx_i16m1
+#define vnclip_vx_i16m1_m	vnclip_wx_i16m1_m
+#define vnclip_vx_i16m2	vnclip_wx_i16m2
+#define vnclip_vx_i16m2_m	vnclip_wx_i16m2_m
+#define vnclip_vx_i16m4	vnclip_wx_i16m4
+#define vnclip_vx_i16m4_m	vnclip_wx_i16m4_m
+#define vnclip_vx_i32m1	vnclip_wx_i32m1
+#define vnclip_vx_i32m1_m	vnclip_wx_i32m1_m
+#define vnclip_vx_i32m2	vnclip_wx_i32m2
+#define vnclip_vx_i32m2_m	vnclip_wx_i32m2_m
+#define vnclip_vx_i32m4	vnclip_wx_i32m4
+#define vnclip_vx_i32m4_m	vnclip_wx_i32m4_m
+
+/* Wrapper only.  */
+#define vfncvt_f_x_v_f16m1	vfncvt_f_x_w_f16m1
+#define vfncvt_f_x_v_f16m1_m	vfncvt_f_x_w_f16m1_m
+#define vfncvt_f_x_v_f16m2	vfncvt_f_x_w_f16m2
+#define vfncvt_f_x_v_f16m2_m	vfncvt_f_x_w_f16m2_m
+#define vfncvt_f_x_v_f16m4	vfncvt_f_x_w_f16m4
+#define vfncvt_f_x_v_f16m4_m	vfncvt_f_x_w_f16m4_m
+#define vfncvt_f_x_v_f32m1	vfncvt_f_x_w_f32m1
+#define vfncvt_f_x_v_f32m1_m	vfncvt_f_x_w_f32m1_m
+#define vfncvt_f_x_v_f32m2	vfncvt_f_x_w_f32m2
+#define vfncvt_f_x_v_f32m2_m	vfncvt_f_x_w_f32m2_m
+#define vfncvt_f_x_v_f32m4	vfncvt_f_x_w_f32m4
+#define vfncvt_f_x_v_f32m4_m	vfncvt_f_x_w_f32m4_m
+
+/* Wrapper only.  */
+#define vfncvt_f_xu_v_f16m1	vfncvt_f_xu_w_f16m1
+#define vfncvt_f_xu_v_f16m1_m	vfncvt_f_xu_w_f16m1_m
+#define vfncvt_f_xu_v_f16m2	vfncvt_f_xu_w_f16m2
+#define vfncvt_f_xu_v_f16m2_m	vfncvt_f_xu_w_f16m2_m
+#define vfncvt_f_xu_v_f16m4	vfncvt_f_xu_w_f16m4
+#define vfncvt_f_xu_v_f16m4_m	vfncvt_f_xu_w_f16m4_m
+#define vfncvt_f_xu_v_f32m1	vfncvt_f_xu_w_f32m1
+#define vfncvt_f_xu_v_f32m1_m	vfncvt_f_xu_w_f32m1_m
+#define vfncvt_f_xu_v_f32m2	vfncvt_f_xu_w_f32m2
+#define vfncvt_f_xu_v_f32m2_m	vfncvt_f_xu_w_f32m2_m
+#define vfncvt_f_xu_v_f32m4	vfncvt_f_xu_w_f32m4
+#define vfncvt_f_xu_v_f32m4_m	vfncvt_f_xu_w_f32m4_m
+
+/* Wrapper only.  */
+#define vfncvt_f_f_v_f16m1	vfncvt_f_f_w_f16m1
+#define vfncvt_f_f_v_f16m1_m	vfncvt_f_f_w_f16m1_m
+#define vfncvt_f_f_v_f16m2	vfncvt_f_f_w_f16m2
+#define vfncvt_f_f_v_f16m2_m	vfncvt_f_f_w_f16m2_m
+#define vfncvt_f_f_v_f16m4	vfncvt_f_f_w_f16m4
+#define vfncvt_f_f_v_f16m4_m	vfncvt_f_f_w_f16m4_m
+#define vfncvt_f_f_v_f32m1	vfncvt_f_f_w_f32m1
+#define vfncvt_f_f_v_f32m1_m	vfncvt_f_f_w_f32m1_m
+#define vfncvt_f_f_v_f32m2	vfncvt_f_f_w_f32m2
+#define vfncvt_f_f_v_f32m2_m	vfncvt_f_f_w_f32m2_m
+#define vfncvt_f_f_v_f32m4	vfncvt_f_f_w_f32m4
+#define vfncvt_f_f_v_f32m4_m	vfncvt_f_f_w_f32m4_m
+
+//#endif
diff --git a/tests/fortran_example/SConscript b/tests/fortran_example/SConscript
new file mode 100644
index 00000000..9776aa11
--- /dev/null
+++ b/tests/fortran_example/SConscript
@@ -0,0 +1,20 @@
+# RT-Thread building script for component
+
+from building import *
+
+cwd = GetCurrentDir()
+src = Glob('*.cpp')
+CPPPATH = [cwd, "../install/include"]
+
+CPPDEFINES = [
+    'HAVE_CCONFIG_H',
+]
+group = DefineGroup('openblas_fortran',
+                    src,
+                    depend=[''],
+                    CPPPATH=CPPPATH,
+                    CPPDEFINES=CPPDEFINES,
+                    LIBS=['libopenblas', 'cxx', 'stdc++'],
+                    LIBPATH=['../install/lib'])
+
+Return('group')
diff --git a/tests/fortran_example/SConstruct b/tests/fortran_example/SConstruct
new file mode 100644
index 00000000..349829cf
--- /dev/null
+++ b/tests/fortran_example/SConstruct
@@ -0,0 +1,8 @@
+import os
+import sys
+
+# add building.py path
+sys.path = sys.path + [os.path.join('..', '..', '..', '..', 'tools')]
+from building import *
+
+BuildApplication('blas_fortran', 'SConscript', usr_root = '../../..')
diff --git a/tests/fortran_example/openblas_fortran.cpp b/tests/fortran_example/openblas_fortran.cpp
new file mode 100644
index 00000000..1813e4d8
--- /dev/null
+++ b/tests/fortran_example/openblas_fortran.cpp
@@ -0,0 +1,97 @@
+/*
+ * Copyright (c) 2006-2021, RT-Thread Development Team
+ *
+ * SPDX-License-Identifier: Apache-2.0
+ *
+ * Change Logs:
+ * Date           Author       Notes
+ * 2023/03/07     YeC          The first version
+ *
+ * @brief  Test for OpenBLAS fortran interface.
+ * All the classes tested in this file are summarized as below:
+ * --openblas fortran inference
+ */
+
+#include "stdio.h"
+#include "stdlib.h"
+#include "f77blas.h"
+#include "sys/time.h"
+#include "time.h"
+#include <iostream>
+#include <string>
+#include <sstream>
+
+using namespace std;
+
+extern void dgemm_(char*, char*, int*, int*,int*, double*, double*, int*, double*, int*, double*, double*, int*);
+
+string expect = R"(16.801 18.002 18.003 16.801 15.602 22.803 )";
+
+int main(int argc, char* argv[])
+{
+  int i = 0;
+  int m = 2;
+  int n = 3;
+  int k = 4;
+  int sizeofa = m * k;
+  int sizeofb = k * n;
+  int sizeofc = m * n;
+  char ta = 'N';
+  char tb = 'N';
+  double alpha = 1.2;
+  double beta = 0.001;
+
+  double* A = (double*)malloc(sizeof(double) * sizeofa);
+  double* B = (double*)malloc(sizeof(double) * sizeofb);
+  double* C = (double*)malloc(sizeof(double) * sizeofc);
+
+  srand((unsigned)time(NULL));
+
+  for (i=0; i<sizeofa; i++)
+    A[i] = i%3+1;//(rand()%100)/10.0;
+
+  for (i=0; i<sizeofb; i++)
+    B[i] = i%3+1;//(rand()%100)/10.0;
+
+  for (i=0; i<sizeofc; i++)
+    C[i] = i%3+1;//(rand()%100)/10.0;
+
+  printf("m=%d,n=%d,k=%d,alpha=%lf,beta=%lf,sizeofc=%d\n",m,n,k,alpha,beta,sizeofc);
+  dgemm_(&ta, &tb, &m, &n, &k, &alpha, A, &m, B, &k, &beta, C, &m);
+
+  printf("This is matrix A\n\n");
+  for(i=0; i < sizeofa; i++)
+    printf("%lf ", A[i]);
+  printf("\n");
+  
+  printf("This is matrix B\n\n");
+  for(i=0; i < sizeofb; i++)
+    printf("%lf ", B[i]);
+  printf("\n");
+  
+  stringstream ss;
+  streambuf   *buffer = cout.rdbuf();
+  cout.rdbuf(ss.rdbuf());
+  
+  for(int i=0;i<sizeofc;i++)
+    {cout<<C[i]<<" ";}
+  
+  cout.rdbuf(buffer);
+  string s(ss.str());
+  cout << "*********************************************************" << endl;
+  cout << "This is the result:" << endl;
+  cout << s << endl;
+  cout << "*********************************************************" << endl;
+  cout << "This is the reference:" << endl;
+  cout << expect << endl;
+
+  if (expect == s)
+    cout << "{Test PASS}." << endl;
+  else
+    cout << "{Test FAIL}." << endl;
+
+  free(A);
+  free(B);
+  free(C);
+  return 0;
+}
diff --git a/tests/openblas_level1/SConscript b/tests/openblas_level1/SConscript
index a44672ed..5960cfbc 100644
--- a/tests/openblas_level1/SConscript
+++ b/tests/openblas_level1/SConscript
@@ -14,7 +14,7 @@ group = DefineGroup('openblas_level1',
                     depend=[''],
                     CPPPATH=CPPPATH,
                     CPPDEFINES=CPPDEFINES,
-                    LIBS=['libopenblas'],
+                    LIBS=['libopenblas', 'cxx', 'stdc++'],
                     LIBPATH=['../install/lib'])
 
 Return('group')
diff --git a/tests/openblas_level1/openblas_level1.cpp b/tests/openblas_level1/openblas_level1.cpp
index acb0f5eb..f97d72e5 100644
--- a/tests/openblas_level1/openblas_level1.cpp
+++ b/tests/openblas_level1/openblas_level1.cpp
@@ -5,31 +5,56 @@
  *
  * Change Logs:
  * Date           Author       Notes
- * 2022/11/03     YeC          The first version
+ * 2023/03/07     YeC          The first version
  *
- * @brief  Test for OpenBLAS
+ * @brief  Test for OpenBLAS level1 interface.
+ * All the classes tested in this file are summarized as below:
+ * --cblas_saxpy
  */
 
 #include "cblas.h"
-
+#include <iostream>
+#include <string>
+#include <sstream>
 #define N 4
-int main(int argc, char **argv)
+
+using namespace std;
+
+string expect = R"(4 7 11 14 )";
+
+int main(int argc, char ** argv)
 {
-    float alpha = 3;
-    float x[4]  = {1.0, 2, 3, 4};
+    float alpha=3;
+    float x[4]={1.0,2,3,4};
     /* 1,2,3,4
      */
-    float y[4]  = {1, 1, 2, 2};
+    float y[4]={1,1,2,2};
     /* 1,1,2,2
      */
 
-    cblas_saxpy(N, alpha, x, 1, y, 1);
-    printf("sscal result: \n");
-    char _s[20] = {0};
-    for (int j = 0; j < N; j++) {
-        sprintf(_s, "%f", y[j]);
-        printf("%s, ", _s);
-    }
-    printf("\n");
+    cblas_saxpy(N, alpha, x , 1, y, 1);
+
+    stringstream ss;
+    streambuf   *buffer = cout.rdbuf();
+    cout.rdbuf(ss.rdbuf());
+  
+    for(int i=0;i<N;i++)
+      {cout<<y[i]<<" ";}
+  
+    cout.rdbuf(buffer);
+    string s(ss.str());
+    cout << "*********************************************************" << endl;
+    cout << "This is the result:" << endl;
+    cout << s << endl;
+    cout << "*********************************************************" << endl;
+    cout << "This is the reference:" << endl;
+    cout << expect << endl;
+
+    if (expect == s)
+      cout << "{Test PASS}." << endl;
+    else
+      cout << "{Test FAIL}." << endl;
+    
     return 0;
 }
+
diff --git a/tests/openblas_level2/SConscript b/tests/openblas_level2/SConscript
index ee37b8bc..b87dae92 100644
--- a/tests/openblas_level2/SConscript
+++ b/tests/openblas_level2/SConscript
@@ -14,7 +14,7 @@ group = DefineGroup('openblas_level2',
                     depend=[''],
                     CPPPATH=CPPPATH,
                     CPPDEFINES=CPPDEFINES,
-                    LIBS=['libopenblas'],
+                    LIBS=['libopenblas', 'cxx', 'stdc++'],
                     LIBPATH=['../install/lib'])
 
 Return('group')
diff --git a/tests/openblas_level2/openblas_level2.cpp b/tests/openblas_level2/openblas_level2.cpp
index dbd82667..65db2da2 100644
--- a/tests/openblas_level2/openblas_level2.cpp
+++ b/tests/openblas_level2/openblas_level2.cpp
@@ -5,35 +5,63 @@
  *
  * Change Logs:
  * Date           Author       Notes
- * 2022/11/03     YeC          The first version
+ * 2023/03/07     YeC          The first version
  *
- * @brief  Test for OpenBLAS
+ * @brief  Test for OpenBLAS Interface level2.
+ * All the classes tested in this file are summarized as below:
+ * --cblas_saxpy
  */
 
 #include "cblas.h"
-int main(int argc, char **argv)
+#include <iostream>
+#include <string>
+#include <sstream>
+
+using namespace std;
+
+string expect = R"(20 40 10 20 30 60 )";
+
+int main(int argc, char ** argv)
 {
-    float   x[2] = {1.0, 2.0};
-    float   y[3] = {2.0, 1.0, 3.0};
-    float   A[6] = {0};
+    float x[2] = {1.0, 2.0};
+    float y[3] = {2.0, 1.0, 3.0};
+    float A[6] = { 0 };
     blasint rows = 2, cols = 3;
-    float   alpha = 10;
+    float alpha = 10;
     blasint inc_x = 1, inc_y = 1;
     blasint lda = 2;
 
-    // 矩阵按列优先存储
-    // A <== alpha*x*y' + A （y'表示y的转置）
+    //矩阵按列优先存储
+    //A <== alpha*x*y' + A （y'表示y的转置）
     cblas_sger(CblasColMajor, rows, cols, alpha, x, inc_x, y, inc_y, A, lda);
-
-    printf("sger result: \n");
-    char _s[20] = {0};
-    for (int i = 0; i < rows; i++) {
-        for (int j = 0; j < cols; j++) {
-            sprintf(_s, "%f", A[i * cols + j]);
-            printf("%s, ", _s);
-        }
-        printf("\n");
+    
+    
+    stringstream ss;
+    streambuf   *buffer = cout.rdbuf();
+    cout.rdbuf(ss.rdbuf());
+  
+    for(int i=0;i<rows;i++)
+    {
+    	for(int j=0;j<cols;j++)
+    	{
+           cout<<A[i*cols+j]<<" ";
+    	}
     }
-    printf("\n");
+  
+    cout.rdbuf(buffer);
+    string s(ss.str());
+    cout << "*********************************************************" << endl;
+    cout << "This is the result:" << endl;
+    cout << s << endl;
+    cout << "*********************************************************" << endl;
+    cout << "This is the reference:" << endl;
+    cout << expect << endl;
+
+    if (expect == s)
+      cout << "{Test PASS}." << endl;
+    else
+      cout << "{Test FAIL}." << endl;
+    
     return 0;
 }
+
diff --git a/tests/openblas_level3/SConscript b/tests/openblas_level3/SConscript
index e5bd0b1e..4abdb6f6 100644
--- a/tests/openblas_level3/SConscript
+++ b/tests/openblas_level3/SConscript
@@ -14,7 +14,7 @@ group = DefineGroup('openblas_level3',
                     depend=[''],
                     CPPPATH=CPPPATH,
                     CPPDEFINES=CPPDEFINES,
-                    LIBS=['libopenblas'],
+                    LIBS=['libopenblas', 'cxx', 'stdc++'],
                     LIBPATH=['../install/lib'])
 
 Return('group')
diff --git a/tests/openblas_level3/openblas_level3.cpp b/tests/openblas_level3/openblas_level3.cpp
index c16d8831..80149de3 100644
--- a/tests/openblas_level3/openblas_level3.cpp
+++ b/tests/openblas_level3/openblas_level3.cpp
@@ -5,42 +5,64 @@
  *
  * Change Logs:
  * Date           Author       Notes
- * 2022/11/03     YeC          The first version
+ * 2023/03/07     YeC          The first version
  *
- * @brief  Test for OpenBLAS
+ * @brief  Test for OpenBLAS Interface level3.
+ * All the classes tested in this file are summarized as below:
+ * --cblas_sgemm
  */
 
 #include "cblas.h"
+#include <iostream>
+#include <string>
+#include <sstream>
+
+using namespace std;
+
+string expect = R"(7 10 15 22 )";
+
 #define M 2
 #define N 2
 #define K 2
-int main(int argc, char **argv)
-{
-    float alpha    = 1;
-    float beta     = 0;
-    int   lda      = K;
-    int   ldb      = N;
-    int   ldc      = N;
-    float A[M * K] = {1, 2, 3, 4};
+int main(int argc, char ** argv){
+    float alpha=1;
+    float beta=0;
+    int lda=K;
+    int ldb=N;
+    int ldc=N;
+    float A[M*K]={1,2,3,4};
     /* 1,2
      * 3,4
      */
-    float B[K * N] = {1, 2, 3, 4};
+    float B[K*N]={1,2,3,4};
     /* 1,2
      * 3,4
      */
-    float C[M * N];
+    float C[M*N];
     cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasNoTrans, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
+    
+    stringstream ss;
+    streambuf   *buffer = cout.rdbuf();
+    cout.rdbuf(ss.rdbuf());
+    
+    for(int i=0;i<M*N;i++)
+        {cout<<C[i]<<" ";}
+    
+    cout.rdbuf(buffer);
+    string s(ss.str());
+    cout << "*********************************************************" << endl;
+    cout << "This is the result:" << endl;
+    cout << s << endl;
+    cout << "*********************************************************" << endl;
+    cout << "This is the reference:" << endl;
+    cout << expect << endl;
 
-    printf("sgemm result: \n");
-    char _s[20] = {0};
-    for (int i = 0; i < M; i++) {
-        for (int j = 0; j < N; j++) {
-            sprintf(_s, "%f", C[i * N + j]);
-            printf("%s, ", _s);
-        }
-        printf("\n");
-    }
-    printf("\n");
+    if (expect == s)
+        cout << "{Test PASS}." << endl;
+    else
+        cout << "{Test FAIL}." << endl;
+
+    
     return 0;
 }
+
-- 
2.34.1

